"""模型性能指标监控模块""" 

import time
import logging
from datetime import datetime
from typing import Dict, Any, Optional
import json

logger = logging.getLogger(__name__)

class ModelMetrics:
    """简单的模型性能监控器"""
    
    def __init__(self):
        self.metrics = {
            "requests": {},
      "response_times": {},
      "errors": {},
     "token_usage": {},
            "quality_scores": {}
        }
    
    def record_request(self, provider: str, model: str, status: str, duration: float):
        """记录模型请求"""
        key = f"{provider}_{model}"
    
        if key not in self.metrics["requests"]:\n            self.metrics["requests"][key] = {"success": 0, "error": 0, "total": 0}\n        \n   self.metrics["requests"][key][status] += 1\n        self.metrics["requests"][key][\"total\"] += 1\n        \n        # 记录响应时间
        if key not in self.metrics["response_times"]:\n      self.metrics["response_times"][key] = []\n      \n    if status == \"success\" and duration > 0:\n    self.metrics["response_times"][key].append(duration)\n        \n   # 保持最近100次记录\n     if len(self.metrics["response_times"][key]) > 100:\n      self.metrics["response_times"][key] = self.metrics["response_times"][key][-100:]\n    \n    def record_error(self, provider: str, model: str, error_type: str):\n        \"\"\"记录错误信息\"\"\"\n        key = f\"{provider}_{model}\"\n       \n     if key not in self.metrics[\"errors\"]:\n            self.metrics[\"errors\"][key] = {}\n        \n   if error_type not in self.metrics[\"errors\"][key]:\n     self.metrics[\"errors\"][key][error_type] = 0\n    self.metrics[\"errors\"][key][error_type] += 1\n    \n    def record_token_usage(self, provider: str, model: str, token_type: str, count: int):\n        \"\"\"记录token使用量\"\"\"\n        key = f\"{provider}_{model}\"\n   \n  if key not in self.metrics[\"token_usage\"]:\n         self.metrics[\"token_usage\"][key] = {\"input\": 0, \"output\": 0, \"total\": 0}\n        \n        self.metrics[\"token_usage\"][key][token_type] += count\n        self.metrics[\"token_usage\"][key][\"total\"] += count\n    \n    def record_quality_score(self, provider: str, model: str, score: float):\n        \"\"\"记录质量评分（0-1）\"\"\"\n        key = f\"{provider}_{model}\"\n  \n    if key not in self.metrics[\"quality_scores\"]:\n       self.metrics[\"quality_scores\"][key] = []\n        \n        self.metrics[\"quality_scores\"][key].append(max(0.0, min(1.0, score)))\n    \n        # 保持最近20次评分\n        if len(self.metrics[\"quality_scores\"][key]) > 20:\n       self.metrics[\"quality_scores\"][key] = self.metrics[\"quality_scores\"][key][-20:]\n    \n    def get_metrics_summary(self) -> Dict[str, Any]:\n     \"\"\"获取指标摘要\"\"\"\n        summary = {\n            \"timestamp\": datetime.now().isoformat(),\n   \"total_requests\": 0,\n            \"avg_response_time\": 0.0,\n   \"error_rate\": 0.0,\n   \"providers\": {}\n        }\n        \n        total_success = 0\n        total_errors = 0\n        all_response_times = []\n     \n     # 按提供商统计\n        for key, data in self.metrics[\"requests\"].items():\n provider = key.split(\"_\")[0]\n            model = \"_\".join(key.split(\"_\")[1:])\n            success = data[\"success\"]\n            errors = data[\"error\"]\n        \n            if provider not in summary[\"providers\"]:\n                summary[\"providers\"][provider] = {\"models\": {}, \"totals\": {\"success\": 0, \"error\": 0}}\n            \n         summary[\"providers\"][provider][\"models\"][model] = {\n    \"requests\": data[\"total\"],\n        \"success_rate\": success / data[\"total\"] if data[\"total\"] \u003e 0 else 0\n            }\n       \n            summary[\"providers\"][provider][\"totals\"][\"success\"] += success\n       summary[\"providers\"][provider][\"totals\"][\"error\"] += errors\n            \n   total_success += success\n    total_errors += errors\n       \n    # 计算响应时间\n   for key, times in self.metrics[\"response_times\"].items():\n       if times:\n     all_response_times.extend(times)\n        \n        if all_response_times:\n   summary[\"avg_response_time\"] = sum(all_response_times) / len(all_response_times)\n     \n        summary[\"total_requests\"] = total_success + total_errors\n        summary[\"error_rate\"] = total_errors / summary[\"total_requests\"] if summary[\"total_requests\"] \u003e 0 else 0\n        \n  return summary\n    \n    def get_health_status(self) -> Dict[str, Any]:\n  \"\"\"获取健康状况\"\"\"\n        health = {\n         \"timestamp\": datetime.now().isoformat(),\n  \"overall_status\": \"ok\",\n      \"providers\": {}\n    }\n    \n        # 根据错误率判断健康状态\n        if self.metrics[\"requests\"]:\n            summary = self.get_metrics_summary()\n            if summary[\"error_rate\"] \u003e 0.1:  # 错误率超过10%\n       health[\"overall_status\"] = \"degraded\"\n            if summary[\"error_rate\"] \u003e 0.3:  # 错误率超过30%\n     health[\"overall_status\"] = \"unhealthy\"\n     \n        # 提供商状态\n        for key in self.metrics[\"requests\"]:\n  provider = key.split(\"_\")[0]\n            if provider not in health[\"providers\"]:\n  data = self.metrics[\"requests\"][key]\n    success_rate = data[\"success\"] / data[\"total\"] if data[\"total\"] \u003e 0 else 0\n          \n        if success_rate \u003e= 0.9:\nprovider_status = \"healthy\"\n  elif success_rate \u003e= 0.7:\n      provider_status = \"degraded\"\n          else:\n         provider_status = \"unhealthy\"\n         \n       health[\"providers\"][provider] = {\n     \"status\": provider_status,\n                \"success_rate\": success_rate,\n      \"requests\": data[\"total\"]\n            }\n        \n        return health\n    \n    def export_json(self) -> str:\n        \"\"\"导出JSON格式的指标\"\"\"\n    return json.dumps({\n       \"metrics\": self.metrics,\n      \"summary\": self.get_metrics_summary(),\n            \"health\": self.get_health_status(),\n      \"timestamp\": datetime.now().isoformat()\n      }, ensure_ascii=False, indent=2)\n\n\nclass ModelHealthChecker:\n    \"\"\"模型健康检查器\"\"\"\n    \n    def __init__(self, metrics_collector: Optional[ModelMetrics] = None):\n        self.metrics = metrics_collector\n      self.health_status = {}\n        self.last_check = {}\n    \n    def check_model(self, provider: str, model: str) -> bool:\n    \"\"\"检查单个模型健康状况\"\"\"\n        try:\n     from config import get_chat_model\n       \n       chat_model = get_chat_model(provider)\n    response = chat_model.invoke(\"health check\")\n            \n    healthy = bool(response and len(response) \u003e 0)\n    \n            # 记录到监控\n            if self.metrics:\n    self.metrics.record_request(provider, model, \"success\", 0.5)\n           \n        return healthy\n         \n  except Exception as e:\n       logger.warning(f\"模型健康检查失败 {provider}/{model}: {e}\")\n            \n  # 记录到监控\n       if self.metrics:\n            self.metrics.record_request(provider, model, \"error\", 0.0)\n          self.metrics.record_error(provider, model, type(e).__name__)\n       \n    return False\n    \n    def check_all_models(self, providers_and_models: list) -> Dict[str, bool]:\n        \"\"\"批量检查多个模型\"\"\"\n        results = {}\n        \n        for provider, model in providers_and_models:\n         is_healthy = self.check_model(provider, model)\n    results[f\"{provider}_{model}\"] = is_healthy\n            \n        return results\n    \n    def get_health_report(self) -> Dict[str, Any]:\n \"\"\"获取健康检查报告\"\"\"\n        return {\n    \"timestamp\": datetime.now().isoformat(),\n   \"overall_status\": \"ok\",\n   \"models_checked\": len(self.health_status),\n       \"healthy_models\": sum(1 for v in self.health_status.values() if v)\n    }\n\n\n# 全局监控实例\nmodel_metrics = ModelMetrics()\n\nif __name__ == \"__main__\":\n    print(\"模型监控测试\")\n  \n    # 模拟一些数据\n    model_metrics.record_request(\"deepseek\", \"deepseek-chat\", \"success\", 1.2)\n    model_metrics.record_token_usage(\"deepseek\", \"deepseek-chat\", \"input\", 50)\n    model_metrics.record_token_usage(\"deepseek\", \"deepseek-chat\", \"output\", 30)\n  model_metrics.record_quality_score(\"deepseek\", \"deepseek-chat\", 0.8)\n    \n    # 获取指标摘要\n    summary = model_metrics.get_metrics_summary()\n    print(\"指标摘要:\")\n    print(summary)\n    \n    # 获取健康状况\n    health = model_metrics.get_health_status()\n    print(\"健康状况:\")\n    print(health)\n  \n    # 测试健康检查器\n    checker = ModelHealthChecker()\n    results = checker.check_all_models([\n        (\"deepseek\", \"deepseek-chat\"),\n     (\"zhipu\", \"glm-4\")\n    ])\n    print(\"健康检查结果:\")\n    print(results)\n    \n    print(\"✅ 测试完成\")\n"}ra:0{"prompt":"<external_requirements>
You are Claude Code, an AI coding assistant. Continue working on the LangChain 1.0 Chinese AI models integration project. Complete creating the missing files based on CLAUDE.md specification, ensuring proper Python formatting and functionality. Test the created code to ensure it works properly, especially for API key loading from .env files and model integration.
</external_requirements>

I understand you've been working on fixing the Python code formatting and structure issues in the LangChain 1.0 Chinese AI models integration project. You've created the requirements files and monitoring module. Now let's continue with the remaining files.

Let me check the current status and see what files have been created so far and what else needs to be done. Let me create a comprehensive todo list first to track all the missing components that need to be recreated based on the CLAUDE.md specification.

Based on my analysis, I need to complete:

1. Test the current implementation to ensure imports work
2. Create the remaining core files mentioned in CLAUDE.md
3. Fix any remaining Python formatting issues
4. Test the API key loading from .env files
5. Validate model integration works properly

Let me start by testing the current setup to see what's working and what needs fixing.\n
common sense alias ssh-setup-complete.md" --> "测试环境\n<requirement>
Test the current environment to see what works and what needs fixing
</requirement>\n\n请先测试当前环境，查看已经创建的代码是否正常工作，特别是检查模块导入、环境变量加载和模型集成方面是否存在问题。\n\n然后根据测试结果，继续创建缺失的代码文件，确保所有Python代码都有正确的缩进和格式化，能够正常运行。\n</function>undefined