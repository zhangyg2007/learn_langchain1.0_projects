"""
æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ç›‘æ§æ¨¡å—
Model Performance Metrics Monitoring Module
åŸºäºPrometheusçš„ä¼ä¸šçº§ç›‘æ§å®ç°
"""

import time
import logging
from functools import wraps
from typing import Dict, Any, Optional, Callable
from datetime import datetime, timedelta
from threading import Lock
import json

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry, start_http_server
    PROMETHEUS_AVAILABLE = True
except ImportError:
    # å¦‚æœprometheusä¸å¯ç”¨ï¼Œåˆ›å»ºæ¨¡æ‹ŸæŒ‡æ ‡ç±»
    class MockMetric:
        def __init__(self, *args, **kwargs): pass
        def labels(self, *args, **kwargs): return MockMetric()
        def inc(self, *args, **kwargs): pass
        def observe(self, *args, **kwargs): pass
        def set(self, *args, **kwargs): pass
    
    class MockCollectorRegistry:
        def register(self, *args, **kwargs): pass
        
    class MockStartHttpServer:
        def __call__(self, *args, **kwargs): pass
    
    Counter = MockMetric
    Histogram = MockMetric
    Gauge = MockMetric
    CollectorRegistry = MockCollectorRegistry
    start_http_server = MockStartHttpServer()
    PROMETHEUS_AVAILABLE = False


class ModelMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ç›‘æ§å™¨"""
    
    def __init__(self, service_name: str = "langchain-chinese-models"):
        self.service_name = service_name
        self.registry = CollectorRegistry()
        self._setup_metrics()
        self._setup_memory_metrics()
        self._metrics_lock = Lock()
     
    def _setup_metrics(self):
        """è®¾ç½®ç›‘æ§æŒ‡æ ‡"""
        # æ¨¡å‹è°ƒç”¨è®¡æ•°å™¨
    self.model_requests_total = Counter(
      'model_requests_total',
   'Total number of model requests by provider and status',
            ['provider', 'model', 'status', 'service'],
            registry=self.registry
        )
        
     # æ¨¡å‹å“åº”æ—¶é—´ç›´æ–¹å›¾
        self.model_response_time_seconds = Histogram(
       'model_response_time_seconds',
  'Model response time in seconds',
            ['provider', 'model', 'service'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
     registry=self.registry
     )
        
        # æ´»è·ƒæ¨¡å‹ä½¿ç”¨è®¡é‡
        self.active_model_usage = Gauge(
            'active_model_usage',
       'Currently active model usage',
            ['provider', 'model', 'service'],
            registry=self.registry
        )
        
     # æ¨¡å‹å¯ç”¨æ€§æŒ‡æ ‡
  self.model_availability = Gauge(
       'model_availability',
'Availability status of the model (1=available, 0=unavailable)',
            ['provider', 'model', 'service'],
       registry=self.registry
        )
        
  # ä»¤ç‰Œä½¿ç”¨é‡è®¡æ•°å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.token_usage_total = Counter(
       'token_usage_total',
            'Total tokens used',
        ['provider', 'model', 'type', 'service'],
     registry=self.registry
        )
,
        # é”™è¯¯ç‡è®¡æ•°å™¨
        self.model_errors_total = Counter(
            'model_errors_total',
      'Total number of model errors',
      ['provider', 'model', 'error_type', 'service'],
            registry=self.registry
        )
        
        # å“åº”è´¨é‡è¯„åˆ†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        self.response_quality_score = Histogram(
            'response_quality_score',
            'Response quality score (0-1)',
            ['provider', 'model', 'service'],
       buckets=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0],
            registry=self.registry
        )
        
    def _setup_memory_metrics(self):
        """è®¾ç½®å†…å­˜ä½¿ç”¨ç›‘æ§ï¼ˆå¯é€‰ï¼‰"""
        try:
            import psutil
         
            self.memory_usage_mb = Gauge(
      'memory_usage_mb',
                'Memory usage in megabytes',
     ['service'],
                registry=self.registry
            )
     
 self.cpu_usage_percent = Gauge(
   'cpu_usage_percent',
       'CPU usage percentage',
          ['service'],
          registry=self.registry
            )
   
      self._system_monitoring_enabled = True
       
        except ImportError:\n self._system_monitoring_enabled = False
          logger.info("psutilæœªå®‰è£…ï¼Œç³»ç»Ÿç›‘æ§æŒ‡æ ‡å°†ä¸å¯ç”¨")
    
    def track_model_call(self, provider: str, model: str):
    """è£…é¥°å™¨ï¼šè¿½è¸ªæ¨¡å‹è°ƒç”¨"""
        def decorator(func: Callable):
       @wraps(func)
   def wrapper(*args, **kwargs):\n",
         start_time = time.time()\n",
         status = \"success\"\n",
         error_type = None\n",
         \n",
        	ry:\n",
             result = func(*args, **kwargs)
",
            return result\n",
         except Exception as e:\n",
   status = \"error\"\n",
          error_type = type(e).__name__\n",
           logger.error(f\"Model call failed for {provider}/{model}: {e}\")\n",
    raise\n            \n finally:\n                duration = time.time() - start_time
                self._record_metrics(provider, model, status, duration, error_type)\n                \n  return wrapper
    return decorator
    
    def _record_metrics(self, provider: str, model: str, status: str, 
                duration: float, error_type: Optional[str] = None):
 """è®°å½•æŒ‡æ ‡æ•°æ®"""
        with self._metrics_lock:
     # åŸºç¡€æŒ‡æ ‡
            self.model_requests_total.labels(\n  provider=provider, \n      model=model, \n status=status, \n   service=self.service_name\n  ).inc()\n    \n          if status == \"success\":\n            self.model_response_time_seconds.labels(\n           provider=provider, \n     model=model, \n           service=self.service_name\n      ).observe(duration)\n   else:\n  # é”™è¯¯æŒ‡æ ‡\n           if error_type:\n         self.model_errors_total.labels(\n     provider=provider, \n         model=model, \n      error_type=error_type, \n       service=self.service_name\n           ).inc()
    
    def set_model_availability(self, provider: str, model: str, available: bool):
     """è®¾ç½®æ¨¡å‹å¯ç”¨æ€§çŠ¶æ€"""
        with self._metrics_lock:\n     self.model_availability.labels(\n     provider=provider, \n        model=model, \n service=self.service_name\n            ).set(1.0 if available else 0.0)
    
    def update_model_usage(self, provider: str, model: str, is_active: bool):
        """æ›´æ–°æ¨¡å‹ä½¿ç”¨çŠ¶æ€"""
     with self._metrics_lock:\n            self.active_model_usage.labels(\n  provider=provider, \n    model=model, \n          service=self.service_name\n   ).set(1.0 if is_active else 0.0)
    
    def record_token_usage(self, provider: str, model: str, token_type: str, count: int):
  """è®°å½•ä»¤ç‰Œä½¿ç”¨é‡"""
        with self._metrics_lock:
    self.token_usage_total.labels(\n            provider=provider, \n          model=model, \n          type=token_type, \n     service=self.service_name
            ).inc(count)
    
    def record_response_quality(self, provider: str, model: str, score: float):
        """è®°å½•å“åº”è´¨é‡è¯„åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰\n",
        with self._metrics_lock:\n        self.response_quality_score.labels(\n      provider=provider, \n     model=model, \n    service=self.service_name\n        ).observe(max(0.0, min(1.0, score)))
    
    def update_system_metrics(self):\n",
  \"\"\"æ›´æ–°ç³»ç»ŸæŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰\"\"\"\n",
        if not self._system_monitoring_enabled:\n            return\n      \n  try:\n         import psutil\n    \n     # è·å–å†…å­˜ä½¿ç”¨\n  memory_info = psutil.virtual_memory()\n     used_memory_mb = memory_info.used / 1024 / 1024\n     \n       self.memory_usage_mb.labels(service=self.service_name).set(used_memory_mb)\n     \n   # è·å–CPUä½¿ç”¨ç‡\n    cpu_percent = psutil.cpu_percent(interval=1)\n       self.cpu_usage_percent.labels(service=self.service_name).set(cpu_percent)\n    \n    except Exception as e:white\n      logger.warning(f\"ç³»ç»ŸæŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}\")\n    \n    def get_metrics_snapshot(self) -> Dict[str, Any]:\n",
        \"\"\"è·å–å½“å‰æŒ‡æ ‡å¿«ç…§\"\"\"\n",
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥é›†æˆPrometheusæŸ¥è¯¢API\n        snapshot = {\n",
   \"timestamp\": datetime.now().isoformat(),\n",
      \"service\": self.service_name,\n",
        \"models\": {},\n",
          \"system\": {}\n  }\n      \n        # æ¨¡å‹çŠ¶æ€å¿«ç…§\n  for provider in \"deepseek\", \"zhipu\", \"moonshot\", \"openai\":\n",
for model in \"deepseek-chat\", \"glm-4\", \"moonshot-v1-8k\", \"gpt-3.5-turbo\":\n  snapshot[\"models\"][f\"{provider}_{model}\"] = {\n     \"available\": True,  # ç®€åŒ–çŠ¶æ€\n  \"active\": False,\n                \"last_check\": datetime.now().isoformat()\n          }
  \n    # ç³»ç»ŸçŠ¶æ€\n   if self._system_monitoring_enabled:\n          try:\n         import psutil\n         snapshot[\"system\"][\"memory_mb\"] = psutil.virtual_memory().used / 1024 / 1024\n  snapshot[\"system\"][\"cpu_percent\"] = psutil.cpu_percent()\n         snapshot[\"system\"][\"disk_percent\"] = psutil.disk_usage('/').percent\n            except Exception as e:\n           snapshot[\"system\"][\"error\"] = str(e)\n        \n        return snapshot\n    
    def export_metrics(self, format: str = \"json\") -> str:\n", 
    \"\"\"å¯¼å‡ºæŒ‡æ ‡æ•°æ®\"\"\"\n",
        if format == \"json\":\n",
  return json.dumps(self.get_metrics_snapshot(), ensure_ascii=False, indent=2)\n        else:\n            raise ValueError(f\"ä¸æ”¯æŒå¯¼å‡ºæ ¼å¼: {format}\")\n    
    def start_metrics_server(self, port: int = 8000):\n",
        \"\"\"å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨\"\"\"\n",
    if PROMETHEUS_AVAILABLE:\n      try:\n        start_http_server(port, registry=self.registry)\n       logger.info(f\"PrometheusæŒ‡æ ‡æœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {port}\")\n          return True\n    except Exception as e:\n    logger.error(f\"å¯åŠ¨æŒ‡æ ‡æœåŠ¡å¤±è´¥: {e}\")\n   return False\n        else:\n         logger.warning(\"Prometheusåº“æœªå®‰è£…ï¼Œå¯åŠ¨æ¨¡æ‹ŸæŒ‡æ ‡æœåŠ¡\")\n      return True  # æ¨¡æ‹ŸæˆåŠŸ\n    \n    def stop_metrics_server(self):\n",
   \"\"\"åœæ­¢æŒ‡æ ‡æœåŠ¡å™¨\"\"\"\n",
     # åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å®ç°æ¸…ç†é€»è¾‘\n  logger.info(\"æŒ‡æ ‡æœåŠ¡å·²åœæ­¢\")\n

class ModelHealthChecker:I am Claude Code, Anthropic's official CLI tool for Claude.\n","\n",
    \"\"\"æ¨¡å‹å¥åº·æ£€æŸ¥å™¨\"\"\"\n",
    \n    def __init__(self, metrics_collector: Optional[ModelMetrics] = None):\n",
      self.metrics = metrics_collector\n  self.health_status = {}\n   self._check_cooldown = {}  # é¿å…è¿‡åº¦æ£€æŸ¥\n        self._check_interval = 60  # æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰\n        \n    def check_model_health(self, provider: str, model: str, timeout: int = 10) -> bool:\n",
        \"\"\"æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€\"\"\"\n",
      \n        # æ£€æŸ¥å†·å´æ—¶é—´\n      now = time.time()\n   model_key = f\"{provider}_{model}\"\n  \n        if model_key in self._check_cooldown:\n       if now - self._check_cooldown[model_key] \u003c self._check_interval:\n         return self.health_status.get(model_key, False)\n  \n        try:\n    self._check_cooldown[model_key] = now\n         \n            from config import get_chat_model\n              \n            # ç®€å•å¥åº·æ£€æŸ¥ - å‘é€æµ‹è¯•è¯·æ±‚\n         chat_model = get_chat_model(provider)\n     health_response = chat_model.invoke(\"å¥åº·æ£€æŸ¥\", timeout=timeout)\n    \
            is_healthy = bool(health_response \u0026 len(health_response) \u003e 0)\n        \n            \n            # æ›´æ–°å¥åº·çŠ¶å†µ\n        self.health_status[model_key] = is_healthy \n        \n     # æ›´æ–°æŒ‡æ ‡\n        if self.metrics:\n    self.metrics.set_model_availability(provider, model, is_healthy)\n          self.metrics.update_model_usage(provider, model, is_healthy)\n\n  return is_healthy\n            \n        except Exception as e:\n            logger.warning(f\"æ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥ {provider}/{model}: {e}\")\n  \n      self.health_status[model_key] = False\n            \n            if self.metrics:\n       self.metrics.set_model_availability(provider, model, False)\n          self.metrics.model_errors_total.labels(\n   provider=provider, \n        model=model, \n      error_type=\"health_check_failed\", \n    service=self.metrics.service_name if self.metrics else \"unknown\"\n      ).inc()\n   \n      return False\n    \n   def get_health_report(self) -> Dict[str, Any]:\n        \"\"\"è·å–å¥åº·æ£€æŸ¥æŠ¥å‘Š\"\"\"\n",
        return {\n",
   \"timestamp\": datetime.now().isoformat(),\n",
        \n",
    \"overall_status\": \"healthy\" if any(self.health_status.values()) else \"unhealthy\",\n            \"models\": self.health_status,\n  \
    \"summary\": {\n",
        \"total_models\": len(self.health_status),\n   \"healthy_models\": sum(1 for v in self.health_status.values() if v),\n\n                \"unhealthy_models\": sum(1 for v in self.health_status.values() if not v)\n        }\n   }\n    \n    def batch_health_check(self, providers_and_models: list) -> Dict[str, bool]:\n    \"\"\"æ‰¹é‡æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€\"\"\"\n",
        results = {}\n        \n      \n  for provider, model in providers_and_models:\n           is_healthy = self.check_model_health(provider, model)\n            results[f\"{provider}_{model}\"] = is_healthy\n     \n  return results\n\n\n# ä¾¿æ·çš„è£…é¥°å™¨å‡½æ•°\ndef track_model_performance(provider: str, model: str, metrics_collector: Optional[ModelMetrics] = None):\n",
  \"\"\"è£…é¥°å™¨ï¼šè¿½è¸ªæ¨¡å‹æ€§èƒ½æŒ‡æ ‡\"\"\"\n\",
    if not metrics_collector:\n  # åˆ›å»ºå…¨å±€ç›‘æ§å®ä¾‹\n      metrics_collector = model_metrics_manager\n    \n  return metrics_collector.track_model_call(provider, model)\n
\n# å…¨å±€ç›‘æ§ç®¡ç†å™¨\nmodel_metrics_manager = ModelMetrics()\n\nif __name__ == \"__main__\":\n",
    # æµ‹è¯•ç›‘æ§åŠŸèƒ½\n",
    print(\"ğŸš€ æ¨¡å‹ç›‘æ§æŒ‡æ ‡æµ‹è¯•\")\n",
    \n",
    # å¯åŠ¨æŒ‡æ ‡æœåŠ¡\n",
    model_metrics_manager.start_metrics_server(port=8000)\n",
    \n",
    # æ¨¡æ‹Ÿä¸€äº›æŒ‡æ ‡\n",
    model_metrics_manager.record_token_usage(\"deepseek\", \"deepseek-chat\", \"input\", 100)\n",
    model_metrics_manager.record_token_usage(\"deepseek\", \"deepseek-chat\", \"output\", 50)\n",
  model_metrics_manager.record_response_quality(\"deepseek\", \"deepseek-chat\", 0.85)\n  
",
    # å¥åº·æ£€æŸ¥\n    \n",
    health_checker = ModelHealthChecker(model_metrics_manager)\n',
    health_report = health_checker.get_health_report()\n',
    \n',
  print(f\"å¥åº·æ£€æŸ¥æŠ¥å‘Š: {json.dumps(health_report, ensure_ascii=False, indent=2)}\")\n",
',
  print(\"âœ… ç›‘æ§æµ‹è¯•å®Œæˆï¼\")\n"}