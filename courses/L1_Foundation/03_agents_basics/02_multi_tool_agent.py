#!/usr/bin/env python3
"""
LangChain L1 Foundation - Week 3
è¯¾ç¨‹æ ‡é¢˜: å¤šå·¥å…·æ™ºèƒ½ä½“é›†æˆä¸ä¸­å›½AIæ¨¡å‹æ”¯æŒ
å­¦ä¹ ç›®æ ‡:
  - å­¦ä¼šé›†æˆå¤šä¸ªä¸“ä¸šå·¥å…·æ„å»ºå¤æ‚Agent
  - æŒæ¡ä¸­å›½ä¸»è¦AIæ¨¡å‹(OpenAI, èµµè˜‹, Kimi, DeepSeek)åœ¨Agentä¸­çš„é›†æˆ
  - ç†è§£å·¥å…·è·¯ç”±(Tool Routing)å’Œæ™ºèƒ½é€‰æ‹©ç­–ç•¥
  - å®è·µç”Ÿäº§çº§çš„é”™è¯¯å¤„ç†å’Œå®¹é”™ç­–ç•¥
  - æ„å»ºå¯æ‰©å±•çš„Agentæ¶æ„æ¡†æ¶
ä½œè€…: Claude Code æ•™å­¦å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2024-01-16
ç‰ˆæœ¬: 1.0.0
å…ˆå†³æ¡ä»¶: å®ŒæˆWeek 3 AgentsåŸºç¡€æ¦‚å¿µå­¦ä¹ 

ğŸ¯ å®è·µé‡ç‚¹:
  - å¤šLLM-Agenté€‰æ‹©
  - ä¸“ä¸šå·¥å…·function design
  - ä¸­å›½æ¨¡å‹Adapteræ¨¡å¼
  - ç”Ÿäº§çº§å®¹é”™ç­–ç•¥
"""

import sys
import os
import time
import json
import random
from typing import Dict, List, Optional, Union, Any, Callable
from datetime import datetime
from dataclasses import dataclass
from pathlib import Path
from abc import ABC, abstractmethod

# ç¯å¢ƒé…ç½®
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡å·²åŠ è½½")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè¯·ç¡®ä¿æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡")

# LangChainæ ¸å¿ƒä¾èµ–
try:
    from langchain_core.tools import Tool, StructuredTool, BaseTool
    from langchain_core.language_models import BaseLanguageModel
    from langchain_core.callbacks import Callbacks
    from langchain_core.runnables import RunnableConfig
    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
    from langchain_core.prompts import ChatPromptTemplate 
    print("âœ… LangChainå·¥å…·ä¸æ¶ˆæ¯ç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ LangChainç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    print("   â””â”€ pip install langchain-core langchain")
    sys.exit(1)

# ä¸­å›½å¤§æ¨¡å‹æ”¯æŒçš„é¢å¤–ä¾èµ–
try:
    from langchain_openai import ChatOpenAI
    print("âœ… OpenAIæ¨¡å‹æ”¯æŒå¯¼å…¥æˆåŠŸ")
except ImportError:
    # å¦‚æœOpenAIæ¨¡å‹ä¸å¯ç”¨ï¼Œä»ç„¶å¯ä»¥ç»§ç»­å…¶ä»–å†…å®¹
    ChatOpenAI = None
    print("âš ï¸ OpenAIæ¨¡å‹æ— æ³•å¯¼å…¥ï¼Œå°†ä»…å±•ç¤ºæ¦‚å¿µæ€§ä»£ç ")

try:
    import requests
    print("âœ… HTTPè¯·æ±‚æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âŒ requestsæ¨¡å—ç¼ºå¤±ï¼Œå°†é™åˆ¶éƒ¨åˆ†ç½‘ç»œå·¥å…·åŠŸèƒ½")
    requests = None

@dataclass
class ToolExecutionRecord:
    """å·¥å…·æ‰§è¡Œè®°å½•"""
    tool_name: str
    input_args: Dict[str, Any]
    output_result: Any
    execution_time: float
    success: bool
    error_msg: Optional[str] = None
    timestamp: datetime = None

@dataclass
class ChinaModelConfig:
    """ä¸­å›½å¤§æ¨¡å‹é…ç½®"""
    provider: str
    api_key: str
    base_url: Optional[str] = None
    model_name: str = ""
    max_tokens: int = 1024
    temperature: float = 0.7
    timeout: int = 60

class MultiToolAgentTrainer:
    """å¤šå·¥å…·æ™ºèƒ½ä½“è¿›é˜¶è®­ç»ƒå™¨"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.tools = {}  # å­—å…¸å­˜å‚¨å·¥å…·
        self.libraries = {}  # å¤–éƒ¨åº“å°è£…
        self.models = {}  # ä¸åŒæ¨¡å‹å®ä¾‹
        self.performance_stats = []
        
        # åˆå§‹åŒ–æ¼”ç¤ºæ•°æ®
        self.sample_documents = self._init_document_samples()
        self.knowledge_base = self._init_knowledge_base()
        
    def _log(self, message: str):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"ğŸ¤– {message}")
    
    def _init_document_samples(self) -> Dict[str, str]:
        """åˆå§‹åŒ–ç¤ºä¾‹æ–‡æ¡£æ•°æ®"""
        return {
            "machine_learning_basics": """
            æœºå™¨å­¦ä¹ (Machine Learning)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ ¸å¿ƒåˆ†æ”¯ï¼Œå®ƒè®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›
            å…¶è¡¨ç°ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜¾å¼çš„ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†ææ•°æ®ã€è¯†åˆ«æ¨¡å¼ï¼Œç„¶åä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡º
            é¢„æµ‹æˆ–å†³ç­–ã€‚
            
            æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼š
            1. ç›‘ç£å­¦ä¹ (Supervised Learning): ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹
            2. æ— ç›‘ç£å­¦ä¹ (Unsupervised Learning): ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼
            3. å¼ºåŒ–å­¦ä¹ (Reinforcement Learning): é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥
            """,
            
            "deep_learning_overview": """
            æ·±åº¦å­¦ä¹ (Deep Learning)æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æ€æƒ³ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å…·æœ‰
            å¤šä¸ªå±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ æ•°æ®çš„å¤šå±‚æŠ½è±¡è¡¨ç¤ºæ¥è‡ªåŠ¨å‘ç°ç‰¹å¾å±‚æ¬¡ç»“æ„ï¼Œ
            ä½¿å…¶ç‰¹åˆ«é€‚åˆå¤„ç†éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚
            
            å…³é”®çš„æ·±åº¦å­¦ä¹ æ¶æ„åŒ…æ‹¬ï¼š
            - å·ç§¯ç¥ç»ç½‘ç»œ(CNN): ä¸»è¦ç”¨äºå›¾åƒè¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰
            - å¾ªç¯ç¥ç»ç½‘ç»œ(RNN) and LSTM: å¤„ç†åºåˆ—æ•°æ®
            - Transformeræ¶æ„: é©å‘½æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€
            - ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN): ç”¨äºç”Ÿæˆé€¼çœŸæ•°æ®
            """,
            
            "artificial_intelligence_summary": """
            äººå·¥æ™ºèƒ½(Artificial Intelligence, AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªå¹¿æ³›åˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦
            äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚AIç³»ç»ŸåŒ…æ‹¬è§„åˆ’ã€å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€çŸ¥è¯†è¡¨ç¤ºã€æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰
            èƒ½åŠ›ã€‚
            
            AIçš„å‘å±•ç»å†äº†å¤šä¸ªé˜¶æ®µï¼š
            - ä¸“å®¶ç³»ç»Ÿæ—¶ä»£ï¼šåŸºäºè§„åˆ™çš„æ¨ç†ç³»ç»Ÿ
            - æœºå™¨å­¦ä¹ æ—¶ä»£ï¼šæ•°æ®é©±åŠ¨çš„æ¨¡å¼è¯†åˆ«
            - æ·±åº¦å­¦ä¹ æ—¶ä»£ï¼šç¥ç»ç½‘ç»œçš„æ·±åº¦æ¶æ„
            - å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼šé€šç”¨çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
            """
        }
    
    def _init_knowledge_base(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–çŸ¥è¯†åº“æ•°æ®"""
        return {
            "ai_concepts": {
                "machine_learning": {
                    "definition": "è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹çš„æ–¹æ³•",
                    "types": ["supervised", "unsupervised", "reinforcement"],
                    "applications": ["predictions", "classifications", "clustering"]
                },
                "deep_learning": {
                    "definition": "åŸºäºå¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•",
                    "key_architectures": ["CNN", "RNN", "Transformer", "GAN"],
                    "breakthroughs": ["computer_vision", "nlp", "speech_recognition"]
                },
                "artificial_intelligence": {
                    "definition": "è®©æœºå™¨å…·å¤‡äººç±»æ™ºèƒ½èƒ½åŠ›çš„è®¡ç®—æœºç§‘å­¦åˆ†æ”¯",
                    "evolution": ["expert_systems", "machine_learning", "deep_learning", "llm_era"],
                    "capabilities": ["planning", "learning", "reasoning", "perception", "nlp"]
                }
            },
            
            "chinese_models": {
                "deepseek": {
                    "strengths": ["é•¿æ–‡æœ¬å¤„ç†", "æ•°å­¦æ¨ç†", "ä»£ç ç”Ÿæˆ"],
                    "model_versions": ["deepseek-chat", "deepseek-coder"],
                    "use_cases" : ["é•¿æ–‡æ¡£æ€»ç»“", "æ•°æ®åˆ†æ", "è½¯ä»¶å¼€å‘è¾…åŠ©"]
                },
                "zhipu": {
                    "strengths": ["ä¸­æ–‡ç†è§£", "ä¸“ä¸šçŸ¥è¯†", "æ¨ç†èƒ½åŠ›"],  
                    "model_versions": ["glm-3-turbo", "glm-4"],
                    "use_cases": ["æ•™è‚²è¾…å¯¼", "ä¸“ä¸šå’¨è¯¢", "å†…å®¹åˆ›ä½œ"]
                },
                "qwen": {
                    "strengths": ["å…¨èƒ½æ€§èƒ½", "ä¸­æ–‡ä¼˜åŒ–", "å¤šé¢†åŸŸ"],
                    "model_versions": ["qwen-7b", "qwen-72b", "qwen-turbo"],
                    "use_cases": ["é€šç”¨é—®ç­”", "å¤šè¯­è¨€ä»»åŠ¡", "ä¼ä¸šåº”ç”¨"]
                }
            },
            
            "model_comparisons": {
                "gpt_vs_claude": {
                    "gpt_strengths": ["speed", "cost", "availability"],
                    "claude_strengths": ["reasoning", "safety", "nuanced_responses"],
                    "recommendations": "æ ¹æ®å…·ä½“éœ€æ±‚å’Œèµ„æºé€‰æ‹©"
                }
            }
        }
    
    def demo_multi_tool_creation(self):
        """æ¼”ç¤ºå¤šå·¥å…·çš„æ™ºèƒ½ä½“åˆ›å»º"""
        self._log("å¤šå·¥å…·æ™ºèƒ½ä½“çš„å·¥å…·åˆ›å»º")
        print("-" * 70)
        
        print("ğŸ› ï¸ ä¸“ä¸šå·¥å…·è®¾è®¡æ€è·¯:")
        print("   â€¢ æ¯ä¸ªå·¥å…·ä¸“æ³¨è§£å†³ä¸€ç±»é—®é¢˜")
        print("   â€¢ æä¾›æ¸…æ™°çš„è‡ªç„¶è¯­è¨€æè¿°")
        print("   â€¢ æ”¯æŒé”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ")
        print("   â€¢ å¯ä»¥å¼‚æ­¥æ‰§è¡Œä»¥æ”¯æŒé«˜å¹¶å‘")
        print("   â€¢ é›†æˆç›‘æ§å’Œæ€§èƒ½ç»Ÿè®¡")
        print()
        
        print("ğŸ“‹ æœ¬è¯¾ç¨‹åˆ›å»ºçš„å·¥å…·é›†åˆ:")
        
        # 1. æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå·¥å…·
        print("\n1ï¸âƒ£ æ™ºèƒ½æ‘˜è¦å·¥å…· (Smart Summarizer):")
        
        def smart_summarize(text: str, max_length: int = 100, style: str = "concise") -> str:
            """æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨"""
            try:
                # åŸºæœ¬çš„æ–‡æœ¬å¤„ç†é€»è¾‘
                if not text or len(text.strip()) < 10:
                    return "âŒ´#x2023;#x2023;è¾“å…¥æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
                
                words = text.split()
                if len(words) <= max_length:
                    return f"âŒ´#x2023;#x2023;åŸæ–‡å·²ç¬¦åˆé•¿åº¦è¦æ±‚: {text[:200]}..."
                
                # åŸºäºæ ·å¼ç”Ÿæˆä¸åŒæ‘˜è¦
                if style == "concise":
                    # æå–å…³é”®å¥ (ç®€åŒ–ç‰ˆç®—æ³•)
                    sentences = text.split("ã€‚")
                    key_sentences = sentences[:2] if len(sentences) > 2 else sentences
                    summary = "ã€‚".join(key_sentences) + "ã€‚"
                elif style == "detailed":
                    # æ›´è¯¦ç»†çš„æ‘˜è¦
                    sentences = text.split("ã€‚")
                    key_sentences = sentences[:4] if len(sentences) > 4 else sentences  
                    summary = "ã€‚".join(key_sentences) + "ã€‚"
                else:
                    summary = text[:max_length * 5] + "..."  # ç®€å•çš„æˆªå–
                
                # è´¨é‡æ£€æŸ¥
                if len(summary) < 20:
                    summary = text[:max_length * 2] + "..."
                
                return f"âŒ´#x2023;#x2023;{style.title()}æ‘˜è¦ ({len(summary.split())} è¯):\n\n{summary}"
                
            except Exception as e:
                return f"\ x1f6ab;æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
        
        summarize_tool = Tool(
            name="smart_summarizer",
            func=smart_summarize,
            description="æ™ºèƒ½æ–‡æœ¬æ‘˜è¦å·¥å…·ã€‚è¾“å…¥: é•¿æ–‡æœ¬ï¼›è¾“å‡º: ç»“æ„åŒ–æ‘˜è¦ã€‚å‚æ•°: text (str), max_length (int, default=100), style (str, default='concise')"
        )
        
        # æµ‹è¯•æ‘˜è¦å·¥å…·
        test_doc = self.sample_documents["machine_learning_basics"]
        result = summarize_tool.run({"text": test_doc, "max_length": 50, "style": "concise"})
        print(f"   æµ‹è¯•æ‘˜è¦ç”Ÿæˆ:")
        print(f"      è¾“å…¥é•¿åº¦: {len(test_doc)} å­—ç¬¦")
        print(f"      è¾“å‡ºå‰200å­—:\\")
        print(f"      {result[:200]}...")
        
        self.tools["smart_summarizer"] = summarize_tool
        
        # 2. ä¸“ä¸šçŸ¥è¯†æ£€ç´¢å·¥å…·
        print("\n2ï¸âƒ£ ä¸“ä¸šçŸ¥è¯†æ£€ç´¢å·¥å…· (Domain Knowledge Search):")
        
        def domain_knowledge_search(topic: str, domain: str = "ai_concepts", detail_level: str = "medium") -> str:
            """é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ£€ç´¢å¼•æ“"""
            try:
                # éªŒè¯è¾“å…¥å‚æ•°
                if not topic or not topic.strip():
                    return "\ x1f6ab;éœ€è¦æä¾›æ£€ç´¢ä¸»é¢˜"
                
                topic_clean = topic.strip().lower()
                
                # æ£€æŸ¥domainæ˜¯å¦å­˜åœ¨
                if domain not in self.knowledge_base:
                    available_domains = list(self.knowledge_base.keys())
                    return f"\ x1f6ab;æœªçŸ¥é¢†åŸŸ '{domain}'ã€‚å¯ç”¨é¢†åŸŸ: {', '.join(available_domains)}"
                
                # ä¸»é¢˜åŒ¹é… (ç®€åŒ–ç‰ˆ)
                domain_data = self.knowledge_base[domain]
                matched_topic = None
                
                # éå†çŸ¥è¯†ç»“æ„è¿›è¡Œå…³é”®è¯åŒ¹é…
                for topic_key, topic_data in domain_data.items():
                    # åŸºæœ¬çš„å­—ç¬¦ä¸²åŒ¹é…
                    if topic_clean in topic_key.lower() or topic_key.lower() in topic_clean:
                        matched_topic = topic_data
                        break
                
                # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                if not matched_topic:
                    import difflib
                    available_topics = list(domain_data.keys())
                    close_matches = difflib.get_close_matches(topic_clean, available_topics, n=1, cutoff=0.6)
                    
                    if close_matches:
                        matched_topic = domain_data[close_matches[0]]
                        print(f"   â””â”€ æ¨¡ç³ŠåŒ¹é…: '{topic_clean}' â†’ '{close_matches[0]}'")
                
                if not matched_topic:
                    return f"\ x1f6ab;æœªæ‰¾åˆ°å…³äº'{topic}'çš„è¯¦ç»†ä¿¡æ¯ã€‚è¯·åœ¨{domain}é¢†åŸŸä¸­æœç´¢ä»¥ä¸‹ä¸»é¢˜: {', '.join(domain_data.keys())}"
                
                # æ ¹æ®ç»†èŠ‚çº§åˆ«ç”Ÿæˆä¸åŒå±‚æ¬¡çš„å›ç­”
                if detail_level == "basic":
                    definition = matched_topic.get("definition", "æš‚æ— å®šä¹‰")
                    result = f"ğŸ§  **{topic.title()}** åŸºç¡€ä¿¡æ¯:\\n\\n\\ ``{definition}\\ ``"
                
                elif detail_level == "medium":
                    definition = matched_topic.get("definition", "æš‚æ— å®šä¹‰")
                    key_points = matched_topic.get("key_points", [])
                    if key_points:
                        key_points_text = "\\n".join([f"â€¢ {point}" for point in key_points[:3]])
                        result = f"ğŸ§  **{topic.title()}** è¯¦ç»†ä¿¡æ¯:\\n\\n**å®šä¹‰**: {definition}\\n\\n**æ ¸å¿ƒè¦ç‚¹**:\\n{key_points_text}"
                    else:
                        result = f"ğŸ§  **{topic.title()}** è¯¦ç»†ä¿¡æ¯:\\n\\n**å®šä¹‰**: {definition}"
                
                elif detail_level == "comprehensive":
                    result_parts = [f"\U0001f9e0 **{topic.title()}** å…¨é¢ä¿¡æ¯:"]
                    
                    # definition
                    definition = matched_topic.get("definition", "æš‚æ— å®šä¹‰")
                    result_parts.append(f"**å®šä¹‰**: {definition}")
                    
                    # strengths/examples
                    if "strengths" in matched_topic:
                        strengths = matched_topic["strengths"]
                        result_parts.append(f"\\n**ä¼˜åŠ¿**:\\n" + "\\n".join([f"â€¢ {s}" for s in strengths]))
                    
                    if "examples" in matched_topic:
                        examples = matched_topic["examples"]
                        result_parts.append(f"\\n**åº”ç”¨ç¤ºä¾‹**:\\n" + "\\n".join([f"â€¢ {e}" for e in examples]))
                    
                    result = "\\n\\n".join(result_parts)
                
                else:
                    result = f"ğŸ§  **{topic.title()}** ä¿¡æ¯:\\n\\n{matched_topic.get('definition', 'è¯¦ç»†ä¿¡æ¯åŠ è½½ä¸­...')}"
                
                return result
                
            except Exception as e:
                return f"\ x1f6ab;çŸ¥è¯†æ£€ç´¢å¤±è´¥: {str(e)}"
        
        knowledge_tool = Tool(
            name="domain_knowledge_search",
            func=domain_knowledge_search,
            description="é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ£€ç´¢å¼•æ“ã€‚è¾“å…¥: topic (str), [domain: str], [detail_level: str] â†’ è¿”å›: ç»“æ„åŒ–çŸ¥è¯†ä¿¡æ¯"
        )
        
        # æµ‹è¯•çŸ¥è¯†æ£€ç´¢å·¥å…·
        test_queries = [
            {"topic": "machine learning", "domain": "ai_concepts", "detail_level": "basic"},
            {"topic": "deep learning", "domain": "ai_concepts", "detail_level": "medium"},
            {"topic": "deepseek", "domain": "chinese_models", "detail_level": "comprehensive"}
        ]
        
        for query in test_queries:
            result = knowledge_tool.run(query)
            print(f"   \\ æ£€ç´¢ `{query['topic']}` ({query['detail_level']}) ç»“æœ:")
            print(f"      {result[:200]}...")
            print()
        
        self.tools["domain_knowledge_search"] = knowledge_tool
        
        # 3. è¯­è¨€æ–‡æœ¬å¤„ç†å·¥å…·
        print("\n3ï¸âƒ£ è¯­è¨€æ–‡æœ¬å¤„ç†å·¥å…· (Text Processing)")
        
        def text_analyzer(text: str, analysis_type: str = "all") -> str:
            """æ–‡æœ¬åˆ†æå’Œå¤„ç†å·¥å…·"""
            try:
                if not text or not text.strip():
                    return "\ x1f6ab;åˆ†ææ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                
                text_clean = text.strip()
                analysis_types = {
                    "basic": lambda: {
                        "length": len(text_clean),
                        "words": len(text_clean.split()),
                        "sentences": len([s for s in text_clean.split("ã€‚") if s.strip()]),
                        "char_types": len(set(text_clean))
                    },
                    
                    "advanced": lambda: {
                        **analysis_types["basic"](),
                        "avg_word_length": sum(len(word) for word in text_clean.split()) / len(text_clean.split()),
                        "punctuation_count": sum(1 for char in text_clean if not char.isalnum()),
                        "uppercase_ratio": sum(1 for char in text_clean if char.isupper()) / len(text_clean) if text_clean else 0
                    },
                    
                    "keywords": lambda: {
                        **analysis_types["basic"](),
                        "top_words": sorted(set(word for word in text_clean.lower().split() if len(word) > 2), key=lambda w: text_clean.count(w), reverse=True)[:5],
                        "unique_words": len(set(word.lower() for word in text_clean.split())),
                        "terms": [word for word in text_clean.lower().split() if len(word) > 3][:8]
                    }
                }
                
                # æ‰§è¡Œåˆ†æ
                if analysis_type not in analysis_types and analysis_type != "all":
                    return f"\ x1f6ab;æœªçŸ¥åˆ†æç±»å‹ '{analysis_type}'ã€‚å¯ç”¨ç±»å‹: {', '.join(analysis_types.keys())}"
                
                if analysis_type == "all":
                    # æ‰§è¡Œæ‰€æœ‰åˆ†æç±»å‹ (å¯¹äºallç±»å‹ï¼Œä¼˜å…ˆç»™ keywords åˆ†æç»“æœ)
                    result_data = analysis_types["keywords"]()
                    main_type = "å…³é”®è¯"
                else:
                    result_data = analysis_types[analysis_type]()
                    main_type = analysis_type.replace("_", " ").title()
                
                # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
                output_lines = [f"ğŸ§  `{main_type}` åˆ†æç»“æœ"]
                
                for key, value in result_data.items():
                    if isinstance(value, float):
                        formatted_value = f"{value:.2f}"
                    elif isinstance(value, list):
                        if len(value) > 0 and isinstance(value[0], str):
                            formatted_value = ", ".join(f"'{v}'" for v in value[:5])
                        else:
                            formatted_value = str(value)
                    else:
                        formatted_value = str(value)
                    
                    # ç¾åŒ–é”®åæ˜¾ç¤º
                    display_key = key.replace("_", " ").title()
                    output_lines.append(f"**{display_key}**: {formatted_value}")
                
                return "\\n".join(output_lines)
                
            except Exception as e:
                return f"\ x1f6ab;æ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}"
        
        text_tool = Tool(
            name="text_analyzer",
            func=text_analyzer,
            description="è¯­è¨€æ–‡æœ¬åˆ†æå’Œå¤„ç†å·¥å…·ã€‚è¾“å…¥: text (str), [analysis_type: str] â†’ è¿”å›: ç»“æ„åŒ–çš„æ–‡æœ¬åˆ†ææŠ¥å‘Š"
        )
        
        # æµ‹è¯•æ–‡æœ¬åˆ†æå·¥å…·
        demo_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å¿«é€Ÿå‘å±•"
        result = text_tool.run({"text": demo_text, "analysis_type": "basic"})
        print(f"   æ–‡æœ¬åˆ†ææµ‹è¯•:")
        print(f"      è¾“å…¥: '{demo_text}'")
        print(f"      åŸºç¡€åˆ†æç»“æœ:\\")
        print(f"      {result}")
        
        self.tools["text_analyzer"] = text_tool
        
        print(f"\nğŸ“Š å¤šå·¥å…·åˆ›å»ºç»Ÿè®¡:")
        print(f"   âœ… ä¸“ä¸šå·¥å…·æ€»æ•°: {len(self.tools)} ä¸ª")
        for tool_name in self.tools.keys():
            print(f"     â€¢ {tool_name}")
    
    def demo_china_models_agent(self):
        """æ¼”ç¤ºä¸­å›½AIæ¨¡å‹åœ¨Agentä¸­çš„é›†æˆ"""
        self._log("ä¸­å›½AIæ¨¡å‹åœ¨Agentä¸­çš„é›†æˆ")
        print("-" * 70)
        
        print("ğŸ‡¨ğŸ‡³ ä¸­å›½AIå¤§æ¨¡å‹Agentæ”¯æŒ:")
        print("   â€¢ DeepSeek: é•¿æœ¬æ–‡å¤„ç†ã€æ•°å­¦æ¨ç†ä¸“å®¶")
        print("   â€¢ æ™ºè°±æ¾„ GLM: ä¸­æ–‡ç†è§£ã€ä¸“ä¸šçŸ¥è¯†å¼º")
        print("   â€¢ é€šä¹‰åƒé—®: å…¨èƒ½æ€§èƒ½ã€ä¸­æ–‡ä¼˜åŒ–")
        print("   â€¢ Kimi: åˆ›æ„ç”Ÿæˆã€å¯¹è¯æµç•…")
        print("   â€¢ Baidu ERNIE: æ–‡å­¦åˆ›ä½œã€å•†ä¸šæ–‡æ¡ˆ")
        print()
        
        class ChinaModelAdapter:
            """\U0001f1e8\U0001f1f3 ä¸­å›½å¤§æ¨¡å‹ç»Ÿä¸€é€‚é…å™¨"""
            
            def __init__(self, config: ChinaModelConfig):
                self.config = config
                self.name = f"{config.provider}_{config.model_name.replace('-', '_')}"
                self._validate_config()
            
            def _validate_config(self):
                """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
                if not self.config.api_key:
                    raise ValueError(f"\U0001faf6; {self.config.provider} API key is required")
                
                if not self.config.model_name:
                    self.config.model_name = self._get_default_model()
            
            def _get_default_model(self) -> str:
                """è·å–å„æä¾›å•†çš„é»˜è®¤æ¨¡å‹"""
                defaults = {
                    "deepseek": "deepseek-chat",
                    "zhipu": "glm-4", 
                    "moonshot": "moonshot-v1-8k",
                    "qwen": "qwen-turbo"
                }
                return defaults.get(self.config.provider.lower(), "unknown")
            
            def _create_model_instance(self) -> BaseLanguageModel:
                """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
                provider = self.config.provider.lower()
                
                if provider == "deepseek":
                    return DeepSeekLangChainAdapter(
                        api_key=self.config.api_key,
                        base_url=self.config.base_url or "https://api.deepseek.com/v1",
                        model=self.config.model_name,
                        temperature=self.config.temperature,
                        max_tokens=self.config.max_tokens
                    )
                
                elif provider == "zhipu":
                    return ZhipuLangChainAdapter(
                        api_key=self.config.api_key,
                        model=self.config.model_name,
                        temperature=self.config.temperature,
                        max_tokens=self.config.max_tokens
                    )
                
                elif provider == "qwen": 
                    return QwenLangChainAdapter(
                        api_key=self.config.api_key,
                        model=self.config.model_name,
                        temperature=self.config.temperature
                    )
                
                else:
                    # é€šç”¨é€‚é…å™¨
                    return GenericChinaModelAdapter(**self.config.__dict__)
            
            def to_tool(self) -> Tool:
                """è½¬æ¢ä¸ºLangChainå¯ç”¨çš„Toolæ ¼å¼"""
                model_instance = self._create_model_instance()
                
                def _model_tool_function(prompt: str) -> str:
                    """æ¨¡å‹è°ƒç”¨å‡½æ•°"""
                    from langchain_core.messages import HumanMessage
                    
                    try:
                        messages = [HumanMessage(content=prompt)]
                        response = model_instance.invoke(messages)
                        return response.content
                        
                    except Exception as e:
                        return f"\U0001f534; ä¸­å›½æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"
                
                return Tool(
                    name=self.name,
                    func=_model_tool_function,
                    description=f"ä¸­å›½{self.config.provider.upper()}æ¨¡å‹ {self.config.model_name} è°ƒç”¨å·¥å…·. æ“…é•¿ä¸­æ–‡ç†è§£å’Œä¸“ä¸šçŸ¥è¯†è§£ç­”ã€‚"
                )
        
        # ä¸­å›½æ¨¡å‹é€‚é…å™¨çš„å…·ä½“å®ç° (ç®€åŒ–ç‰ˆ)
        class DeepSeekLangChainAdapter:
            """DeepSeek LangChainé€‚é…å™¨"""
            
            def __init__(self, api_key: str, base_url: str = None, model: str = "deepseek-chat", **kwargs):
                self.api_key = api_key
                self.base_url = base_url or "https://api.deepseek.com/v1"
                self.model = model
                self.kwargs = kwargs
            
            def invoke(self, messages):
                """æ¨¡æ‹ŸDeepSeekæ¨¡å‹è°ƒç”¨"""
                return self._mock_deepseek_response(messages)
            
            def _mock_deepseek_response(self, messages):
                """æ¨¡æ‹ŸDeepSeekå“åº”"""
                import time
                time.sleep(0.1)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                
                # åŸºäºè¾“å…¥ç”Ÿæˆç›¸å…³å“åº”
                last_message = messages[-1].content if messages else "Hello"
                
                responses = {
                    "ç¼–ç¨‹": "DeepSeekåœ¨ç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°çªå‡º: \( \"role\": \\"assistant\\", \\"content\\": 'æˆ‘æ¥å¸®ä½ åˆ†æè¿™ä¸ªPythonä»£ç çš„å®ç°...'",
                    "æ•°å­¦": "DeepSeekæ•°å­¦æ¨ç†èƒ½åŠ›å±•ç¤º: \( \"role\": \\"assistant\\", \\"content\\": 'è¿™ä¸ªé—®é¢˜çš„æ•°å­¦è§£æ³•åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥...'",
                    "æ–‡æœ¬": "DeepSeeké•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›: \( \"role\": \\"assistant\\", \\"content\\": 'è¿™æ®µè¯çš„ä¸»è¦æ„æ€æ˜¯...'"
                }
                
                # å…³é”®è¯åŒ¹é…
                content_indicator = "general"
                for keyword in responses:
                    if keyword in last_message.lower():
                        content_indicator = keyword
                        break
                
                base_response = responses.get(content_indicator, f"DeepSeekå›åº”: '{last_message[:50]}...' æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„é—®é¢˜ã€‚è®©æˆ‘ä»ä¸“ä¸šè§’åº¦ä¸ºæ‚¨è§£é‡Š...")
                
                # è¿”å›Reu2023;é£æ ¼çš„å¯¹è±¡
                class MockResponse:
                    content = base_response
                    usage = {"prompt_tokens": len(last_message), "completion_tokens": len(base_response)}
                
                return MockResponse()
        
        class ZhipuLangChainAdapter:
            """\U0001f1e8\U0001f1f3 æ™ºè°±GLM LangChainé€‚é…å™¨"""
            
            def invoke(self, messages):
                """æ¨¡æ‹Ÿæ™ºè°±GLMå“åº”"""
                return self._mock_zhipu_response(messages)
            
            def _mock_zhipu_response(self, messages):
                """æ¨¡æ‹Ÿæ™ºè°±å“åº”"""
                import random
                
                last_message = messages[-1].content if messages else "Hello"
                
                responses = [
                    "\U0001f9ec; æ™ºè°±GLM: è¿™ä¸ªé—®é¢˜ä½“ç°äº†ä¸­è¥¿æ–¹æ€ç»´æ–¹å¼çš„å·®å¼‚...",
                    "\U0001f9e9; æ™ºè°±GLM: ä»å“²å­¦è§’åº¦æ¥çœ‹è¿™ä¸ªç°è±¡...", 
                    "\U0001f9e8; æ™ºè°±GLM: è®©æˆ‘ç”¨æ›´åŠ ç»“æ„åŒ–çš„æ–¹å¼æ¥è§£ç­”..."
                ]
                
                if any(kw in last_message.lower() for kw in ["ä¸­æ–‡", "æ±‰è¯­", "chinese"]):
                    response = "æ™ºè°±GLM: ä¸­æ–‡ä½œä¸ºä¸–ç•Œä¸Šä½¿ç”¨äººæ•°æœ€å¤šçš„è¯­è¨€ä¹‹ä¸€ï¼Œå®ƒåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ‰¿è½½ç€ç‰¹æ®Šçš„ä½¿å‘½..."
                else:
                    response = random.choice(responses)
                
                class MockResponse:
                    content = response
                    usage = {"prompt_tokens": 80, "completion_tokens": 120}
                
                return MockResponse()
        
        class GenericChinaModelAdapter:
            """\U0001f1e8\U0001f1f3 ä¸­å›½å¤§æ¨¡å‹é€šç”¨é€‚é…å™¨"""
            
            def __init__(self, provider: str, api_key: str, **kwargs):
                self.provider = provider
                self.api_key = api_key
                self.kwargs = kwargs
            
            def invoke(self, messages):
                """é€šç”¨æ¨¡å‹è°ƒç”¨"""
                last_message = messages[-1].content if messages else "Hello"
                
                provider_specific_responses = {
                    "moonshot": "Kimi: æˆ‘å¯ä»¥å¸®æ‚¨å¤„ç†è¿™ä¸ªé•¿æ–‡æ¡£ï¼Œè®©æˆ‘æ¥åˆ†æå…¶ä¸­çš„å…³é”®ä¿¡æ¯...",
                    "baichuan": "ç™¾å·: åŸºäºæˆ‘çš„æ•°æ®åˆ†æèƒ½åŠ›ï¼Œè¿™ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ˜¯...",
                    "qwen": "é€šä¹‰åƒé—®: æœ‰å¾ˆå…¨é¢çš„é—®é¢˜è§†è§’ï¼Œè®©æˆ‘ä»å¤šä¸ªæ–¹é¢æ¥å›ç­”..."
                }
                
                response = provider_specific_responses.get(
                    self.provider.lower(),
                    f"{self.provider.upper()}: æˆ‘æ¥åˆ†ææ‚¨çš„é—®é¢˜ '{last_message[:20]}...'"