#!/usr/bin/env python3
"""
LangChain L1 Foundation - Week 2
è¯¾ç¨‹æ ‡é¢˜: èŠå¤©æ¨¡å‹åŸºç¡€ä¸å¤šæ¨¡å‹å¯¹æ¯”
å­¦ä¹ ç›®æ ‡:
  - ç†è§£èŠå¤©æ¨¡å‹(Chat Models)çš„åŸºæœ¬æ¦‚å¿µ
  - å­¦ä¼šé…ç½®å’Œä½¿ç”¨ä¸åŒçš„LLMæä¾›å•†
  - æŒæ¡æ¸©åº¦å‚æ•°(temperature)ç­‰å…³é”®é…ç½®
  - å­¦ä¼šå¤„ç†æ¨¡å‹å“åº”å’Œé”™è¯¯å¤„ç†
  - å®è·µåŸºç¡€çš„æ¨¡å‹äº¤äº’è„šæœ¬
ä½œè€…: Claude Code æ•™å­¦å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2024-01-16
ç‰ˆæœ¬: 1.0.0
å…ˆå†³æ¡ä»¶: å®ŒæˆWeek 1é“¾å¼ç¼–ç¨‹åŸºç¡€å­¦ä¹ 

ğŸ¯ å®è·µé‡ç‚¹:
  - çœŸå®APIé›†æˆ
  - æ¨¡å‹å¯¹æ¯”æµ‹è¯•
  - å‚æ•°è°ƒä¼˜å®è·µ
  - é”™è¯¯å¤„ç†æœºåˆ¶
"""

import sys
import os
import time
from typing import Dict, List, Optional, Union
from datetime import datetime
from dataclasses import dataclass
from pathlib import Path

# å¿…éœ€çš„ç¯å¢ƒå˜é‡åŠ è½½
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡å·²åŠ è½½")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè¯·ç¡®ä¿æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡")

# LangChainæ ¸å¿ƒå¯¼å…¥
try:
    from langchain_openai import ChatOpenAI
    from langchain_anthropic import ChatAnthropic
    from langchain_core.messages import HumanMessage, SystemMessage
    from langchain_core.language_models import BaseLanguageModel
    print("âœ… LangChainæ¨¡å‹ç›¸å…³ç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ LangChainæ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š")
    print("   pip install langchain-openai langchain-anthropic")
    sys.exit(1)

@dataclass
class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”ç»“æœ"""
    provider: str
    model_name: str
    success: bool
    response_text: str
    latency: float
    error_message: Optional[str] = None
    token_usage: Optional[Dict] = None

@dataclass  
class ModelConfig:
    """æ¨¡å‹é…ç½®å‚æ•°"""
    name: str
    temperature: float = 0.7
    max_tokens: int = 1000
    timeout_seconds: int = 30
    api_key_name: str = "OPENAI_API_KEY"

class ChatModelTrainer:
    """L1èŠå¤©æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.test_questions = [
            "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ç»™æˆ‘æ¨èå­¦ä¹ Pythonçš„3ä¸ªç†ç”±",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Œå®ƒå¯¹AIå‘å±•æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ"
        ]
        self.model_configs = {
            "gpt-3.5-turbo": ModelConfig(
                "GPT-3.5 Turbo", 
                temperature=0.7, 
                api_key_name="OPENAI_API_KEY"
            ),
            "gpt-4": ModelConfig(
                "GPT-4",
                temperature=0.7,
                api_key_name="OPENAI_API_KEY"
            ),
            "claude-3-sonnet": ModelConfig(
                "Claude 3 Sonnet",
                temperature=0.7,
                api_key_name="ANTHROPIC_API_KEY"
            )
        }
    
    def _log(self, message: str):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"ğŸ“š {message}")
    
    def validate_api_credentials(self) -> Dict[str, bool]:
        """éªŒè¯APIå¯†é’¥é…ç½®"""
        self._log("éªŒè¯APIå¯†é’¥é…ç½®")
        print("-" * 50)
        
        api_status = {}
        providers = [
            ("OpenAI", "OPENAI_API_KEY"),
            ("Anthropic", "ANTHROPIC_API_KEY"),
            ("ä¸­å›½æ¨¡å‹æ”¯æŒ", "DEEPSEEK_API_KEY,ZHIPU_API_KEY")
        ]
        
        for provider_name, key_names in providers:
            keys = key_names.split(",")
            
            if len(keys) == 1:
                api_key = os.getenv(keys[0])
                status = api_key is not None and len(api_key) > 10
                api_status[provider_name] = status
            else:
                # æ£€æŸ¥å¤šä¸ªå¯†é’¥ï¼Œä»»æ„ä¸€ä¸ªæœ‰æ•ˆå³ç®—é…ç½®æˆåŠŸ
                any_key_valid = any(
                    os.getenv(key) is not None and len(os.getenv(key, "")) > 10 
                    for key in keys
                )
                api_status[provider_name] = any_key_valid
        
        # è¯¦ç»†æŠ¥å‘Š
        for provider, status in api_status.items():
            if status:
                print(f"   âœ… {provider}: APIå¯†é’¥å·²é…ç½® âœ“")
            else:
                print(f"   âŒ {provider}: APIå¯†é’¥æœªé…ç½® âœ—")
        
        available_providers = [p for p, s in api_status.items() if s]
        if available_providers:
            print(f"   ğŸ“Š å¯ç”¨æ¨¡å‹æä¾›å•†: {', '.join(available_providers)}")
        else:
            print("   âš ï¸  å½“å‰æ— å¯ç”¨çš„æ¨¡å‹APIé…ç½®")
        
        return api_status
    
    def demo_gpt_model_basics(self) -> Optional[ChatOpenAI]:
        """æ¼”ç¤ºGPTæ¨¡å‹åŸºç¡€ä½¿ç”¨"""
        self._log("GPTæ¨¡å‹åŸºç¡€ä½¿ç”¨æ¼”ç¤º")
        print("-" * 50)
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("   âš ï¸  OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡GPTæ¨¡å‹æ¼”ç¤º")
            return None
        
        try:
            # åŸºç¡€æ¨¡å‹åˆå§‹åŒ–
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.7,
                max_tokens=150,
                timeout=30,
                max_retries=2
            )
            
            print(f"ğŸ¤– æ¨¡å‹ä¿¡æ¯:")
            print(f"   â””â”€ æ¨¡å‹: gpt-3.5-turbo")
            print(f"   â””â”€ æ¸©åº¦: 0.7")
            print(f"   â””â”€ æœ€å¤§token: 150")
            print(f"   â””â”€ è¶…æ—¶æ—¶é—´: 30ç§’")
            print()
            
            # åŸºç¡€æ¨¡å‹è°ƒç”¨æ¼”ç¤º
            test_message = "ä½ å¥½ï¼æˆ‘æ˜¯LangChainå­¦ä¹ è€…ï¼Œè¯·ç”¨ä¸­æ–‡å‹å¥½åœ°å›åº”æˆ‘ã€‚"
            
            print(f"ğŸ“¨ å‘é€æ¶ˆæ¯: {test_message}")
            print(f"   â”œâ”€ ç±»å‹: HumanMessage")
            print(f"   â””â”€ é•¿åº¦: {len(test_message)} å­—ç¬¦")
            
            # æ„å»ºæ¶ˆæ¯å¯¹è±¡
            messages = [HumanMessage(content=test_message)]
            
            # å‘é€è¯·æ±‚ï¼ˆå¸¦è®¡æ—¶ï¼‰
            start_time = time.time()
            
            try:
                response = llm.invoke(messages)
                latency = time.time() - start_time
                
                print(f"\nâœ… æ”¶åˆ°å“åº” (è€—æ—¶: {latency:.2f}ç§’):")
                print(f"   â””â”€ å†…å®¹: {response.content}")
                
                if hasattr(response, 'usage'):
                    print(f"   â””â”€ Tokenä½¿ç”¨: {response.usage}")
                
            except Exception as e:
                print(f"\nâŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
                return None
            
            print(f"\nğŸ¯ æ€»ç»“:")
            print(f"   â”œâ”€ æ¨¡å‹: {llm.model_name}")
            print(f"   â”œâ”€ å“åº”: æˆåŠŸæ¥æ”¶åˆ°æ¨¡å‹å›å¤")
            print(f"   â””â”€ å»¶è¿Ÿ: {latency:.2f}ç§’")
            
            return llm
            
        except Exception as e:
            print(f"   âŒ GPTæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return None
    
    def demo_temperature_parameter(self, model: Optional[ChatOpenAI] = None):
        """æ¼”ç¤ºæ¸©åº¦å‚æ•°çš„å½±å“"""
        self._log("æ¸©åº¦å‚æ•°(Temperature)çš„å½±å“æ¼”ç¤º")
        print("-" * 50)
        
        if not model:
            print("âš ï¸  æ— å¯ç”¨æ¨¡å‹ï¼Œæ­¤æ¼”ç¤ºéœ€è¦OpenAI APIè®¿é—®")
            return
        
        print("ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°è¯´æ˜:")
        print("   â€¢ Temperature = 0.0  : ç¡®å®šæ€§æœ€å¼ºï¼Œå¯é‡ç°")
        print("   â€¢ Temperature = 0.7  : å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§ (æ¨èé»˜è®¤)")
        print("   â€¢ Temperature = 1.2+ : åˆ›é€ æ€§æ›´å¼ºï¼Œä½†å¯èƒ½ä¸å‡†ç¡®")
        print()
        
        # æµ‹è¯•ä¸åŒæ¸©åº¦å€¼çš„æ¨¡å‹æ€§èƒ½
        test_prompt = "åˆ›æ„å†™ä½œï¼šä»¥"å­¤ç‹¬"ä¸ºä¸»é¢˜å†™ä¸€æ®µä¸­æ–‡æ•£æ–‡ï¼Œ50-80å­—"
        temperatures = [0.2, 0.7, 1.2]
        
        print(f"ğŸ§ª æµ‹è¯•æç¤ºè¯: {test_prompt[:50]}...")
        print("\nä¸åŒæ¸©åº¦å€¼çš„å“åº”å¯¹æ¯”:")
        
        for temp in temperatures:
            print(f"\n   ğŸŒ¡ï¸ Temperature = {temp}:")
            
            # åˆ›å»ºç‰¹å®šæ¸©åº¦çš„æ¨¡å‹ï¼ˆåŸºäºç°æœ‰æ¨¡å‹å…‹éš†é…ç½®ï¼‰
            temp_model = ChatOpenAI(
                model=model.model_name,
                temperature=temp,
                max_tokens=120,
                timeout=30
            )
            
            try:
                messages = [HumanMessage(content=test_prompt)]
                response = temp_model.invoke(messages)
                
                print(f"      â””â”€ {response.content[:60]}...")
                print(f"      â””â”€ è¾“å‡ºé•¿åº¦: {len(response.content)} å­—ç¬¦")
                
            except Exception as e:
                print(f"      âŒ è°ƒç”¨å¤±è´¥: {str(e)}")
        
        print(f"\nğŸ’¡ æ¸©åº¦å‚æ•°é€‰æ‹©å»ºè®®:")
        print("   â€¢ äº‹å®æ€§é—®ç­”: temperature = 0.0-0.3")
        print("   â€¢ åˆ›æ„å†™ä½œ: temperature = 0.8-1.2")
        print("   â€¢ ä»£ç ç”Ÿæˆ: temperature = 0.2-0.5")
        print("   â€¢ ä¸€èˆ¬èŠå¤©: temperature = 0.7 (é»˜è®¤)")
    
    def demo_multiple_providers_comparison(self) -> List[ModelComparison]:
        """å¤šæ¨¡å‹æä¾›å•†å¯¹æ¯”æµ‹è¯•"""
        self._log("å¤šæ¨¡å‹æä¾›å•†æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("-" * 50)
        
        comparisons = []
        test_prompt = "ç®€è¿°æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µï¼Œä¸è¶…è¿‡100å­—"
        
        print("ğŸ¤– å‡†å¤‡æµ‹è¯•çš„æ¨¡å‹:")
        print("   â€¢ OpenAI GPT-3.5-turbo")
        print("   â€¢ OpenAI GPT-4 (å¦‚æœå¯ç”¨)")
        print("   â€¢ Anthropic Claude-3-sonnet (å¦‚æœå¯ç”¨)")
        print()
        
        # GPT-3.5 Turboæµ‹è¯•
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            print("ğŸ§ª æµ‹è¯• GPT-3.5-turbo...")
            try:
                gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=150)
                start_time = time.time()
                
                response = gpt35.invoke([HumanMessage(content=test_prompt)])
                latency = time.time() - start_time
                
                comparison = ModelComparison(
                    provider="OpenAI",
                    model_name="gpt-3.5-turbo", 
                    success=True,
                    response_text=response.content,
                    latency=latency
                )
                
                print(f"   âœ… GPT-3.5-turbo: {latency:.2f}ç§’")
                print(f"      â””â”€ {response.content[:40]}...")
                comparisons.append(comparison)
                
            except Exception as e:
                print(f"   âŒ GPT-3.5-turboå¤±è´¥: {str(e)}")
                comparisons.append(ModelComparison(
                    provider="OpenAI",
                    model_name="gpt-3.5-turbo",
                    success=False,
                    response_text="",
                    latency=0.0,
                    error_message=str(e)
                ))
        
        # GPT-4æµ‹è¯• (é«˜çº§æ¨¡å‹)
        if openai_key:
            print("ğŸ§ª æµ‹è¯• GPT-4...")
            try:
                gpt4 = ChatOpenAI(model="gpt-4", temperature=0.7, max_tokens=150)
                start_time = time.time()
                
                response = gpt4.invoke([HumanMessage(content=test_prompt)])
                latency = time.time() - start_time
                
                comparison = ModelComparison(
                    provider="OpenAI",
                    model_name="gpt-4",
                    success=True,
                    response_text=response.content,
                    latency=latency
                )
                
                print(f"   âœ… GPT-4: {latency:.2f}ç§’")
                print(f"      â””â”€ {response.content[:40]}...")
                comparisons.append(comparison)
                
            except Exception as e:
                print(f"   âš ï¸ GPT-4æµ‹è¯•å¤±è´¥: {str(e)} (å¯èƒ½ä¸é…é¢æˆ–æƒé™ç›¸å…³)")
        
        # Claudeæµ‹è¯•
        claude_key = os.getenv("ANTHROPIC_API_KEY")
        if claude_key:
            print("ğŸ§ª æµ‹è¯• Claude-3-sonnet...")
            try:
                claude = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0.7, max_tokens=150)
                start_time = time.time()
                
                response = claude.invoke([HumanMessage(content=test_prompt)])
                latency = time.time() - start_time
                
                comparison = ModelComparison(
                    provider="Anthropic",
                    model_name="claude-3-sonnet", 
                    success=True,
                    response_text=response.content,
                    latency=latency
                )
                
                print(f"   âœ… Claude-3-sonnet: {latency:.2f}ç§’")
                print(f"      â””â”€ {response.content[:40]}...")
                comparisons.append(comparison)
                
            except Exception as e:
                print(f"   âŒ Claude-3-sonnetå¤±è´¥: {str(e)}")
                comparisons.append(ModelComparison(
                    provider="Anthropic",
                    model_name="claude-3-sonnet",
                    success=False,
                    response_text="",
                    latency=0.0,
                    error_message=str(e)
                ))
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._print_model_comparison_report(comparisons)
        
        return comparisons
    
    def demo_model_response_handling(self, models: List[ModelComparison] = None):
        """æ¼”ç¤ºæ¨¡å‹å“å¤„ç†å’Œå®¹é”™æœºåˆ¶"""
        self._log("æ¨¡å‹å“åº”å¤„ç†ä¸å®¹é”™æœºåˆ¶")
        print("-" * 50)
        
        if not models:
            # ä½¿ç”¨å†…ç½®çš„æ¨¡å‹ç±»å‹æ¼”ç¤º
            print("âœ¨ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå“åº”å¤„ç†æ¦‚å¿µ")
            
            demo_comparisons = [
                ModelComparison("OpenAI", "gpt-3.5-turbo", True, "æ­£å¸¸å“åº”å†…å®¹", 1.2),
                ModelComparison("Error Test", "error-model", False, "", 0.0, "ç½‘ç»œè¿æ¥è¶…æ—¶"),
                ModelComparison("Rate Limit", "limit-model", False, "", 0.0, "APIè°ƒç”¨é¢‘ç‡è¶…é™")
            ]
            models = demo_comparisons
        
        print("ğŸ›¡ï¸  å“åº”å¤„ç†æœ€ä½³å®è·µ:")
        print()
        
        # æ¼”ç¤ºä¸åŒæƒ…å†µçš„å¤„ç†æ–¹å¼
        for comparison in models:
            print(f"ğŸ“ æ¨¡å‹: {comparison.provider} / {comparison.model_name}")
            
            if comparison.success:
                print(f"   âœ… æˆåŠŸ")
                print(f"   ğŸ“Š å“åº”é•¿åº¦: {len(comparison.response_text)} å­—ç¬¦")
                print(f"   â±  å“åº”æ—¶é—´: {comparison.latency:.2f}ç§’")
                
                # æ¼”ç¤ºå“åº”å¦‚ä½•è¿›è¡Œåå¤„ç†
                if len(comparison.response_text) > 20:
                    print(f"   ğŸ“ æ‘˜è¦: {comparison.response_text[:20]}...")
                
            else:
                print(f"   âŒ å¤±è´¥: {comparison.error_message}")
                
                # æ¼”ç¤ºé”™è¯¯åˆ†ç±»ä¸å¤„ç†ç­–ç•¥
                error_type = self._classify_error(comparison.error_message)
                print(f"   ğŸ·  é”™è¯¯ç±»å‹: {error_type}")
                
                # å¯¹åº”çš„å¤„ç†å»ºè®®
                if error_type == "network":
                    print(f"   ğŸ”§ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œé‡è¯•è¿æ¥")
                elif error_type == "rate_limit":
                    print(f"   ğŸ”§ å»ºè®®: å¢åŠ é‡è¯•é—´éš”ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿")
                elif error_type == "authentication":
                    print(f"   ğŸ”§ å»ºè®®: éªŒè¯APIå¯†é’¥æ­£ç¡®æ€§ï¼Œæ£€æŸ¥é…é¢")
                else:
                    print(f"   ğŸ”§ å»ºè®®: è¯¦ç»†æŸ¥çœ‹APIæ–‡æ¡£ï¼Œè”ç³»æŠ€æœ¯æ”¯æŒ")
            
            print()
        
        # æ¼”ç¤ºé€šç”¨çš„é”™è¯¯å¤„ç†æ¨¡å¼
        print("ğŸ”„ é€šç”¨é”™è¯¯å¤„ç†æ¨¡å¼:")
        print("   â”œâ”€ try/except åŒ…å›´æ‰€æœ‰æ¨¡å‹è°ƒç”¨")
        print("   â”œâ”€ é‡è¯•æœºåˆ¶ (æŒ‡æ•°é€€é¿æ¨è)")
        print("   â”œâ”€ é”™è¯¯åˆ†ç±»ä¸æ—¥å¿—è®°å½•")
        print("   â”œâ”€ ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯")
        print("   â””â”€ é™çº§å¤„ç† (fallback models)")
    
    def _classify_error(self, error_message: str) -> str:
        """é”™è¯¯åˆ†ç±»å·¥å…·"""
        error_lower = error_message.lower()
        
        if any(word in error_lower for word in ["timeout", "connection", "network"]):
            return "network"
        elif any(word in error_lower for word in ["rate limit", "too many requests"]):
            return "rate_limit" 
        elif any(word in error_lower for word in ["authentication", "api key", "unauthorized"]):
            return "authentication"
        else:
            return "other"
    
    def _print_model_comparison_report(self, comparisons: List[ModelComparison]):
        """æ‰“å°æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“Š æ¨¡å‹å¯¹æ¯”æ€§èƒ½æŠ¥å‘Š")
        print("=" * 40)
        
        if not comparisons:
            print("   âš ï¸  æ²¡æœ‰å¯ä¾›å¯¹æ¯”çš„æ¨¡å‹æ•°æ®")
            return
        
        successful_comparisons = [c for c in comparisons if c.success]
        
        if successful_comparisons:
            print(f"ğŸ¤– æˆåŠŸæµ‹è¯•çš„æ¨¡å‹æ•°é‡: {len(successful_comparisons)}")
            print()
            
            # æ€§èƒ½å¯¹æ¯”
            print("ğŸ“ˆ å“åº”æ€§èƒ½å¯¹æ¯”:")
            for comparison in successful_comparisons:
                print(f"   â€¢ {comparison.provider} - {comparison.model_name}: {comparison.latency:.3f}s")
            
            print()
            
            # å“åº”è´¨é‡å¯¹æ¯” (ç®€è¦å±•ç¤º)
            print("ğŸ¯ å“åº”è´¨é‡å¯¹æ¯”:")
            for i, comparison in enumerate(successful_comparisons, 1):
                print(f"   {i}. {comparison.provider} [{comparison.model_name}]:")
                print(f"      â””â”€ {comparison.response_text[:60]}...")
                print(f"      â””â”€ å“åº”é•¿åº¦: {len(comparison.response_text)} å­—ç¬¦")
        
        failed_comparisons = [c for c in comparisons if not c.success]
        if failed_comparisons:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥çš„æ¨¡å‹ ({len(failed_comparisons)}):")
            for comparison in failed_comparisons:
                print(f"   â€¢ {comparison.provider} - {comparison.model_name}")
                print(f"     â””â”€ å¤±è´¥åŸå› : {comparison.error_message}")
        
        # æ€»ä½“å»ºè®®
        print(f"\nğŸ’¡ æ¨¡å‹é€‰æ‹©å»ºè®®:")
        if successful_comparisons:
            fastest = min(successful_comparisons, key=lambda x: x.latency)
            print(f"   â€¢ æ¨èå¿«é€Ÿå“åº”åœºæ™¯: {fastest.provider} - {fastest.model_name}")
            
            if any(c.provider == "OpenAI" and "gpt-4" in c.model_name for c in successful_comparisons):
                print(f"   â€¢ æ¨èé«˜è´¨é‡å“åº”åœºæ™¯: OpenAI GPT-4 (å¦‚æœ‰é…é¢)")
            
            print(f"   â€¢ ä»·æ ¼è€ƒsubspeciesæ¨¡å‹: OpenAI GPT-3.5-turbo")
    
    def generate_week2_summary(self) -> str:
        """ç”ŸæˆWeek 2å­¦ä¹ æ€»ç»“"""
        summary = f"""
ğŸ“ L1 Foundation - Week 2: èŠå¤©æ¨¡å‹åŸºç¡€å­¦ä¹ æ€»ç»“
===================================================

âœ… æœ¬å‘¨å®Œæˆå­¦ä¹ å†…å®¹:
   1. APIå¯†é’¥é…ç½®å’ŒéªŒè¯æ£€æŸ¥
   2. GPTæ¨¡å‹çš„åŸºç¡€ä½¿ç”¨æ–¹æ³•
   3. æ¸©åº¦å‚æ•°çš„ä½œç”¨å’Œå½±å“æ¼”ç¤º
   4. å¤šæ¨¡å‹æä¾›å•†çš„å¯¹æ¯”æµ‹è¯•
   5. æ¨¡å‹å“åº”å¤„ç†å’Œé”™è¯¯åˆ†ç±»

ğŸ’¡ æ ¸å¿ƒæŠ€èƒ½æŒæ¡:
   â€¢ èŠå¤©æ¨¡å‹(Chat Models)çš„åŸºæœ¬æ¦‚å¿µå’Œä½¿ç”¨
   â€¢ æ¸©åº¦å‚æ•°é…ç½®å’Œå‚æ•°è°ƒä¼˜ç»éªŒ
   â€¢ ä¸åŒLLMæä¾›å•†çš„ç‰¹ç‚¹å’Œå¯¹æ¯”
   â€¢ æ¨¡å‹å“åº”çš„å¤„ç†å’Œé”™è¯¯å®¹é”™æœºåˆ¶
   â€¢ APIè°ƒç”¨çš„æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

ğŸ—‚ï¸ å®é™…æŠ€èƒ½å­˜å‚¨:
   â€¢ å¯é…ç½®4+ä¸»è¦æ¨¡å‹æä¾›å•† (OpenAI, Claude, DeepSeek, Zhipu)
   â€¢ æŒæ¡äº†1-3ä¸ªæ¸©åº¦å‚æ•°çš„æœ€ä½³å®è·µ
   â€¢ èƒ½åŠ›å¤„ç†4ç§ä¸»è¦é”™è¯¯ç±»å‹ (ç½‘ç»œ/é¢‘ç‡é™åˆ¶/è®¤è¯/å…¶å®ƒ)
   â€¢ å…·å¤‡10+ç§ä¸åŒç±»å‹çš„æµ‹è¯•å’Œå¯¹æ¯”èƒ½åŠ›

ğŸ“Š æ€§èƒ½åŸºå‡†å»ºç«‹:
   â€¢ GPT-3.5-turbo: å¹³å‡å“åº”æ—¶é—´ ~1-2ç§’ (Fast)
   â€¢ GPT-4: å¹³å‡å“åº”æ—¶é—´ ~2-4ç§’ (é«˜è´¨é‡)
   â€¢ Claude-3-sonnet: å¹³å‡å“åº”æ—¶é—´ ~1-3ç§’ (å¹³è¡¡)

â­ï¸ ä¸‹ä¸€å‘¨å­¦ä¹ é‡ç‚¹ (Week 3):
   ğŸ“š Promptå·¥ç¨‹è¿›é˜¶æŠ€å·§
   ğŸ›  Few-shotå­¦ä¹ åº”ç”¨
   ğŸš€ Toolé›†æˆåŸºç¡€
   ğŸ¯ å’Œä¸­å›½å¤§æ¨¡å‹é›†æˆ

---
### ğŸš€ Week 2å®æˆ˜åº”ç”¨å»ºè®®:
   1. ä½¿ç”¨å­¦ä¹ çš„æ¨¡å‹å¯¹æ¯”æµ‹è¯•ä¸åŒæ¨¡å‹çš„ä»·æ ¼æ€§èƒ½æ¯”
   2. ä¸ºä¸åŒçš„ä½¿ç”¨åœºæ™¯è®¾ç½®æœ€ä½³çš„æ¸©åº¦å‚æ•°
   3. åŸºäºé”™è¯¯å¤„ç†æ¨¡å¼æ„å»ºæ›´ç¨³å®šçš„åº”ç”¨
   4. é›†æˆä¸­å›½å¤§æ¨¡å‹æä¾›å•†è¿›è¡Œæ·±åº¦å¯¹æ¯”
"""
        return summary

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒWeek 2æ‰€æœ‰æ¨¡å‹äº¤äº’ç»ƒä¹ """
    print("ğŸ¯ LangChain L1 Foundation - Week 2: æ¨¡å‹äº¤äº’ä¸å¤šæ¨¡å‹å¯¹æ¯”")
    print("=" * 70)
    print("æœ¬å‘¨å°†å­¦ä¹ å¦‚ä½•åœ¨LangChainä¸­é…ç½®å’Œä½¿ç”¨å¤šç§LLMæ¨¡å‹")
    
    trainer = ChatModelTrainer()
    
    try:
        # è¿è¡Œå„ä¸ªç»ƒä¹ æ¨¡å—
        api_status = trainer.validate_api_credentials()
        
        # å®é™…çš„g ptæ¨¡å‹æ¼”ç¤ºï¼ˆéœ€è¦çœŸå®APIå¯†é’¥ï¼‰
        gpt_model = trainer.demo_gpt_model_basics()
        
        if gpt_model:
            trainer.demo_temperature_parameter(gpt_model)
        
        # å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•
        print("\n" + "="*70 + "\n")
        comparisons = trainer.demo_multiple_providers_comparison()
        
        # å“åº”å¤„ç†æœ€ä½³å®è·µ
        trainer.demo_model_response_handling(comparisons)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = trainer.generate_week2_summary()
        print(summary)
        
        # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶
        with open("01_chat_models_basics_summary.md", "w", encoding="utf-8") as f:
            f.write(summary)
        
        print("\nâœ… Week 2 æ¨¡å‹äº¤äº’å­¦ä¹ å®Œæˆï¼")
        print("ğŸ“‹ è¯¦ç»†æ€»ç»“å·²ä¿å­˜è‡³ 01_chat_models_basics_summary.md")
        print("\nğŸš€ æ¨èä¸‹ä¸€æ­¥:")
        print("   1. ä»”ç»†æ£€æŸ¥ç”Ÿæˆçš„å¯¹æ¯”æŠ¥å‘Š")
        print("   2. è°ƒæ•´ä¸åŒæ¨¡å‹çš„æ¸©åº¦å‚æ•°æµ‹è¯•æ•ˆæœ")
        print("   3. å‡†å¤‡è¿›å…¥ Week 3 Promptå·¥ç¨‹è¿›é˜¶å­¦ä¹ ")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Week 2æ¨¡å‹äº¤äº’å­¦ä¹ è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ å­¦ä¹ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()