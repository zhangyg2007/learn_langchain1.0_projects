#!/usr/bin/env python3
"""
LangChain L2 Intermediate - Week 4
è¯¾ç¨‹æ ‡é¢˜: RAGç³»ç»Ÿæ„å»º - å‘é‡å­˜å‚¨åŸºç¡€ä¸æ–‡æ¡£å¤„ç†
å­¦ä¹ ç›®æ ‡:
  - æŒæ¡å‘é‡æ•°æ®åº“(ChromaDB, Weaviate)çš„åŸºæœ¬ä½¿ç”¨
  - å­¦ä¼šæ–‡æœ¬åˆ†å‰²(Text Splitting)çš„æœ€ä½³å®è·µ
  - ç†è§£åµŒå…¥(Embedding)å‘é‡çš„å·¥ä½œåŸç†
  - å®è·µæ–‡æ¡£åŠ è½½å™¨(Document Loaders)çš„åº”ç”¨
  - æ„å»ºåŸºç¡€çš„RAGæ£€ç´¢é“¾
ä½œè€…: Claude Code æ•™å­¦å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2024-01-16
ç‰ˆæœ¬: 1.0.0
å…ˆå†³æ¡ä»¶: å®ŒæˆL1 Foundationè®¤è¯ï¼ŒæŒæ¡Agentå¼€å‘åŸºç¡€

ğŸ¯ å®è·µé‡ç‚¹:
  - å‘é‡æ•°æ®åº“æ“ä½œ
  - æ–‡æœ¬åˆ†å—ç­–ç•¥
  - Document Storageè®¾è®¡
  - RAGç³»ç»Ÿé›†æˆæµ‹è¯•
"""

import sys
import os
import time
from typing import Dict, List, Optional, Union, Any
from datetime import datetime
from dataclasses import dataclass
from pathlib import Path
import json
import numpy as np

# ç¯å¢ƒé…ç½®
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… ç¯å¢ƒå˜é‡å·²åŠ è½½")
except ImportError:
    print("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œè¯·ç¡®ä¿æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡")

# LangChain RAGç›¸å…³å¯¼å…¥
try:
    from langchain_community.vectorstores import Chroma, Weaviate
    from langchain_community.document_loaders import TextLoader, DirectoryLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
    from langchain_core.documents import Document
    from langchain_core.vectorstores import VectorStoreRetriever
    from langchain_core.embeddings import Embeddings
    print("âœ… LangChain RAGç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ LangChain RAGç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š")
    print("   pip install langchain-community langchain-text-splitters")
    print("   pip install chromadb weaviate-client")
    sys.exit(1)

# ä¸­å›½æ¨¡å‹æ”¯æŒ
try:
    from langchain_openai import OpenAIEmbeddings
    print("âœ… OpenAI Embeddingså¯¼å…¥æˆåŠŸ")
except ImportError as e:
    OpenAIEmbeddings = None
    print(f"âš ï¸ OpenAI Embeddingså¯¼å…¥å¤±è´¥: {e}")

# ä¸­å›½åµŒå…¥æ¨¡å‹æ”¯æŒ
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_openai import OpenAIEmbeddings
    print("âœ… åµŒå…¥æ¨¡å‹æ”¯æŒå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âš ï¸ éƒ¨åˆ†åµŒå…¥æ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")

@dataclass
class VectorStoreResult:
    """å‘é‡å­˜å‚¨æ“ä½œç»“æœ"""
    operation: str
    success: bool
    documents_count: int
    vector_count: int
    execution_time: float
    error: Optional[str] = None

@dataclass
class TextSplitStats:
    """æ–‡æœ¬åˆ†å‰²ç»Ÿè®¡"""
    original_length: int
    chunks_count: int
    avg_chunk_size: float
    min_chunk_size: int
    max_chunk_size: int
    overlapping_chars: int

class RAGBasicsTrainer:
    """L2 RAGåŸºç¡€è®­ç»ƒå™¨"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.vector_stores = {}
        self.sample_documents = self._init_sample_documents()
        self.embedding_models = {}
        self.exercises_completed = []
        
    def _log(self, message: str):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"ğŸ“š {message}")
    
    def _init_sample_documents(self) -> List[Document]:
        """åˆå§‹åŒ–ç¤ºä¾‹æ–‡æ¡£"""
        documents = [
            {
                "content": """
                å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“ä¸åŒï¼Œ
                å‘é‡æ•°æ®åº“èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é«˜ç»´å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬ChromaDBã€Weaviateã€
                Qdrantç­‰ã€‚å®ƒä»¬åœ¨RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ã€‚
                """,
                "metadata": {"topic": "å‘é‡æ•°æ®åº“", "category": "åŸºç¡€æ¦‚å¿µ", "source": "L2è¯¾ç¨‹"}
            },
            {
                "content": """
                æ–‡æœ¬åˆ†å—æ˜¯å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°æ®µè½çš„è¿‡ç¨‹ï¼Œåœ¨RAGç³»ç»Ÿä¸­éå¸¸é‡è¦ã€‚æ­£ç¡®çš„åˆ†å—ç­–ç•¥å¯ä»¥
                å¹³è¡¡è¯­ä¹‰å®Œæ•´æ€§å’Œæ£€ç´¢æ•ˆç‡ã€‚å¸¸ç”¨çš„åˆ†å—æ–¹æ³•åŒ…æ‹¬æŒ‰å­—ç¬¦æ•°åˆ†å—ã€æŒ‰å¥å­åˆ†å—ã€æŒ‰æ®µè½åˆ†å—ç­‰ã€‚
                åˆ†å—æ—¶éœ€è¦è€ƒè™‘é‡å å­—ç¬¦ä»¥é¿å…è¯­ä¹‰æ–­è£‚ã€‚
                """,
                "metadata": {"topic": "æ–‡æœ¬åˆ†å—", "category": "å¤„ç†æŠ€æœ¯", "source": "L2è¯¾ç¨‹"}
            },
            {
                "content": """
                åµŒå…¥(Embedding)æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹ã€‚è¿™äº›å‘é‡èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œ
                ä½¿å¾—ç›¸ä¼¼å«ä¹‰çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ã€‚å¸¸ç”¨çš„åµŒå…¥æ¨¡å‹åŒ…æ‹¬OpenAIçš„text-embedding-ada-002ã€
                ä»¥åŠHuggingFaceæä¾›çš„å¤šç§é¢„è®­ç»ƒæ¨¡å‹ã€‚
                """,
                "metadata": {"topic": "åµŒå…¥å‘é‡", "category": "AIæ¦‚å¿µ", "source": "L2è¯¾ç¨‹"}
            }
        ]
        
        return [
            Document(page_content=doc["content"], metadata=doc["metadata"])
            for doc in documents
        ]
    
    def demo_embedding_vectors_basics(self):
        """æ¼”ç¤ºåµŒå…¥å‘é‡åŸºç¡€æ¦‚å¿µ"""
        self._log("åµŒå…¥å‘é‡åŸºç¡€æ¦‚å¿µæ¼”ç¤º")
        print("-" * 60)
        
        print("ğŸ§  ä»€ä¹ˆæ˜¯Text Embeddingï¼Ÿ")
        print("   â€¢ å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹")
        print("   â€¢ å‘é‡èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯") 
        print("   â€¢ ç›¸ä¼¼å«ä¹‰çš„æ–‡æœ¬å…·æœ‰ç›¸è¿‘çš„å‘é‡è¡¨ç¤º")
        print("   â€¢ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ç”¨äºæœç´¢å’ŒåŒ¹é…")
        print()
        
        # æ¼”ç¤ºåµŒå…¥å‘é‡çš„ç”Ÿæˆ
        print("ğŸ”§ åµŒå…¥å‘é‡ç”Ÿæˆè¿‡ç¨‹æ¼”ç¤º:")
        
        sample_texts = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œçš„åº”ç”¨",
            "è‡ªç„¶è¯­è¨€å¤„ç†ç”¨äºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€"
        ]
        
        for i, text in enumerate(sample_texts, 1):
            print(f"\n   {i}. æ–‡æœ¬: '{text}'")
            print(f"      â””â”€ å­—ç¬¦æ•°: {len(text)}")
            print(f"      â””â”€ è¯è¯­æ•°: {len(text.split())}")
            print(f"      â””â”€ é¢„æœŸå‘é‡ç»´åº¦: 1536 (ada-002) æˆ– 768 (HuggingFace)")
            
        print(f"\nğŸ’¡ åµŒå…¥å‘é‡åœ¨RAGä¸­çš„ä½œç”¨:")
        print("   â”œâ”€ ç”¨æˆ·æŸ¥è¯¢ â†’ æŸ¥è¯¢å‘é‡")
        print("   â”œâ”€ æ–‡æ¡£åˆ†å— â†’ æ–‡æ¡£å‘é‡")  
        print("   â”œâ”€ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—")
        print("   â””â”€ æœ€ç›¸ä¼¼æ–‡æ¡£æ®µè¿”å›")\n        
        # æ¼”ç¤ºå‘é‡ç›¸ä¼¼åº¦æ¦‚å¿µ (æ¨¡æ‹Ÿ)
        print("\nğŸ“Š å‘é‡ç›¸ä¼¼åº¦æ¦‚å¿µæ¼”ç¤º:")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ–‡æœ¬-å‘é‡æ˜ å°„
        text_vectors = {
            "æœºå™¨å­¦ä¹ åº”ç”¨": [0.8,  0.9,  0.7,  0.6],
            "æ·±åº¦å­¦ä¹ æŠ€æœ¯": [0.9,  0.8,  0.6,  0.7],
            "è‡ªç„¶è¯­è¨€å¤„ç†": [0.6,  0.7,  0.9,  0.8],
            "æ•°æ®åº“ç®¡ç†":   [0.3,  0.4,  0.2,  0.1]  # ä¸å‰ä¸‰ä¸ªä¸ç›¸å…³çš„ä¸»é¢˜
        }
        
        query_vector = [0.85, 0.88, 0.65, 0.62]  # "æœºå™¨å­¦ä¹ æ·±åº¦å­¦ä¹ "
        
        print(f"   æŸ¥è¯¢å‘é‡: {query_vector}")
        print("   æ–‡æ¡£å‘é‡ç›¸ä¼¼åº¦è®¡ç®—:")
        
        def cosine_similarity(vec1, vec2):
            """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (ç®€åŒ–ç‰ˆ)"""
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = sum(a * a for a in vec1) ** 0.5
            magnitude2 = sum(a * a for a in vec2) ** 0.5
            return dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 > 0 else 0
        
        similarities = []
        for doc, doc_vec in text_vectors.items():
            similarity = cosine_similarity(query_vector, doc_vec)
            similarities.append((doc, similarity))
            print(f"      {doc}: {similarity:.3f}")
        
        # æ’åºå¹¶æ˜¾ç¤ºæœ€ç›¸ä¼¼çš„
        similarities.sort(key=lambda x: x[1], reverse=True)
        print(f"\n   æœ€ç›¸ä¼¼æ–‡æ¡£ (Top 2):")
        for i, (doc, sim) in enumerate(similarities[:2], 1):
            print(f"      {i}. {doc} (ç›¸ä¼¼åº¦: {sim:.3f})")
        
        self.exercises_completed.append("embedding_vectors_basics")
    
    def demo_text_splitting_strategies(self):
        """æ¼”ç¤ºæ–‡æœ¬åˆ†å‰²ç­–ç•¥"""
        self._log("æ–‡æœ¬åˆ†å‰²ç­–ç•¥æ¼”ç¤º")
        print("-" * 60)
        
        print("ğŸ“ æ–‡æœ¬åˆ†å—çš„åŸºæœ¬åŸåˆ™:")
        print("   â€¢ è¯­ä¹‰å®Œæ•´æ€§: ä¿æŒåˆ†å—çš„è‡ªç„¶è¯­ä¹‰è¾¹ç•Œ")
        print("   â€¢ å¤§å°é€‚ä¸­: å¹³è¡¡æ£€ç´¢æ•ˆç‡å’Œè¯­ä¹‰å®Œæ•´æ€§")
        print("   â€¢ é‡å å¤„ç†: é¿å…åˆ†æ•£å‰ªåˆ‡é€ æˆçš„ä¿¡æ¯ä¸¢å¤±")
        print("   â€¢ æ ¼å¼ç»Ÿä¸€: ä¿è¯åˆ†å—æ ¼å¼çš„ä¸€è‡´æ€§")
        print()\n        
        # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
        sample_long_text = """
        äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ·±åˆ»åœ°æ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚æœºå™¨å­¦ä¹ ä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›å…¶è¡¨ç°ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜¾å¼çš„ç¼–ç¨‹ã€‚
        
        æ·±åº¦å­¦ä¹ è¿›ä¸€æ­¥æ¨åŠ¨äº†æœºå™¨å­¦ä¹ çš„å‘å±•ã€‚å®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯é‚£äº›å…·æœ‰å¤šä¸ªå±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å‘ç°æ•°æ®çš„å¤šå±‚æŠ½è±¡è¡¨ç¤ºï¼Œ
        ä½¿å…¶ç‰¹åˆ«é€‚åˆå¤„ç†éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚
        
        åœ¨å…·ä½“åº”ç”¨æ–¹é¢ï¼Œæœºå™¨å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„å‘å±•è®©äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡å·²ç»è¶…è¿‡äº†äººç±»æ°´å¹³ã€‚
        åŒæ ·ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒTransformeræ¶æ„çš„å‡ºç°é©å‘½æ€§åœ°æ”¹å˜äº†è¯­è¨€æ¨¡å‹ï¼Œä½¿å¾—æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡çš„æ€§èƒ½å¤§å¹…æå‡ã€‚
        
        å¼ºåŒ–å­¦ä¹ åˆ™åœ¨æ¸¸æˆå¯¹æŠ—å’Œå†³ç­–ä¼˜åŒ–é¢†åŸŸå±•ç°äº†å¼ºå¤§èƒ½åŠ›ã€‚AlphaGoå‡»è´¥äººç±»å›´æ£‹å† å†›å°±æ˜¯ä¸€ä¸ªç»å…¸çš„ä¾‹å­ã€‚é€šè¿‡ä¸æ–­çš„è¯•é”™å’Œå­¦ä¹ ï¼Œæ™ºèƒ½ä½“å¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­
        æ‰¾åˆ°æœ€ä¼˜çš„å†³ç­–ç­–ç•¥ã€‚
        """
        
        print(f"ğŸ“„ æµ‹è¯•æ–‡æ¡£ä¿¡æ¯:")
        print(f"   â””â”€ å­—ç¬¦æ•°: {len(sample_long_text)}")
        print(f"   â””â”€ æ®µè½æ•°: {len(sample_long_text.split())}")
        print(f"   â””â”€ å¥å­æ•°(ä»¥'ã€‚'è®¡ç®—): {sample_long_text.count('ã€‚')}")
        print()
        
        print("ğŸ§ª ä¸åŒåˆ†å—ç­–ç•¥å¯¹æ¯”:")
        \n        # 1. æŒ‰å­—ç¬¦æ•°çš„ç®€å•åˆ†å‰²\n        print("\n1ï¸âƒ£ æŒ‰å­—ç¬¦æ•°çš„ç®€å•åˆ†å‰² (CharacterTextSplitter):")
        character_splitter = CharacterTextSplitter(
            separator="\\n\\n",  # æŒ‰åŒæ¢è¡Œç¬¦åˆ†æ®µ
            chunk_size=400,    # æ¯å—400å­—ç¬¦
            chunk_overlap=50,  # é‡å 50å­—ç¬¦
            length_function=len
        )
        
        char_docs = character_splitter.create_documents([sample_long_text])
        duration_1 = time.time()
        
        stats_1 = self._calculate_split_stats(sample_long_text, char_docs)
        self._print_split_stats("å­—ç¬¦åˆ†å‰²", stats_1, len(char_docs))
        \n        # 2. é€’å½’åˆ†å‰² (æ›´æ™ºèƒ½)\n        print("\n2ï¸âƒ£ é€’å½’åˆ†å‰² (RecursiveCharacterTextSplitter):")
        recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,\n            chunk_overlap=50,\n            length_function=len,\n            separators=["\\n\\n", "\\n", " ", ""]  # ä¼˜å…ˆçº§: æ®µè½â†’å¥å­â†’è¯è¯­â†’å­—ç¬¦\n        )\n        \n        recursive_docs = recursive_splitter.create_documents([sample_long_text])\n        duration_2 = time.time()\n        \n        stats_2 = self._calculate_split_stats(sample_long_text, recursive_docs)\n        self._print_split_stats("é€’å½’åˆ†å‰²", stats_2, len(recursive_docs))\n        \n        # 3. HTMLMarkdownSplitter (æ ¼å¼åŒ–æ–‡æ¡£) - æ¼”ç¤ºæ¦‚å¿µ\n        print("\\n3ï¸âƒ£ æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å‰²çš„æ¦‚å¿µ:")\n        print("   â”œâ”€ æŒ‰å¥å­åˆ†å‰²: ä¿æŒè¯­ä¹‰å®Œæ•´æ€§")
        print("   â”œâ”€ æŒ‰æ®µè½åˆ†å‰²: ä¿æŒä¸»é¢˜è¿è´¯æ€§")
        print("   â””â”€ æŒ‰ç« åˆ†å‰² (é•¿æ–‡æœ¬): æŒ‰æ–‡æ¡£ç»“æ„")
        \n        # æ˜¾ç¤ºå‰å‡ ä¸ªåˆ†å‰²ç»“æœçš„æ ·ä¾‹\n        print(f"\\nğŸ“‹ åˆ†å‰²ç»“æœæ ·ä¾‹å¯¹æ¯”:")
        sample_indexes = [0, 1, 2] if len(char_docs) >= 3 else list(range(len(char_docs)))\n        \n        for i in sample_indexes:
            if i < len(char_docs) and i < len(recursive_docs):\n                print(f"\\n   åˆ†å— {i+1}"):""")\n                print(f"      å­—ç¬¦åˆ†å‰²: {char_docs[i].page_content[:100]}...")
                print(f"      é€’å½’åˆ†å‰²: {recursive_docs[i].page_content[:100]}...")
                print(f"      é•¿åº¦å¯¹æ¯”: {len(char_docs[i].page_content)} vs {len(recursive_docs[i].page_content)}")
        \n        self.exercises_completed.append("text_splitting_strategies")\n    \n    def _calculate_split_stats(self, original_text: str, split_docs: List[Document]) -> TextSplitStats:
        """è®¡ç®—åˆ†å‰²ç»Ÿè®¡ä¿¡æ¯"""\n        if not split_docs:\n            return TextSplitStats(
                original_lengthå¼¯å¼¯=len(original_text),
                chunks_count=0,
                avg_chunk_size=0.0,
                min_chunk_size=0,
                max_chunk_size=0,
                overlapping_chars=0
            )
        \n        chunk_sizes = [len(doc.page_content) for doc in split_docs]
        \n        stats = TextSplitStats(\n            original_length=len(original_text),\n            chunks_count=len(split_docs),\n            avg_chunk_size=sum(chunk_sizes) / len(chunk_sizes),\n            min_chunk_size=min(chunk_sizes),\n            max_chunk_size=max(chunk_sizes),\n            overlapping_chars=0  # ç®€åŒ–è®¡ç®—\n        )\n        return stats\n    \n    def _print_split_stats(self, method_name: str, stats: TextSplitStats, doc_count: int):\n        """æ‰“å°åˆ†å‰²ç»Ÿè®¡"""\n        print(f"\\n   {method_name} ç»Ÿè®¡:")
        print(f"      â”œâ”€ æ€»åˆ†å—æ•°: {stats.chunks_count}")\n        print(f"      â”œâ”€ å¹³å‡å—å¤§å°: {stats.avg_chunk_size:.1f} å­—ç¬¦")
        print(f"      â”œâ”€ æœ€å°/æœ€å¤§å—: {stats.min_chunk_size}/{stats.max_chunk_size} å­—ç¬¦") \n        print(f"      â””â”€ å—å¤§å°æ–¹å·®: {self._calculate_variance([len(doc.page_content) for doc in range(doc_count)]) if doc_count \u003e 0 else 0:.1f}")
    \n    def _calculate_variance(self, values: List[float]) -> float:\n        """è®¡ç®—æ–¹å·®"""\n        if not values:
            return 0.0\n        mean = sum(values) / len(values)\n        return sum((x - mean) ** 2 for x in values) / len(values)
    
    def demo_vector_databases_setup(self):
        """æ¼”ç¤ºå‘é‡æ•°æ®åº“çš„è®¾ç½®å’Œä½¿ç”¨"""
        self._log("å‘é‡æ•°æ®åº“è®¾ç½®å’Œä½¿ç”¨æ¼”ç¤º")
        print("-" * 60)
        \n        print("ğŸ’¾ å‘é‡æ•°æ®åº“é€‰å‹å¯¹æ¯”:")
        vdb_comparison = {\n            "ChromaDB": {
                "strengths": ["ç®€å•æ˜“ç”¨", "å†…å­˜å­˜å‚¨", "PythonåŸç”Ÿ"],
                "use_cases": ["å¿«é€ŸåŸå‹", "å°å‹é¡¹ç›®", "å­¦ä¹ å®éªŒ"], 
                "limitations": ["æ‰©å±•æ€§æœ‰é™", "ç”Ÿäº§çº§åŠŸèƒ½æ¬ ç¼º"]
            },\n            "Weaviate": {
                "strengths": ["ä¼ä¸šçº§åŠŸèƒ½", "GraphQL API", "åˆ†å¸ƒå¼æ”¯æŒ"],\n                "use_cases": ["ä¼ä¸šåº”ç”¨", "å¤§è§„æ¨¡éƒ¨ç½²", "å¤æ‚æŸ¥è¯¢"],\n                "limitations": ["å­¦ä¹ æ›²çº¿é™¡å³­", "èµ„æºæ¶ˆè€—è¾ƒå¤§"]
            },\n            "Qdrant": {\n                "strengths": ["é«˜æ€§èƒ½", "Rustç¼–å†™", "çŸ¢é‡è¿‡æ»¤"],
                "use_cases": ["é«˜å¹¶å‘åœºæ™¯", "å®æ—¶æœç´¢", "è¿‡æ»¤æŸ¥è¯¢"],
                "limitations": ["é…ç½®å¤æ‚", "æ–‡æ¡£ç›¸å¯¹å°‘"]
            },\n            "Pinecone": {
                "strengths": ["æ‰˜ç®¡æœåŠ¡", "è‡ªåŠ¨æ‰©å±•", "ç”Ÿäº§å°±ç»ª"],\n                "use_cases": ["ç”Ÿäº§ç¯å¢ƒ", "æ— éœ€ç»´æŠ¤", "å¿«é€Ÿéƒ¨ç½²"],
                "limitations": ["ä»·æ ¼è¾ƒé«˜", "æ•°æ®éšç§è€ƒè™‘"]
            }\n        }\n        \n        for vdb_name, details in vdb_comparison.items():
            print(f"\\n   ğŸ“ {vdb_name}:")
            print(f"      â””â”€ ä¼˜åŠ¿: {', '.join(details['strengths'])}")
            print(f"      â””â”€ åœºæ™¯: {', '.join(details['use_cases'][:2])}")
        \n        print(f"\\nğŸ”§ ChromaDB åŸºç¡€ä½¿ç”¨æ¼”ç¤º:")
        \n        # 1. ChromaDB åŸºç¡€è®¾ç½®\n        print(f"\\n1ï¸âƒ£ ChromaDB è®¾ç½®å’Œåˆå§‹åŒ–:")
        \n        try:\n            # åˆ›å»ºæŒä¹…åŒ–å‘é‡å­˜å‚¨\n            chroma_persist_dir = "./chroma_db_demo"\n            os.makedirs(chroma_persist_dir, exist_ok=True)\n            \n            print(f"      â””â”€ åˆ›å»ºæŒä¹…åŒ–ç›®å½•: {chroma_persist_dir}")
            \n            # ä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥ (é¿å…APIä¾èµ–)\n            from langchain_community.embeddings import FakeEmbeddings\n            embeddings = FakeEmbeddings(size=256)\n            print(f"      â””â”€ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {embeddings.__class__.__name__}")
            \n            # åˆ›å»ºé›†åˆ (Collection)\n            start_time = time.time()\n            \n            vector_store = Chroma(\n                collection_name="demo_rag_collection",
                embedding_function=embeddings,\n                persist_directory=chroma_persist_dir\n            )\n            \n            init_time = time.time() - start_time\n            print(f"      â””â”€ é›†åˆåˆå§‹åŒ–å®Œæˆ: {init_time:.3f}ç§’")
            \n            # 2. æ–‡æ¡£æ·»åŠ å’Œå‘é‡ç”Ÿæˆ\n            print(f"\\n2ï¸âƒ£ æ–‡æ¡£å¤„ç†å’Œå‘é‡ç”Ÿæˆ:")
            \n            process_time = time.time()\n            doc_texts = [\n                "å‘é‡æ•°æ®åº“æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å­˜å‚¨å’Œç®¡ç†æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚",
                "ChromaDBæä¾›äº†ç®€å•æ˜“ç”¨çš„Python APIï¼Œæ”¯æŒå†…å­˜å’ŒæŒä¹…åŒ–ä¸¤ç§æ¨¡å¼ã€‚",\n                "åµŒå…¥å‘é‡èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ã€‚"
            ]\n            \n            docs = [Document(page_content=text) for text in doc_texts]\n            \n            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨\n            ids = vector_store.add_documents(docs)\n            process_time = time.time() - process_time\n            \n            print(f"      â””â”€ å¤„ç†çš„æ–‡æ¡£æ•°: {len(docs)}")\n            print(f"      â””â”€ ç”Ÿæˆå‘é‡IDæ•°é‡: {len(ids)}")
            print(f"      â””â”€ å¤„ç†æ—¶é—´: {process_time:.3f}ç§’")
            \n            # 3. ç›¸ä¼¼åº¦æœç´¢æ¼”ç¤º\n            print(f"\\n3ï¸âƒ£ å‘é‡ç›¸ä¼¼åº¦æœç´¢:")
            \n            query = "å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"\n            
            print(f"   æŸ¥è¯¢æ–‡æœ¬: '{query}'")\n            print("   æœç´¢ç»“æœ:")
            \n            search_time = time.time()\n            results = vector_store.similarity_search(query, k=2)\n            search_time = time.time() - search_time\n            \n            for i, doc in enumerate(results, 1):\n                print(f"      {i}. ç›¸ä¼¼åº¦å¾—åˆ†: {getattr(doc, 'score', 'N/A')}")\n                print(f"         æ–‡æ¡£å†…å®¹: {doc.page_content[:80]}...")
                \n            print(f"      â””â”€ æœç´¢è€—æ—¶: {search_time:.3f}ç§’")
            \n            # 4. é«˜çº§åŠŸèƒ½æ¼”ç¤º\n            print(f"\\n4ï¸âƒ£ å‘é‡å­˜å‚¨é«˜çº§åŠŸèƒ½:")
            \n            # ç›¸ä¼¼åº¦æœç´¢åŠ åˆ†æ•°\n            search_score_result = vector_store.similarity_search_with_score(query, k=3)\n            if search_score_result:\n                print("   å¸¦åˆ†æ•°çš„æœç´¢ç»“æœ:")
                for doc, score in search_score_result:
                    print(f"      â”œâ”€ ç›¸ä¼¼åº¦: {score:.4f}")
                    print(f"      â””â”€ å†…å®¹: {doc.page_content[:60]}...")
            \n            # æŒä¹…åŒ–å­˜å‚¨\n            print(f"\\n   æŒä¹…åŒ–å­˜å‚¨:")\n            vector_store.persist()\n            print(f"      â””â”€ æ•°æ®å·²æŒä¹…åŒ–åˆ°: {chroma_persist_dir}")
            \n            self.vector_stores["chroma_demo"] = vector_store\n            \n        except Exception as e:\n            print(f"      âŒ ChromaDBæ¼”ç¤ºå¤±è´¥: {str(e)}")
            print(f"      ğŸ’¡ è¯·ç¡®ä¿ChromaDBå·²æ­£ç¡®å®‰è£…: pip install chromadb")
        \n        # 5. Weaviate æ¦‚å¿µæ¼”ç¤º (ä¸»è¦æ¦‚å¿µ)\n        print(f"\\nğŸŒ Weaviate æ¦‚å¿µæ¼”ç¤º:")
        print("   â”œâ”€ éœ€è¦ç‹¬ç«‹éƒ¨ç½²æˆ–äº‘æœåŠ¡")\n        print("   â”œâ”€ æ”¯æŒGraphQLæŸ¥è¯¢è¯­è¨€")\n        print("   â”œâ”€ å†…ç½®å‘é‡ç”Ÿæˆå’Œé¢„å¤„ç†")\n        print("   â””â”€ é€‚åˆä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒ")\n        \n        self.exercises_completed.append("vector_databases_setup")\n    
    def demo_document_loaders_integration(self):\n        """æ¼”ç¤ºæ–‡æ¡£åŠ è½½å™¨çš„é›†æˆä½¿ç”¨"""
        self._log("æ–‡æ¡£åŠ è½½å™¨é›†æˆä½¿ç”¨æ¼”ç¤º")\n        print("-" * 60)\n        
        print("ğŸ“ æ–‡æ¡£åŠ è½½å™¨çš„ä½œç”¨:")\n        print("   â€¢ ä»å„ç§æ¥æºåŠ è½½æ–‡æ¡£å†…å®¹")\n        print("   â€¢ æ ‡å‡†åŒ–æ–‡æ¡£æ ¼å¼ (Documentå¯¹è±¡)")
        print("   â€¢ æå–å…ƒæ•°æ®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯")\n        print("   â€¢ æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„è§£æ")
        print()\n        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_doc_dir = "./test_documents"
        os.makedirs(test_doc_dir, exist_ok=True)\n        \n        # åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•æ–‡æ¡£\n        test_files = {\n            "ai_overview.txt": """\näººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚\n\nAIæŠ€æœ¯çš„å‘å±•ç»å†äº†å¤šä¸ªé˜¶æ®µï¼ŒåŒ…æ‹¬ç¬¦å·AIã€ç»Ÿè®¡å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ã€‚\næ¯ä¸ªé˜¶æ®µéƒ½ä»£è¡¨äº†ä¸åŒçš„æ–¹æ³•å’Œé‡ç‚¹ã€‚\n\nå½“å‰AIçš„çƒ­ç‚¹åŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººæŠ€æœ¯ç­‰ã€‚\n""",
            \n            "ml_basics.txt": """\næœºå™¨å­¦ä¹ (ML)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ çš„ç®—æ³•ã€‚\n\nä¸»è¦çš„æœºå™¨å­¦ä¹ ç±»å‹åŒ…æ‹¬ï¼š\n1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹\n2. æ— ç›‘ç£å­¦ä¹ ï¼šä»æœªæ ‡è®°æ•°æ®ä¸­æå–æ¨¡å¼\n3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥\n\næœºå™¨å­¦ä¹ åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬æ¨èç³»ç»Ÿã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—è¯Šæ–­ç­‰ã€‚\n""",
            \n            "deep_learning.txt": """\næ·±åº¦å­¦ä¹ æ˜¯åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œã€‚\n\nå®ƒé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„éç»“æ„åŒ–æ•°æ®ï¼Œ\nå¦‚å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚\n\nå…³é”®æŠ€æœ¯åŒ…æ‹¬CNNã€RNNã€Transformerç­‰æ¶æ„ã€‚\n"""
        }\n        \n        for filename, content in test_files.items():\n            file_path = os.path.join(test_doc_dir, filename)\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content.strip())\n            \n        print(f"ğŸ“„ åˆ›å»ºæµ‹è¯•æ–‡æ¡£å®Œæˆ: {len(test_files)} ä¸ªæ–‡ä»¶")
        \n        print(f"\\n1ï¸âƒ£ å•ç‹¬æ–‡ä»¶åŠ è½½:")
        \n        # å•ä¸ªæ–‡ä»¶åŠ è½½\n        try:\n            text_loader = TextLoader(test_doc_dir + "/ai_overview.txt", encoding='utf-8')\n            single_doc = text_loader.load()[0]\n            \n            print(f"   æ–‡ä»¶: ai_overview.txt")\n            print(f"   å­—ç¬¦æ•°: {len(single_doc.page_content)}")\n            print(f"   å…ƒæ•°æ®: {single_doc.metadata}")\n            print(f"   å†…å®¹é¢„è§ˆ: {single_doc.page_content[:100]}...")
            \n        except Exception as e:\n            print(f"   âŒ å•ä¸ªæ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
        \n        print(f"\\n2ï¸âƒ£ ç›®å½•æ‰¹é‡åŠ è½½:")
        \n        # ç›®å½•æ‰¹é‡åŠ è½½\n        try:\n            directory_loader = DirectoryLoader(\n                test_doc_dir,
                glob="*.txt",\n                loader_cls=TextLoader,\n                loader_kwargs={'encoding': 'utf-8'}\n            )\n            \n            all_docs = directory_loader.load()\n            print(f"   åŠ è½½çš„æ–‡æ¡£æ•°é‡: {len(all_docs)}")
            \n            for i, doc in enumerate(all_docs):\n                print(f"   {i+1}. æ–‡ä»¶: {doc.metadata.get('source', 'æœªçŸ¥')}")
                print(f"         å¤§å°: {len(doc.page_content)} å­—ç¬¦")
                print(f"         é¢„è§ˆ: {doc.page_content[:80]}...")\n                if i \u003e= 1:  # åªæ˜¾ç¤ºå‰2ä¸ª\n                    break\n                
        except Exception as e:\n            print(f"   âŒ ç›®å½•åŠ è½½å¤±è´¥: {str(e)}")
        \n        print(f"\\n3ï¸âƒ£ PDFç­‰å…¶ä»–æ ¼å¼åŠ è½½ (æ¦‚å¿µæ¼”ç¤º):")
        print("   â”œâ”€ PDFLoader: è§£æPDFæ–‡æ¡£")\n        print("   â”œâ”€ CSVLoader: å¤„ç†CSVæ•°æ®æ–‡ä»¶")\n        print("   â”œâ”€ JSONLoader: è§£æJSONç»“æ„åŒ–æ•°æ®")
        print("   â”œâ”€ WebBaseLoader: åŠ è½½ç½‘é¡µå†…å®¹")\n        print("   â””â”€ è‡ªå®šä¹‰åŠ è½½å™¨: æ”¯æŒç‰¹æ®Šæ ¼å¼æ–‡ä»¶")
        \n        # æ¸…ç†æµ‹è¯•æ–‡ä»¶\n        try:\n            for filename in test_files.keys():
                os.remove(os.path.join(test_doc_dir, filename))\n            os.rmdir(test_doc_dir)\n            print(f"\\nğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶å®Œæˆ")
            \n        except Exception:\n            pass  # å¿½ç•¥æ¸…ç†é”™è¯¯\n        \n        self.exercises_completed.append("document_loaders_integration")\n    \n    def demo_basic_retrieval_chain(self):\n        """æ¼”ç¤ºåŸºç¡€æ£€ç´¢é“¾æ„å»º"""
        self._log("åŸºç¡€RAGæ£€ç´¢é“¾æ„å»ºæ¼”ç¤º")\n        print("-" * 60)
        \n        print("ğŸ”— RAGåŸºæœ¬å·¥ä½œæµç¨‹:")
        workflow = [\n            ("æ–‡æ¡£è¾“å…¥", "åŠ è½½å¹¶å¤„ç†åŸå§‹æ–‡æ¡£" ),\n            ("æ–‡æœ¬åˆ†å‰²", "å°†é•¿æ–‡æœ¬åˆ†å—"),\n            (\"å‘é‡ç”Ÿæˆ\", \"ä¸ºæ¯ä¸ªåˆ†å—ç”ŸæˆåµŒå…¥å‘é‡\"),\n            (\"å‘é‡å­˜å‚¨\",\"å°†å‘é‡å­˜å‚¨åˆ°æ•°æ®åº“\"),\n            (\"ç”¨æˆ·æŸ¥è¯¢\", \"æ¥æ”¶ç”¨æˆ·é—®é¢˜\"),\n            (\"æŸ¥è¯¢å‘é‡åŒ–\", \"å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡\"),\n            (\"ç›¸ä¼¼åº¦æœç´¢\", \"åœ¨å‘é‡åº“ä¸­æŸ¥æ‰¾ç›¸ä¼¼åˆ†å—\"),\n            (\"ä¸Šä¸‹æ–‡ç»„è£…\", \"å°†ç›¸å…³åˆ†å—ç»„åˆä¸ºä¸Šä¸‹æ–‡\"),\n            (\"ç”Ÿæˆå›ç­”\",\"ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ\")\n        ]\n        \n        print("   RAGå¤„ç†æµç¨‹:")\n        for i, (step, desc) in enumerate(workflow, 1):\n            print(f"      {i}. {step}: {desc}")\n        print()
        \n        print("ğŸ§ª åŸºç¡€RAGæ£€ç´¢é“¾æ„å»º:")
        \n        # æ¨¡æ‹Ÿçš„RAGå®ç° (ä¸ä¾èµ–çœŸå®æ¨¡å‹)\n        class SimpleRAGChain:\n            """ç®€åŒ–çš„RAGæ£€ç´¢é“¾å®ç°"""\n            \n            def __init__(self, name="åŸºç¡€RAGé“¾"):\n                self.name = name\n                self.documents = []\n                self.embeddings = {}  # ç®€åŒ–çš„å‘é‡å­˜å‚¨
                self.split_docs = []\n                \n            def split_documents(self, documents: List[Document]) -> List[Document]:\n                """æ–‡æ¡£åˆ†å— (ç®€åŒ–ç‰ˆ)"""
                return documents  # ç®€åŒ–ï¼šæŒ‰åŸæ–‡æ¡£è¿”å›\n            \n            def create_embeddings(self, documents: List[Document]) -> Dict[str, List[float]]:\n                """ç”Ÿæˆæ¨¡æ‹ŸåµŒå…¥å‘é‡"""
                embeddings = {}\n                for i, doc in enumerate(documents):\n                    # ä½¿ç”¨ç®€å•çš„æ¨¡æ‹Ÿå‘é‡ (é¿å…ä¾èµ–çœŸå®æ¨¡å‹)\n                    text_hash = hash(doc.page_content) % 1000\n                    vector = [float((text_hash * (j + 1)) % 100) / 100.0 for j in range(64)]\n                    doc_id = f\"doc_{i}\"\n                    self.embeddings[doc_id] = vector\n                return self.embeddings\n            \n            def similarity_search(self, query: str, top_k: int = 2) -> List[Document]:\n                ""\"ç›¸ä¼¼åº¦æœç´¢ (ç®€åŒ–å®ç°)""\"\n                # æ¨¡æ‹ŸæŸ¥è¯¢å‘é‡\n                query_hash = hash(query) % 1000\n                query_vector = [float((query_hash * (j + 1)) % 100) / 100.0 for j in range(64)]\n                \n                # è®¡ç®—ä¸æ¯ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦\n                similarities = []\n                for doc_id, doc_vec in self.embeddings.items():\n                    similarity = self._cosine_similarity(query_vector, doc_vec)\n                    similarities.append((doc_id, similarity))\n                \n                # æ’åºå¹¶è¿”å›æœ€ç›¸ä¼¼çš„\n                similarities.sort(key=lambda x: x[1], reverse=True)\n                \n                # ä»åŸå§‹æ–‡æ¡£ä¸­æ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£ (ç®€åŒ–ç‰ˆæœ¬)\n                result_docs = []\n                for doc_id, score in similarities[:top_k]:\n                    doc_index = int(doc_id.split('_')[1])\n                    if doc_index \u003c len(self.documents):\n                        doc = self.documents[doc_index]\n                        result_docs.append(doc)\n                \n                return result_docs\n            \n            def _cosine_similarity(self, vec1, vec2) -> float:\n                """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""\n                dot_product = sum(a * b for a, b in zip(vec1, vec2))\n                magnitude1 = sum(a * a for a in vec1) ** 0.5
                magnitude2 = sum(a * a for a in vec2) ** 0.5\n                return dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 \u003e 0 else 0\n            \n            def process_query(self, query: str) -> Dict[str, Any]:\n                """å¤„ç†æŸ¥è¯¢å¹¶è¿”å›ç»“æœ""\"\n                start_time = time.time()\n                \n                # æ–‡æ¡£åˆ†å—\n                split_docs = self.split_documents(self.documents)\n                splits_time = time.time() - start_time\n                \n                # ç”ŸæˆåµŒå…¥ (å¦‚æœè¿˜æ²¡æœ‰)\n                if not self.embeddings:\n                    self.create_embeddings(split_docs)\n                embed_time = time.time() - start_time - splits_time\n                \n                # ç›¸ä¼¼åº¦æœç´¢\n                relevant_docs = self.similarity_search(query, top_k=2)\n                search_time = time.time() - start_time - splits_time - embed_time\n                \n                # ç»„åˆä¸Šä¸‹æ–‡\n                context = "\\n\".join(doc.page_content for doc in relevant_docs)\n                \n                # æ¨¡æ‹ŸLLMç”Ÿæˆå›ç­”\n                generated_answer = self._simulate_llm_response(context, query)\n                \n                total_time = time.time() - start_time\n                \n                return {\n                    "query": query,\n                    "relevant_chunks": [doc.page_content for doc in relevant_docs],\n                    "context": context,\n                    "answer": generated_answer,\n                    "execution_times": {\n                        "splitting": splits_time\n                        \"embedding": embed_time,\n                        \"search\": search_time,\n                        \"total\": total_time\n                    },\n                    \"chunk_count\": len(split_docs)\n                }\n            \n            def _simulate_llm_response(self, context: str, query: str) -> str:\n                """æ¨¡æ‹ŸLLMå“åº”ç”Ÿæˆ""\"\n                # åŸºäºä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢ç”Ÿæˆæ¨¡æ‹Ÿå›ç­”\n                key_concepts = []\n                if \"å‘é‡\" in context:\n                    key_concepts.append(\"å‘é‡æ•°æ®åº“\")\n                if \"åµŒå…¥\" in context:\n                    key_concepts.append(\"åµŒå…¥è¡¨ç¤º\")\n                if \"æœç´¢\" in context:\n                    key_concepts.append(\"ç›¸ä¼¼åº¦æœç´¢\")\n                \n                answer = f\"åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œ'{query}'å¯ä»¥è¿™æ ·ç†è§£ï¼š\\n\\n\"\n                \n                if key_concepts:\n                    answer += \"ç›¸å…³æ¦‚å¿µåŒ…æ‹¬ï¼š\" + \", \".join(key_concepts) + \"ã€‚\\n\"
                \n                answer += \"RAGç³»ç»Ÿé€šè¿‡å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ‰¾åˆ°ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„å†…å®¹ï¼Œä»è€Œæä¾›å‡†ç¡®çš„å›ç­”ã€‚\"\n                \n                return answer\n        \n        # æ¼”ç¤ºåŸºç¡€RAGé“¾æ„å»º\n        rag_chain = SimpleRAGChain(\"åŸºç¡€æ¼”ç¤º\"\")\n        \n        # å‡†å¤‡æµ‹è¯•æ–‡æ¡£ (æ¨¡æ‹ŸçœŸå®çŸ¥è¯†åº“)\n        test_knowledge_docs = [
            Document(page_content=\"\"\"\nå‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢é«˜ç»´å‘é‡çš„æ•°æ®åº“ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®åº“ä¸åŒï¼Œ\nå‘é‡æ•°æ®åº“ä¸“ä¸ºç›¸ä¼¼åº¦æœç´¢ä¼˜åŒ–ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ–‡æœ¬ã€å›¾åƒç­‰éç»“æ„åŒ–æ•°æ®çš„æ£€ç´¢ã€‚
            \"\", metadata={\"topic\": \"å‘é‡æ•°æ®åº“\", \"source\": \"çŸ¥è¯†åº“1\"}),\n            
            Document(page_content=\"\"\"\nRAG (Retrieval-Augmented Generation) æ˜¯ä¸€ä¸ªç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æ¡†æ¶ã€‚\nå®ƒé¦–å…ˆæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨ç”Ÿæˆæ¨¡å‹ç”Ÿç»“åˆä¿¡æ¯å’ŒæŸ¥è¯¢ç”Ÿæˆå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚
            \"\", metadata={\"topic\": \"RAGæ¦‚å¿µ\", \"source\": \"çŸ¥è¯†åº“2\"}),\n            
            Document(page_content=\"\"\"\næ–‡æœ¬åµŒå…¥æ˜¯å°†æ–‡å­—è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹ï¼Œè¿™äº›å‘é‡èƒ½å¤Ÿä¿ç•™æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ã€‚\né€šè¿‡åµŒå…¥æŠ€æœ¯ï¼Œè®¡ç®—æœºå¯ä»¥ç†è§£å’Œè®¡ç®—æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚
            \"\", metadata={\"topic\": \"æ–‡æœ¬åµŒå…¥\", \"source\": \"çŸ¥è¯†åº“3\"})\n        ]\n        \n        rag_chain.documents = test_knowledge_docs\n        \n        # æ‰§è¡ŒæŸ¥è¯¢æ¼”ç¤º\n        test_queries = [\n            \"ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ\",\n            \"RAGç³»ç»Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ\", \n            \"æ–‡æœ¬åµŒå…¥æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n        ]\n        \n        print(f\"\\nğŸš€ æ‰§è¡ŒRAGæŸ¥è¯¢æ¼”ç¤º:")\n        \n        for query in test_queries:\n            print(f\"\\n\" + \"-\" * 50)\n            print(f\"æŸ¥è¯¢: '{query}'\")\n            \n            result = rag_chain.process_query(query)\n            \n            print(f\"\\nğŸ“‹ æ£€ç´¢åˆ°çš„ç›¸å…³åˆ†å— ({len(result['relevant_chunks'])} ä¸ª):")\n            for i, chunk in enumerate(result['relevant_chunks']):\n                print(f\"   {i+1}. {chunk[:80]}...\")\n            \n            print(f\"\\nğŸ¤– ç”Ÿæˆå›ç­”:")\n            print(f\"   {result['answer']}\")\n            \n            print(f\"\\nâ±ï¸ æ€§èƒ½ç»Ÿè®¡:\")\n            for step, duration in result['execution_times'].items():\n                if step != 'total' or step == 'total':\n                    print(f\"   {step.title()}: {duration:.3f}ç§’\")\n        \n        # å¯¹æ¯”åˆ†æ\n        print(f\"\\nğŸ“Š RAGç³»ç»Ÿä¼˜åŠ¿åˆ†æ:")\n        rag_advantages = [
            \"ç»“åˆäº†æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„åˆ›é€ æ€§\",|\n            \"èƒ½å¤Ÿå¤„ç†çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆçš„é—®é¢˜\",\n            \"æä¾›å¯è§£é‡Šçš„å›ç­” (æœ‰å¼•ç”¨æ¥æº)\",\n            \"æ”¯æŒåŠ¨æ€çŸ¥è¯†æ›´æ–°è€Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹\",\n            \"æˆæœ¬æ•ˆç›Šæ›´é«˜ (ç›¸æ¯”å¤§æ¨¡å‹ç›´æ¥ç”Ÿæˆ)\"\n        ]\n        \n        for i, advantage in enumerate(rag_advantages, 1):\n            print(f\"   {i}. {advantage}\")\n        \n        self.exercises_completed.append(\"basic_retrieval_chain\")\n    \n    def generate_week4_summary(self) -> str:\n        """ç”ŸæˆWeek 4å­¦ä¹ æ€»ç»“\"\"\"\n        summary = f\"\"\"\nğŸ“ L2 Intermediate - Week 4: RAGç³»ç»ŸåŸºç¡€å­¦ä¹ æ€»ç»“\n=================================================\n\nâœ… æœ¬å‘¨å®Œæˆå­¦ä¹ å†…å®¹:\n   1. åµŒå…¥å‘é‡åŸºç¡€æ¦‚å¿µå’Œç”Ÿæˆè¿‡ç¨‹\n   2. å¤šç§æ–‡æœ¬åˆ†å‰²ç­–ç•¥å’Œæœ€ä½³å®è·µ\n   3. å‘é‡æ•°æ®åº“æ¯”è¾ƒå’ŒChromaDBä½¿ç”¨\n   4. æ–‡æ¡£åŠ è½½å™¨çš„é›†æˆå’Œåº”ç”¨\n   5. åŸºç¡€RAGæ£€ç´¢é“¾çš„å®Œæ•´å®ç°\n\nğŸ’¡ æ ¸å¿ƒæŠ€èƒ½æŒæ¡:\n   â€¢ ç†è§£å‘é‡å­˜å‚¨çš„å·¥ä½œåŸç†å’Œåº”ç”¨åœºæ™¯\n   â€¢ æŒæ¡ä¸åŒåˆ†å—ç­–ç•¥çš„ä¼˜ç¼ºç‚¹å’Œé€‰æ‹©æ–¹æ³•\n   â€¢ å­¦ä¼šä½¿ç”¨å¤šç§å‘é‡åŒ–æŠ€æœ¯å’Œæ¨¡å‹\n   â€¢ èƒ½å¤Ÿé…ç½®å’Œç®¡ç†å‘é‡æ•°æ®åº“ç³»ç»Ÿ\n   â€¢ èƒ½å¤Ÿæ„å»ºåŸºç¡€RAGé—®ç­”ç³»ç»Ÿ\n\nğŸ› ï¸ å®é™…æŠ€èƒ½å»ºç«‹:\n   â€¢ ChromaDBé…ç½®å’Œä½¿ç”¨\n   â€¢ æ–‡æ¡£åŠ è½½å’Œè§£æ\n   â€¢ æ–‡æœ¬åˆ†å‰²å’Œç­–ç•¥é€‰æ‹©\n   â€¢ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—\n   â€¢ RAGç³»ç»Ÿæ€§èƒ½åˆ†æ\n\nâ­ï¸ ä¸‹ä¸€å‘¨å­¦ä¹ é‡ç‚¹ (Week 5):\n   ğŸ“š é«˜çº§RAGæŠ€æœ¯å’Œä¼˜åŒ–\n   ğŸ›  æ£€ç´¢ç®—æ³•çš„æ·±åº¦ä¼˜åŒ–\n   ğŸš€ å¤šçŸ¥è¯†åº“çš„RAGç³»ç»Ÿé›†æˆ\n   ğŸ¯ å’Œä¸­å›½å¤§æ¨¡å‹ç»“åˆçš„RAGå®è·µ\n\n---\n### ğŸš€ Week 4å®ä¹ç¯åº”ç”¨å»ºè®®:\n   1. é…ç½®å’Œæµ‹è¯•ä¸åŒçš„å‘é‡æ•°æ®åº“\n   2. ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²å‚æ•°æé«˜æ£€ç´¢è´¨é‡\n   3. å®è·µç»“åˆä¸­å›½AIæ¨¡å‹çš„RAGç³»ç»Ÿ\n   4. æµ‹è¯•ä¸åŒåµŒå…¥æ¨¡å‹çš„æ•ˆæœå·®å¼‚\n\"\"\"\n        return summary\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°ï¼šæ‰§è¡ŒWeek 4æ‰€æœ‰RAGåŸºç¡€ç»ƒä¹ \"\"\"\n    print(\"ğŸ¯ LangChain L2 Intermediate - Week 4: RAGç³»ç»ŸåŸºç¡€\")\n    print(\"=\" * 60)\n    print(\"æœ¬å‘¨å°†å­¦ä¹ å¦‚ä½•æ„å»ºåŸºç¡€çš„RAGé—®ç­”ç³»ç»Ÿ\")\n    \n    trainer = RAGBasicsTrainer()\n    \n    try:\n        # ä¾æ¬¡æ‰§è¡Œå„ä¸ªç»ƒä¹ æ¨¡å—\n        trainer.demo_embedding_vectors_basics()\n        trainer.demo_text_splitting_strategies()\n        trainer.demo_vector_databases_setup()\n        trainer.demo_document_loaders_integration()\n        trainer.demo_basic_retrieval_chain()\n        \n        # ç”Ÿæˆå­¦ä¹ æ€»ç»“\n        summary = trainer.generate_week4_summary()\n        print(summary)\n        \n        # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶\n        with open(\"01_rag_basics_summary.md\", \"w\", encoding=\"utf-8\") as f:\n            f.write(summary)\n        \n        print(\"\\nâœ… Week 4 RAGåŸºç¡€å­¦ä¹ å®Œæˆï¼\")\n        print(\"ğŸ“‹ å­¦ä¹ æ€»ç»“å·²ä¿å­˜è‡³ 01_rag_basics_summary.md\")\n        print(\"\\nğŸš€ æ¨èä¸‹ä¸€æ­¥:\")\n        print(\"   1. æµ‹è¯•ä¸åŒçš„å‘é‡æ•°æ®åº“é…ç½®\")\n        print(\"   2. ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²å‚æ•°\")\n        print(\"   3. å°è¯•ç»“åˆä¸­å›½å¤§æ¨¡å‹çš„RAGç³»ç»Ÿ\")\n        print(\"   4. å‡†å¤‡è¿›å…¥Week 5é«˜çº§RAGæŠ€æœ¯å­¦ä¹ \")\n        \n    except KeyboardInterrupt:\n        print(\"\\n\\nâš ï¸ Week 4 RAGåŸºç¡€å­¦ä¹ è¢«ä¸­æ–­\")\n    except Exception as e:\n        print(f\"\\n\\nâŒ å­¦ä¹ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","content_path":"/home/ubuntu/learn_langchain1.0_projects/courses/L2_Intermediate/01_rag_systems/01_vector_stores_basics.py"}