#!/usr/bin/env python3
"""
LangChain L3 Advanced - Week 12  ç»Ÿä¸€æ¥å£å±‚ï¼ˆä¼ä¸šçº§å¤šå¹³å°é›†æˆï¼‰
è¯¾ç¨‹æ ‡é¢˜: å¤šå¹³å°ç»Ÿä¸€AIå·¥ä½œæµAPI
å­¦ä¹ ç›®æ ‡:
  - è®¾è®¡å¹¶å®æ–½ç»Ÿä¸€AIå·¥ä½œæµAPIæ¶æ„
  - å®ç°Dify/RAGFlow/n8næ™ºèƒ½è·¯ç”±é€‰æ‹©
  - å­¦ä¹ ç»Ÿä¸€è®¤è¯ä¸æƒé™ç®¡ç†ä½“ç³»
  - æŒæ¡åŠ¨æ€è´Ÿè½½å‡è¡¡ä¸æ•…éšœè½¬ç§»
ä½œè€…: Claude Code æ•™å­¦å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2024-01-17
ç‰ˆæœ¬: 1.0.0
å…ˆå†³æ¡ä»¶: å®Œæˆ03_n8n_workflow_automation.py
"""

import asyncio
import json
import uuid
import time
import random
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import logging
import httpx
from pydantic import BaseModel, Field, validator
import toml

# ä¼ä¸šçº§é›†æˆä¾èµ–
try:
    from fastapi import FastAPI, HTTPException, Depends, Request, Response
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import JSONResponse
    fastapi_available = True
    print("âœ… FastAPIé›†æˆæˆåŠŸ")
except ImportError:
    fastapi_available = False
    print("âš ï¸ FastAPIå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨åŸºäºæ³›åŒ–HTTPçš„å¤‡ç”¨å®ç°")

try:
    import redis.asyncio as redis
    redis_available = True
    print("âœ… Redisé›†æˆæˆåŠŸï¼Œæ”¯æŒç¼“å­˜å’Œä¼šè¯ç®¡ç†")
except ImportError:
    redis_available = False
    print("âš ï¸ Rediså¼‚æ­¥å®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥")

try:
    from cachetools import TTLCache
    cachetools_available = True
    print("âœ… Cachetools LRUç¼“å­˜ç®¡ç†é›†æˆ")
except ImportError:
    cachetools_available = False
    print("âš ï¸ Cachetoolså¯¼å…¥å¤±è´¥ï¼ˆå¯é€‰ä¾èµ–ï¼‰")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -----------------------------
# ä¼ä¸šçº§æ•°æ®æ¨¡å‹å®šä¹‰
# -----------------------------

class AIPlatform(Enum):
    """AIå·¥ä½œæµå¹³å°æšä¸¾"""
    DIFY = "dify"
    RAGFLOW = "ragflow"
    N8N = "n8n"
    LANGFLOW = "langflow"
    FLOWISE = "flowise"
    CUSTOM = "custom"
    UNIFIED = "unified"  # åŠ¨æ€å¹³å°é€‰æ‹©

class QueryPriority(Enum):
    """æŸ¥è¯¢ä¼˜å…ˆçº§"""
    CRITICAL = "critical"      # å…³é”®å®æ—¶æŸ¥è¯¢
    HIGH = "high"              # é«˜ä¼˜å…ˆçº§
    NORMAL = "normal"          # æ ‡å‡†ä¼˜å…ˆçº§
    BATCH = "batch"           # æ‰¹é‡å¤„ç†

class ResponseFormat(Enum):
    """å“åº”æ ¼å¼"""
    JSON = "json"
    STREAM = "stream"
    MARKDOWN = "markdown"
    XML = "xml"

@dataclass
class EnterpriseAPIConfig:
    """ç»Ÿä¸€ä¼ä¸šAPIç½‘å…³é…ç½®"""
    # åŸºç¡€æœåŠ¡é…ç½®
    api_name: str = "ä¼ä¸šçº§AIç»Ÿä¸€æœåŠ¡API"
    version: str = "v1.0.0"
    api_key: str = ""
    base_url: str = "https://ai-unified-api.enterprise.local"
    listen_port: int = 8000
    
    # å¹³å°é›†æˆç«¯ç‚¹
    platform_endpoints: Dict[str, str] = field(default_factory=lambda: {
        "dify": "http://dify-api:3000/api/v1",
        "ragflow": "http://ragflow-api:9380/api/v1", 
        "n8n": "http://n8n-api:5678/api/v1",
        "langflow": "http://langflow-api:3000/api/v1",
        "custom": "http://custom-api:8080/api/v1"
    })
    
    # æ™ºèƒ½å†³ç­–é…ç½®
    decision_engine_enabled: bool = True
    caching_strategy: str = "redis_memory_hybrid"  # redis/memory/cachetools/hybrid
    fault_tolerance_enabled: bool = True
    concurrent_request_limit: int = 1000
    request_timeout_seconds: int = 60
    
    # ä¼ä¸šçº§æ€§èƒ½
    auto_scaling_enabled: bool = True
    load_balancer_type: str = "intelligent"  # round_robin/intelligent/failover
    circuit_breaker_threshold: float = 0.8
    retry_policy: Dict[str, Any] = field(default_factory=lambda: {
        "max_retries": 3,
        "retry_delay": 1.0,
        "exponential_backoff": True,
        "retry_on": ["timeout", "connection_error", "rate_limit"]
    })
    
    # å®‰å…¨ä¸åˆè§„
    authentication_methods: List[str] = field(default_factory=lambda: ["jwt", "api_key", "sso"])
    encryption_enabled: bool = True
    audit_logging_enabled: bool = True  
    data_retention_days: int = 90
    compliance_standards: List[str] = field(default_factory=lambda: ["ISO27001", "SOC2"])

@dataclass
class UnifiedQueryRequest:
    """ç»Ÿä¸€æŸ¥è¯¢è¯·æ±‚"""
    query: str = Field(..., min_length=1, max_length=2000)
    platform_preference: Optional[str] = Field(default=None, description="å¹³å°åå¥½ AIPlatform")
    context: Optional[List[str]] = Field(default_factory=list, max_items=100)
    priority: QueryPriority = Field(default=QueryPriority.NORMAL)
    response_format: ResponseFormat = Field(default=ResponseFormat.JSON)
    language: str = Field(default="zh", description="å›ç­”è¯­è¨€")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
    request_tracking: Optional[Dict[str, Any]] = None

@dataclass 
class UnifiedQueryResponse:
    """ç»Ÿä¸€æŸ¥è¯¢å“åº”"""
    request_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    query: str
    answer: str
    platform_used: str
    confidence_score: float = 0.0
    sources: List[Dict[str, Any]] = field(default_factory=list)
    processing_time_ms: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    cost_breakdown: Optional[Dict[str, str]] = None
    next_actions: Optional[List[str]] = None
    user_feedback_invited: bool = False
    timestamp: datetime = field(default_factory=datetime.now)

# -----------------------------
# å¹³å°æŠ½è±¡æ¥å£
# -----------------------------

class IPlatformAdapter(ABC):
    """å¹³å°é€‚é…å™¨æ¥å£"""
    
    @abstractmethod
    def get_platform_name(self) -> str:
        """è·å–å¹³å°åç§°"""
        pass
    
    @abstractmethod
    def get_platform_capabilities(self) -> List[str]:
        """è·å–å¹³å°èƒ½åŠ›æ¸…å•"""
        pass
    
    @abstractmethod
    async def execute_query(self, query: UnifiedQueryRequest) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        pass
    
    @abstractmethod
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        pass

# -----------------------------
# å…·ä½“å¹³å°å®ç°
# -----------------------------

class DifyPlatformAdapter(IPlatformAdapter):
    """Difyå¹³å°é€‚é…å™¨"""
    
    def get_platform_name(self) -> str:
        return "dify"
    
    def get_platform_capabilities(self) -> List[str]:
        return ["chat_conversation", "knowledge_base", "document_qa", "workflow_automation", "multi_language"]
    
    async def execute_query(self, query: UnifiedQueryRequest, config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨Dify APIæ‰§è¡ŒæŸ¥è¯¢"""
        
        logger.info(f"ğŸ—£ï¸ è°ƒç”¨Difyå¹³å°æ‰§è¡ŒæŸ¥è¯¢ - æŸ¥è¯¢é•¿åº¦: {len(query.query)}")
        start_time = time.time()
        
        try:
            api_client = httpx.AsyncClient(timeout=30.0)
            
            # æ„å»ºDify APIè¯·æ±‚
            dify_request = {
                "query": query.query,
                "inputs": {
                    "user_id": query.metadata.get("user_id", "anonymous"),
                    "context": query.context or [],
                    "language": query.language,
                    "metadata": query.metadata
                },
                "response_mode": "blocking" if query.response_format == ResponseFormat.JSON else "streaming",
                "user": query.metadata.get("session_id", "unified_api_user")
            }
            
            # é€‰æ‹©åˆé€‚çš„åº”ç”¨ï¼ˆåŸºäºæŸ¥è¯¢ç±»å‹ï¼‰
            app_config = self._select_dify_application(query)
            
            response = await api_client.post(
                f"{config['dify']}/chat/messages",
                json=dify_request,
                headers={"Authorization": f"Bearer {query.metadata.get('dify_app_key', 'demo_key')}"}
            )
            response.raise_for_status()
            
            result = response.json()
            
            # æ ‡å‡†åŒ–å“åº”æ ¼å¼
            return {
                "platform": "dify",
                "answer": result.get("answer", ""),
                "confidence": result.get("metadata", {}).get("confidence", 0.75),
                "sources": result.get("retrieval_results", []),
                "processing_time": result.get("latency", time.time() - start_time),
                "model_used": result.get("model", "glm-4"),
                "metadata": result.get("metadata", {})
            }
            
        except Exception as e:
            logger.error(f"Difyå¹³å°æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {"error": str(e), "platform": "dify"}
    
    def _select_dify_application(self, query: UnifiedQueryRequest) -> Dict[str, Any]:
        """æ ¹æ®æŸ¥è¯¢å†…å®¹é€‰æ‹©åˆé€‚çš„Difyåº”ç”¨"""
        
        # ç®€å•çš„åº”ç”¨é€‰æ‹©é€»è¾‘ï¼ˆå®é™…ä¸­åº”ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†ç±»ï¼‰
        if "çŸ¥è¯†" in query.query or "ä¿¡æ¯" in query.query:
            return {"app_name": "ä¼ä¸šçŸ¥è¯†åŠ©æ‰‹", "knowledge_enabled": True}
        elif "å¯¹è¯" in query.query or "èŠå¤©" in query.query:
            return {"app_name": "æ™ºèƒ½å®¢æœ", "chat_template_enabled": True}
        else:
            return {"app_name": "é€šç”¨é—®ç­”åº”ç”¨", "default_settings": True}
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–Difyå¹³å°æ€§èƒ½æŒ‡æ ‡"""
        return {
            "platform": "dify",
            "avg_response_time_ms": 320,
            "uptime_percentage": 99.5,
            "active_users": 1250,
            "trust_score": 0.82,
            "chinese_optimization": True,
            "knowledge_base_support": True
        }

class RAGFlowPlatformAdapter(IPlatformAdapter):
    """RAGFlowå¹³å°é€‚é…å™¨"""
    
    def get_platform_name(self) -> str:
        return "ragflow"
    
    def get_platform_capabilities(self) -> List[str]:
        return ["enterprise_document_qa", "hybrid_retrieval", "chunk_reranking", "aoi_description", "enterprise_security"]
    
    async def execute_query(self, query: UnifiedQueryRequest, config: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨RAGFlow APIæ‰§è¡ŒæŸ¥è¯¢"""
        
        logger.info(f"ğŸ¤– è°ƒç”¨RAGFlowå¹³å° - é«˜çº§ä¼ä¸šæ£€ç´¢")
        start_time = time.time()
        
        try:
            api_client = httpx.AsyncClient(timeout=40.0)
            
            # æ„å»ºRAGFlowæ£€ç´¢è¯·æ±‚
            dataset_id = query.metadata.get("dataset_id", "default_kb")
            
            ragflow_retrieval = {
                "dataset_id": dataset_id,
                "question": query.query,
                "top_k": min(20, query.metadata.get("max_sources", 10)),
                "similarity_threshold": 0.7,
                "rerank": True,
                "language": query.language,
                "metadata": query.metadata
            }
            
            response = await api_client.post(
                f"{config['ragflow']}/retrieval",
                json=ragflow_retrieval,
                headers={"Authorization": f"Bearer {query.metadata.get('ragflow_api_key', 'demo_key')}"}
            )
    response.raise_for_status()
       
            retrieval_result = response.json()
   
 # ç”ŸæˆåŸºäºæ£€ç´¢çš„ç­”æ¡ˆ
   generated_answer = await self._generate_answer_from_ragflow(retrieval_result, query.query)
         
      return {
                "platform": "ragflow",
   "answer": generated_answer["answer"],
       "confidence": generated_answer["confidence"],
                "sources": retrieval_result.get("chunks", []),
          "processing_time": generated_answer.get("processing_time", random.uniform(400, 800)),
      "model_used": generated_answer.get("model_used", "bge-reranker"),
     "metadata": generated_answer.get("metadata", {})
            }
            
        except Exception as e:
        logger.error(f"RAGFlowå¹³å°æŸ¥è¯¢å¤±è´¥: {str(e)}")
      return {"error": str(e), "platform": "ragflow"}
  
    async def _generate_answer_from_ragflow(self, retrieval_result: Dict[str, Any], 
  query: str) -> Dict[str, Any]:
    """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        
       if not retrieval_result.get("chunks"):
      return {
  "answer": "æ ¹æ®ä¼ä¸šçŸ¥è¯†åº“æ£€ç´¢ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "confidence": 0.0,
              "model_used": "ragflow",
      "metadata": {"error": "no_relevant_documents"}
   }
    
        # ç”Ÿæˆæ¨¡æ‹Ÿå›ç­”ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨LLMç”Ÿæˆï¼‰
        chunks = retrieval_result["chunks"]
        avg_confidence = sum(chunk.get("score", 0.0) for chunk in chunks) / len(chunks)
    
        return {
          "answer": f"åŸºäºä¼ä¸šçŸ¥è¯†åº“æ£€ç´¢åˆ° {len(chunks)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œä¸»è¦å†…å®¹æ¶µç›–äº†æ‚¨çš„æŸ¥è¯¢ï¼š...",
     "confidence": avg_confidence,
    "model_used": "ragflow_enterprise_glm",
            "processing_time": len(chunks) * 35 + 100,  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            "metadata": {"chunks_analyzed": len(chunks)}
        }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–RAGFlowå¹³å°æ€§èƒ½æŒ‡æ ‡"""
        return {
    "platform": "ragflow",
            "avg_response_time_ms": 680,
    "uptime_percentage": 99.2,
            "retrieval_accuracy": 0.88,
    "enterprise_grade": True,
       "document_processing_metrics": True,
  "security_classification": "enterprise"
        }

class N8NPlatformAdapter(IPlatformAdapter):
  """n8nå¹³å°é€‚é…å™¨"""
    
    def get_platform_name(self) -> str:
 return "n8n"
    
    def get_platform_capabilities(self) -> List[str]:
   return ["workflow_automation", "multi_step_processing", "webhook_integration", "business_task_layout", "notification_systems"]
    
    async def execute_query(self, query: UnifiedQueryRequest) -> Dict[str, Any]:
 """è°ƒç”¨n8nå·¥ä½œæµæ‰§è¡Œå¤æ‚å¤„ç†"""
        
        logger.info(f"âš™ï¸ è°ƒç”¨n8nå¹³å°å·¥ä½œæµ - å¤æ‚å¤šæ­¥éª¤å¤„ç†")
    
        try:
     config = self.client.config.platform_endpoints["n8n"]
            
   # é€‰æ‹©åˆé€‚çš„n8nå·¥ä½œæµè¿è¡Œ
            workflow_params = {
     "query": query.query,
     "context": query.context,
                "priority": query.priority.value,
      "metadata": query.metadata,
                "enterprise_session": query.metadata.get("enterprise_session_id", "default")
            }
            
     # æ‰§è¡Œå·¥ä½œæµï¼ˆè¿™é‡Œç®€åŒ–ä¸ºè°ƒç”¨å·¥ä½œæµæ‰§è¡Œï¼‰
    workflow_result = await self._execute_enterprise_workflow(config, workflow_params)
     
        return {
         "platform": "n8n",
      "answer": workflow_result.get("final_output", ""),
                "confirmation": workflow_result.get("confirmation", "æ“ä½œæˆåŠŸå®Œæˆ"), 
    "suggested_actions": workflow_result.get("next_actions", []),
         "processing_time": workflow_result.get("processing_time", random.uniform(300, 900)),
                "model_used": workflow_result.get("primary_model", "enterprise_pipeline"),
        "metadata": {
             "workflow_steps": workflow_result.get("steps_executed", 1),
         "notifications_sent": workflow_result.get("notifications", 0),
            "business_logic_completed": True
     }
      }
      
        except Exception as e:
         logger.error(f"n8nå¹³å°å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
    return {"error": str(e), "platform": "n8n"}
    
    async def _execute_enterprise_workflow(self, config: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼ä¸šçº§n8nå·¥ä½œæµ"""
   
        api_client = httpx.AsyncClient(timeout=60.0)
   
        async with api_client:
            try:
                response = await api_client.post(
   f"{config}/workflows/execute",
                    json=params
                )
       response.raise_for_status()
       return response.json()
     
            except Exception:
     # å›é€€åˆ°æ¨¡æ‹Ÿå·¥ä½œæµç»“æœ
                return {
  "final_output": f"å¤æ‚çš„ä¸šåŠ¡å¤„ç†å·²è®¡åˆ’å¹¶å®Œæˆã€‚å‚æ•°: {len(json.dumps(params))} å­—ç¬¦",
      "confirmation": "å·¥ä½œæµæ‰§è¡Œç¡®è®¤",
          "next_actions": ["ç­‰å¾…äººå·¥å®¡æ ¸", "å‘é€é€šçŸ¥"],
           "steps_executed": 3,
                    "processing_time": random.uniform(200, 800)
   }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–n8nå¹³å°æ€§èƒ½æŒ‡æ ‡"""
        return {
     "platform": "n8n",
      "avg_workflow_time_ms": 750,
         "workflow_success_rate": 0.94,
     "step_execution_reliability": 0.96,
            "enterprise_automation_grade": True,
            "multi_step_complexity": "advanced",
  "api_integration_capable": True
    }

# -----------------------------
# æ™ºèƒ½å†³ç­–å¼•æ“
# -----------------------------

class EnterpriseDecisionEngine:
 """ä¼ä¸šçº§AIå¹³å°æ™ºèƒ½å†³ç­–å¼•æ“"""
    
    def __init__(self, config: EnterpriseAPIConfig):
        self.config = config
  self.platform_quality_matrix = self._initialize_quality_matrix()
     self.usage_metrics_history = self._initialize_metrics_history()
        self.recent_performance_stats = {}
        
        logger.info("ğŸ§  Enterprise AIå†³ç­–å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_quality_matrix(self) -> Dict[str, Dict[str, float]]:
    """åˆå§‹åŒ–å¹³å°è´¨é‡è¯„ä¼°çŸ©é˜µ"""
        
        # é¢„é…ç½®çš„å¹³å°èƒ½åŠ›è¯„åˆ†
        return {
            "dify": {
     "chat_conversation": 0.85,
            "knowledge_base": 0.88,
 "document_qa": 0.82,
        "workflow_automation": 0.75,
         "multi_language": 0.90,
            "enterprise_grade": 0.78
    },
   "ragflow": {
                "enterprise_document_qa": 0.92,
     "hybrid_retrieval": 0.89,
          "chunk_reranking": 0.87,
       "aoi_description": 0.85,
          "enterprise_security": 0.93,
       "scalability": 0.90
    },
     "n8n": {
 "workflow_automation": 0.95,
    "multi_step_processing": 0.90,
          "webhook_integration": 0.88,
                "business_task_layout": 0.86,    
      "notification_systems": 0.85,
    "enterprise_integration": 0.82
   }
  }
    
    def _initialize_metrics_history(self) -> Dict[str, List[Dict[str, Any]]]:
  """åˆå§‹åŒ–å†å²æ€§èƒ½æŒ‡æ ‡"""
        return {platform: [] for platform in AIPlatform}
    
    def select_best_platform(self, query_request: UnifiedQueryRequest) -> str:
  """é€‰æ‹©æœ€é€‚åˆçš„AIå¹³å°"""
      
        query_content = query_request.query
    query_intent = self._analyze_query_intent(query_content)
    performance_requirements = self._determine_performance_requirements(query_request)
        
   logger.info(f"ğŸ§  å†³ç­–å¼•æ“é€‰æ‹©æœ€ä½³å¹³å° - Intent: {query_intent}, Priority: {query_request.priority}")
    
        # å¹³å°è¯„åˆ†çŸ©é˜µ
platform_scores = {}
        
        for platform in self.platform_quality_matrix:
    score = self._calculate_platform_score(
platform, 
       query_intent, 
         performance_requirements,
      query_request
         )
            platform_scores[platform] = score
        
        # é€‰æ‹©æœ€é«˜åˆ†å¹³å°
     best_platform = max(platform_scores, key=platform_scores.get)
        
   logger.info(f"âœ… å†³ç­–ç»“æœ - é€‰æ‹©å¹³å°: {best_platform}, è¯„åˆ†: {platform_scores[best_platform]:.3f}")
        return best_platform
    
    def _analyze_query_intent(self, query: str) -> str:
 """åˆ†ææŸ¥è¯¢æ„å›¾"""
 
        # ç®€å•çš„å…³é”®å­—æ„å›¾åˆ†æ
        query_lower = query.lower()
        
        intent_keywords = {
            "knowledge_based": ["çŸ¥è¯†", "ä¿¡æ¯", "æ˜¯ä»€ä¹ˆ", "å®šä¹‰", "æŒ‡å¯¼", "è§£é‡Š", "è¯´æ˜", "åŸºäºçŸ¥è¯†åº“"],
       "document_search": ["æ–‡æ¡£", "èµ„æ–™", "æ–‡ä»¶", "PDF", "è®ºæ–‡", "æŠ¥å‘Š", "æ‰‹å†Œ", "ä»æ–‡æ¡£"],
    "workflow_automation": ["è‡ªåŠ¨", "æµç¨‹", "å¤„ç†", "è§¦å‘", "å®‰æ’", "å®šæ—¶", "æ‰¹æ¬¡", "å·¥ä½œæµ"],
       "simple_conversation": ["é—®é¢˜", "èŠå¤©", "å¯¹è¯", "è¯·é—®", "å¦‚ä½•", "æ€ä¹ˆ"],
      "data_analysis": ["åˆ†æ", "æ•°æ®", "æŠ¥å‘Š", "ç»Ÿè®¡", "å›¾è¡¨", "æŒ‡æ ‡", "è¶‹åŠ¿"]
        }
 
        # åŒ¹é…æœ€ç›¸å…³çš„æ„å›¾
 best_match = "general_conversation"
       max_score = 0
        
  for intent, keywords in intent_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > max_score:
                max_score = score
              best_match = intent
        
        return best_match
    
    def _determine_performance_requirements(self, request: UnifiedQueryRequest) -> Dict[str, Any]:
 """ç¡®å®šæ€§èƒ½è¦æ±‚"""
        
        priority_requirements = {
   QueryPriority.CRITICAL: {
    "latency_sla_ms": 1000,  # ç§’å“åº”
      "availability_requirement": 0.999,
                "cold_start_tolerance": "none",
        "concurrent_user_support": 1000
     },
     QueryPriority.HIGH: {
     "latency_sla_ms": 2000,
    "availability_requirement": 0.995,
            "cold_start_tolerance": "minimal",
     "concurrent_user_support": 500
   },
    QueryPriority.NORMAL: {
            "latency_sla_ms": 5000,
      "availability_requirement": 0.99,
   "cold_start_tolerance": "acceptable",
                "concurrent_user_support": 100
     },
   QueryPriority.BATCH: {
   "latency_sla_ms": 30000,
             "availability_requirement": 0.95,
    "cold_start_tolerance": "acceptable"
       }
        }
    
        return priority_requirements.get(request.priority, priority_requirements[QueryPriority.NORMAL])
    
    def _calculate_platform_score(self, platform: str, query_intent: str,
    performance_requirements: Dict[str, Any], query_request: UnifiedQueryRequest) -> float:
    """è®¡ç®—å¹³å°è¯„åˆ†"""
      
        # æ„å›¾åŒ¹é…åº¦ï¼ˆæƒé‡40%ï¼‰
        intent_match_score = self._get_intent_match_score(platform, query_intent)
        
   # æ€§èƒ½åŒ¹é…åº¦ï¼ˆæƒé‡30%ï¼‰
        performance_score = self._get_performance_match_score(platform, performance_requirements)
      
     # è¿‘æœŸè¡¨ç°è¯„åˆ†ï¼ˆæƒé‡20%ï¼‰
 recent_performance_score = self._get_recent_performance_score(platform)
        
        # é«˜çº§ç‰¹æ€§è¯„åˆ†ï¼ˆæƒé‡10%ï¼‰
        advanced_features_score = self._get_advanced_features_score(platform, query_request)
        
        # æƒé‡æ€»è®¡ç®—
    total_score = (intent_match_score * 0.4 +
        performance_score * 0.3 +
                       recent_performance_score * 0.2 +
                       advanced_features_score * 0.1)
  
        logger.debug(f"å¹³å° {platform} è¯„åˆ†: {total_score:.3f} (æ„å›¾:{intent_match_score:.2f}, "
f"æ€§èƒ½:{performance_score:.2f}, è¿‘æœŸ:{recent_performance_score:.2f}, é«˜çº§:{advanced_features_score:.2f})")
     
        return total_score
    
    def _get_intent_match_score(self, platform: str, intent: str) -> float:
    """è·å–æ„å›¾åŒ¹é…åº¦åˆ†æ•°"""
   
        platform_capabilities = self.platform_quality_matrix.get(platform, {})
    
        # æ„å›¾åˆ°èƒ½åŠ›æ˜ å°„
        intent_capability_map = {
    "knowledge_based": ["knowledge_base", "document_qa"],
            "document_search": ["enterprise_document_qa", "hybrid_retrieval"],
            "workflow_automation": ["workflow_automation", "multi_step_processing"],
          "simple_conversation": ["chat_conversation", "workflow_automation"],
       "data_analysis": ["business_task_layout", "workflow_automation"]
        }
        
   capabilities_for_intent = intent_capability_map.get(intent, [])
        
        matching_score = 0.0
        for capability in capabilities_for_intent:
         if capability in platform_capabilities:
            matching_score += platform_capabilities[capability]
           
        return matching_score / len(capabilities_for_intent) if capabilities_for_intent else 0.5
   
    def _get_performance_match_score(self, platform: str, requirements: Dict[str, Any]) -> float:
   """è·å–æ€§èƒ½éœ€æ±‚åŒ¹é…åº¦åˆ†æ•°"""
      
        # è·å–å¹³å°å†å²æŒ‡æ ‡
        ava_data = self.recent_performance_stats.get(platform, {})
        
  latency_match = min(1.0, requirements["latency_sla_ms"] / (avg_data.get("avg_response_time_ms", 1000) or 1000))
        availability_match = avg_data.get("uptime_percentage", 0.98) / requirements["availability_requirement"]
        
        return (latency_match + availability_match) / 2.0
 
    def _get_recent_performance_score(self, platform: str) -> float:
        """è·å–è¿‘æœŸè¡¨ç°è¯„åˆ†"""
 
        # ç®€åŒ–å®ç° - åŸºäºå¯ç”¨æ€§å’Œå“åº”æ—¶é—´
        recent_metrics = self.recent_performance_stats.get(platform, {})
        
        uptime_score = recent_metrics.get("uptime_percentage", 0.95)
        response_score = recent_metrics.get("avg_response_time_ms", 1000) / 1000.0
    
        return (uptime_score - response_score * 0.1)  # ç¨å¾®æƒ©ç½šé«˜å»¶è¿Ÿ
    
    def _get_advanced_features_score(self, platform: str, query_request: UnifiedQueryRequest) -> float:
        """è·å–é«˜çº§ç‰¹æ€§åŒ¹é…åº¦åˆ†æ•°"""
   
    score = 0.0
        
        # ä¸­æ–‡å¤„ç†ç‰¹æ€§
        if query_request.language == "zh":
    chinese_support_score = {
    "deepseek": 0.9,
  "zhipu": 0.95,
        "moonshot": 0.88,
   "n8n": 0.7
        }
            score += chinese_support_score.get(platform, 0.6)
  
        # ä¼ä¸šçº§ç‰¹æ€§
        if query_request.priority in [QueryPriority.HIGH, QueryPriority.CRITICAL]:
            enterprise_score = {
         "ragflow": 0.95,
       "dify": 0.82,
       "n8n": 0.75
     }
       score += enterprise_score.get(platform, 0.5)
        
        # APIå®Œæ•´æ€§è¯„åˆ†
        api_quality = {
     "ragflow": 0.88,
    "dify": 0.85, 
      "n8n": 0.90
        }
    score += api_quality.get(platform, 0.75)
        
     return score / sum([1 for _ in chinese_support_score.values())  # æ ‡å‡†åŒ–åˆ†æ•°
    
    def update_performance_metrics(self, platform: str, metrics: Dict[str, Any]) -> None:
        """æ›´æ–°å¹³å°æ€§èƒ½æŒ‡æ ‡"""
<
        # æ›´æ–°å†å²è®°å½•
     self.recent_performance_stats[platform] = metrics
    
        # ä¿å­˜åˆ°å†å²ï¼ˆä¿ç•™æœ€æ–°çš„100æ¡è®°å½•ï¼‰
    if platform not in self.usage_metrics_history:
            self.usage_metrics_history[platform] = []
      
        # æ·»åŠ æ–°è®°å½•
        self.usage_metrics_history[platform].append({
            "timestamp": datetime.now().isoformat(),
            **metrics
    })
        
        # ä¿æŒå†å²å¤§å°ï¼ˆç®€å•çš„LRUï¼‰
        if len(self.usage_metrics_history[platform]) > 100:
 self.usage_metrics_history[platform] = self.usage_metrics_history[platform][-100:]
    
     logger.info(f"ğŸ“ˆ å¹³å° {platform} æ€§èƒ½æŒ‡æ ‡å·²æ›´æ–°")

# -----------------------------
# ç»Ÿä¸€APIç½‘å…³å®ç°
# -----------------------------

class EnterpriseUnifiedAIAPI:
    """ä¼ä¸šçº§ç»Ÿä¸€AIå·¥ä½œæµAPIç½‘å…³"""
 
    def __init__(self, config: EnterpriseAPIConfig):
        self.config = config
     self.decision_engine = EnterpriseDecisionEngine(config)
        self.platform_adapters = self._initialize_platform_adapters()
        
    # ç¼“å­˜å’Œä¼šè¯ç®¡ç†
 self.cache_manager = self._initialize_cache_manager()
        self.session_manager = self._initialize_session_manager()
        
        # è¯·æ±‚é™æµå’ŒQoS
     self.rate_limiter = EnterpriseRateLimiter(config)
        self.qos_manager = EnterpriseQoSManager()
        
        logger.info("ğŸŒ ä¼ä¸šçº§ç»Ÿä¸€AIå·¥ä½œæµAPIç½‘å…³åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_platform_adapters(self) -> Dict[str, IPlatformAdapter]:
        """åˆå§‹åŒ–å¹³å°é€‚é…å™¨"""
     adapters = {}
        
   for platform_name in AIPlatform:
            platform_enum = platform_name.value
            
            if platform_enum == "dify":
       adapters[platform_enum] = DifyPlatformAdapter()
       elif platform_enum == "ragflow":
           adapters[platform_enum] = RAGFlowPlatformAdapter()
            elif platform_enum == "n8n":
   adapters[platform_enum] = N8NPlatformAdapter()
    # å¯ä»¥æ·»åŠ æ›´å¤šå¹³å°é€‚é…å™¨
            else:
     adapters[platform_enum] = None  # æœªå®ç°çš„å¹³å°
 
    return {k: v for k, v in adapters.items() if v is not None}
    
    def _initialize_cache_manager(self):
 """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        
        if redis_available:
            try:
    return RedisCacheManager()
            except Exception as e:
    logger.warning(f"Redisç¼“å­˜åˆå§‹åŒ–å¤±è´¥ {e}ï¼Œå›é€€åˆ°å†…å­˜ç¼“å­˜")
        
        if cachetools_available:
     return CacheToolsManager()
         
  # æœ€ç®€å•çš„ç¼“å­˜ç®¡ç†å™¨
        return SimpleCacheManager()
    
    def _initialize_session_manager(self):
        """åˆå§‹åŒ–ä¼šè¯ç®¡ç†å™¨"""

        return EnterpriseSessionManager(self.config.session_timeout_minutes)
  
    async def process_unified_query(self, query_request: UnifiedQueryRequest) -> UnifiedQueryResponse:
   """å¤„ç†ç»Ÿä¸€çš„AIæŸ¥è¯¢è¯·æ±‚"""
    
        request_id = str(uuid.uuid4())
      start_time = time.time()
  
 logger.info(f"ğŸŒ æ”¶åˆ°ç»Ÿä¸€æŸ¥è¯¢è¯·æ±‚ [RID:{request_id}] - Query: {query_request.query[:50]}...")
    
# 1. è¯·æ±‚éªŒè¯å’Œé¢„å¤„ç†
        is_valid, validation_error = self._validate_query_request(query_request)
  if not is_valid:
     raise HTTPException(status_code=400, detail=validation_error)
        
   # 2. æ£€æŸ¥è¯·æ±‚é™æµ
        if not await self.rate_limiter.is_request_allowed(request_id):
   raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•")
        
        # 3. æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
     cached_response = await self.cache_manager.get_cached_response(query_request)
        if cached_response:
            cached_response["cache_hit"] = True
            logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­è¿”å› - [RID:{request_id}]")
            return cached_response
        
  # 4. æ™ºèƒ½å¹³å°é€‰æ‹©
        best_platform = self.decision_engine.select_best_platform(query_request)
  
     # 5. è¯·æ±‚ä¼˜å…ˆçº§å¤„ç†
        await self.qos_manager.handle_priority(query_request)
    
 # 6. æ‰§è¡Œå¹³å°æŸ¥è¯¢
        try:
            platform_result = await self._execute_platform_query(best_platform, query_request)
     
    # 7. æ„å»ºç»Ÿä¸€å“åº”
        unified_response = await self._build_unified_response(
 query_request, best_platform, platform_result, request_id, start_time
           )
     
    # 8. ç¼“å­˜ç»“æœ
       await self.cache_manager.cache_response(
          query_request, unified_response, ttl_seconds=3600
  )
  
            # 9. æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.update_performance_metrics(best_platform, platform_result)
            
            logger.info(f"âœ… ç»Ÿä¸€æŸ¥è¯¢å¤„ç†å®Œæˆ [RID:{request_id}] - å¹³å°: {best_platform}, "
   f"ç”¨æ—¶: {unified_response.processing_time_ms}ms, ç½®ä¿¡åº¦: {unified_response.confidence_score:.2f}")
        
            return unified_response
            
  except HTTPException:
       raise  # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
               
        except Exception as e:
   logger.error(f"ç»Ÿä¸€æŸ¥è¯¢å¤„ç†å¤±è´¥ [RID:{request_id}]: {str(e)}")
   raise HTTPException(status_code=500, detail="å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯")
    
    def _validate_query_request(self, query_request: UnifiedQueryRequest) -> tuple[bool, str]:
 """éªŒè¯æŸ¥è¯¢è¯·æ±‚çš„æœ‰æ•ˆæ€§"""
        
        if not query_request.query or len(query_request.query.strip()) == 0:
  return False, "Queryå­—æ®µä¸èƒ½ä¸ºç©º"
        
        if len(query_request.query) > 2000:
            return False, "Queryé•¿åº¦ä¸èƒ½è¶…è¿‡2000å­—ç¬¦"
    
        if query_request.priority == QueryPriority.CRITICAL:
            if re.search(r"[\"'<>", query_request.query):
   return False, "é«˜ä¼˜å…ˆçº§æŸ¥è¯¢åŒ…å«æ½œåœ¨å®‰å…¨é£é™©çš„å­—ç¬¦"
        
 return True, ""
    
    async def _execute_platform_query(self, platform: str, query_request: UnifiedQueryRequest) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“å¹³å°çš„æŸ¥è¯¢"""
   
        adapter = self.platform_adapters.get(platform)
        if not adapter:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„AIå¹³å°: {platform}")
       
     # åŠ ä¸Šä¼ä¸šå’Œå®‰å…¨ä¸Šä¸‹æ–‡
     enhanced_query = UnifiedQueryRequest(
    **query_request.__dict__,
            metadata={**query_request.metadata, 
        "platform_execution": platform,
 "enterprise_context": True,
                      "access_token": self._generate_temp_access_token()}
      )
        
        logger.debug(f"ğŸš€ æ‰§è¡Œå¹³å°æŸ¥è¯¢ - Platform: {platform}")
        
        try:
            return await adapter.execute_query(enhanced_query)
            
        except Exception as e:
            logger.error(f"å¹³å° {platform} æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
       if self.config.fault_tolerance_enabled:
       # æ•…éšœè½¬ç§»åˆ°å¤‡é€‰å¹³å°
        return await self._failover_to_alternative_platform(query_request, platform)
         else:
        raise HTTPException(status_code=503, detail=f"AIå¹³å° {platform} æš‚æ—¶ä¸å¯ç”¨")
    
    async def _failover_to_alternative_platform(self, query_request: UnifiedQueryRequest, 
                                             failed_platform: str) -> Dict[str, Any]:
    """æ•…éšœè½¬ç§»åˆ°å¤‡é€‰AIå¹³å°"""
 
        fallback_order = self._get_fallback_platform_order(failed_platform)
        
    logger.warning(f"âš ï¸ å¼€å§‹æ•…éšœè½¬ç§» - åŸå¹³å°: {failed_platform}")
   
  for fallback_platform in fallback_order:
            if fallback_platform == failed_platform:
     continue
     
       logger.info(f"\u200fâš¡ å°è¯•æ•…éšœè½¬ç§» - Fallback Platform: {fallback_platform}")
    
            try:
  fallback_result = await self._execute_platform_query(fallback_platform, query_request)
          fallback_result["fallback_enabled"] = True
    return fallback_result
        
           except Exception as fallback_e:
 ((å¯¤)
       logger.warning(f"æ•…éšœè½¬ç§»åˆ° {fallback_platform} å¤±è´¥: {fallback_e}")
   continue
        
    # æ‰€æœ‰å¤±è´¥
        raise HTTPException(status_code=503, detail="æ‰€æœ‰AIå¹³å°å‡ä¸å¯ç”¨")
 
    def _get_fallback_platform_order(self, failed_platform: str) -> List[str]:
 """è·å–æ•…éšœè½¬ç§»å¤‡é€‰å¹³å°é¡ºåº"""
        
        # å®šä¹‰å¹³å°æ•…éšœè½¬ç§»ä¼˜å…ˆçº§
        fallback_strategy = {
            "dify": ["ragflow", "n8n", "langflow"],
     "ragflow": ["dify", "n8n", "langflow"],
        "n8n": ["dify", "ragflow", "langflow"],
     "langflow": ["dify", "ragflow", "n8n"],
       "flowise": ["dify", "ragflow", "langflow"]
        }
   
        return fallback_strategy.get(failed_platform, ["dify", "ragflow", "n8n"])
    
    async def _build_unified_response(self, query_request: UnifiedQueryRequest, 
    platform_used: str, platform_result: Dict[str, Any],
                        request_id: str, start_time: float) -> UnifiedQueryResponse:
     """æ„å»ºç»Ÿä¸€æ ¼å¼å“åº”"""
   
        processing_time_ms = int((time.time() - start_time) * 1000)
        
 # æ ‡å‡†åŒ–é”™è¯¯å¤„ç†
     if "error" in platform_result:
    return UnifiedQueryResponse(
         request_id=request_id,
       query=query_request.query,
    answer=f"AIå¤„ç†æ—¶é‡åˆ°é”™è¯¯: {platform_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
            platform_used=platform_used,
      confidence_score=0.0,
            processing_time_ms=processing_time_ms,
 sources=[],
          metadata={"error": True, "platform_error": platform_result.get("error")},
  next_actions=["é‡è¯•è¯·æ±‚", "è”ç³»æŠ€æœ¯æ”¯æŒ"]
        )
      
   # æˆåŠŸçš„æ ‡å‡†åŒ–å“åº”
  return UnifiedQueryResponse(
            request_id=request_id,
            query=query_request.query,
            answer=platform_result.get("answer", ""),
    platform_used=platform_used,
    confidence_score=float(platform_result.get("confidence", 0.0)),
            sources=platform_result.get("sources", []),
         processing_time_ms=processing_time_ms,
            model_used=platform_result.get("model_used", platform_used),
     metadata={
         "platform_metadata": platform_result.get("metadata", {}),
        "rightaway_from_cache": False,
  "enterprise_class": True,
       },
  metadata.get("next_actions", []),
  user_feedback_invited=True if platform_result.get("confidence", 0) < 0.5 else False
        )
    
    def _generate_temp_access_token(self) -> str:
     """ç”Ÿæˆä¸´æ—¶è®¿é—®ä»¤ç‰Œï¼ˆç”¨äºAPIé—´éªŒè¯ï¼‰"""
        return f"iat_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    def update_performance_metrics(self, platform: str, result_data: Dict[str, Any]) -> None:
        """æ›´æ–°å¹³å°æ€§èƒ½æŒ‡æ ‡"""
        
 processing_time = result_data.get("processing_time", 0)
    success = "error" not in result_data
    confidence = result_data.get("confidence", 0.5)
        
  performance_metrics = {
            "platform": platform,
  "timestamp": datetime.now().isoformat(),
            "processing_time": processing_time,
     "success": success,
            "confidence": confidence,
       "fallback_enabled": result_data.get("fallback_enabled", False)
     }
        
    self.decision_engine.update_performance_metrics(platform, performance_metrics)
        
    async def get_api_health_status(self) -> Dict[str, Any]:
"""è·å–APIå¥åº·çŠ¶æ€"""
        
        total_requests = sum(metrics["total_executions"] 
for metrics in self.decision_engine.usage_metrics_history.values() 
if metrics)
        
    outage_summary = {}  # ç®€åŒ–çš„çŠ¶æ€è®¡ç®—
        
        return {
     "service_name": self.config.api_name,
            "version": self.config.version,
         "status": "healthy", 
    "total_api_requests": total_requests,
    "uptime_percentage": 99.7,
            "last_updated": datetime.now().isoformat(),
     "platform_statuses": self._generate_platform_health_summary(),
 "performance_summary": self._generate_performance_summary()
        }
    
    def _generate_platform_health_summary(self) -> Dict[str, Dict[str, Any]]:
 """ç”Ÿæˆæ‰€æœ‰å¹³å°çš„å¥åº·çŠ¶æ€æ‘˜è¦"""
        
        status_summary = {}
  
     for platform, adapter in self.platform_adapters.items():
     try:
   performance = adapter.get_performance_metrics()
    reliability_score = performance.get("uptime_percentage", 0.95) * performance.get("trust_score", 0.8)
       
   status_summary[platform] = {
  "status": "healthy" if reliability_score > 0.8 else "degraded",
           "uptime": performance.get("uptime_percentage", 0),
         "avg_latency_ms": performance.get("avg_response_time_ms", 1000),
            "last_check": datetime.now().isoformat()
     }
       
            except Exception as e:
    status_summary[platform] = {
     "status": "unknown",
          "error": str(e),
      "last_check": datetime.now().isoformat()
 }
        
        return status_summary
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
   """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        
        # ç®€åŒ–çš„æ€§èƒ½ç»Ÿè®¡
        avg_processing_time = 580  # é€šç”¨å¹³å‡å“åº”æ—¶é—´
   
        return {
            "average_response_time_ms": avg_processing_time,
            "peak_traffic_time": "business_hours_expected",
      "throughput_capacity": "enterprise_scale",
   "optimization_suggestions": ["è€ƒè™‘å¯ç”¨æ›´æ¿€è¿›çš„ç¼“å­˜ç­–ç•¥", "æ£€æŸ¥æ…¢æŸ¥è¯¢", "ç›‘æ§å¤§è§„æ¨¡æ–‡æ¡£æ£€ç´¢"]
        }

# -----------------------------
# ç¼“å­˜ç®¡ç†å™¨
# -----------------------------

class BaseCacheManager(ABC):
    """åŸºç¡€ç¼“å­˜ç®¡ç†å™¨"""
  
    @abstractmethod
    async def get_cached_response(self, request: UnifiedQueryRequest) -> Optional[UnifiedQueryResponse]:
     """è·å–ç¼“å­˜çš„å“åº”"""
        pass
    
    @abstractmethod
    async def cache_response(self, request: UnifiedQueryRequest, response: UnifiedQueryResponse, ttl_seconds: int = 3600) -> None:
     """ç¼“å­˜å“åº”"""
        pass

class RedisCacheManager(BaseCacheManager):
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = redis.from_url(
            "redis://localhost:6379/8",  # ä¸“ç”¨ç¼“å­˜æ•°æ®åº“
   decode_responses=True
  )
        logger.info("ğŸ”Œ Redisç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–")
    
    def _generate_cache_key(self, request: UnifiedQueryRequest) -> str:
        """ç”Ÿæˆç¼“å­˜Key"""
        key_parts = [
            request.query,
     request.language,  
           request.response_format.value,
    str(request.priority.value),
   json.dumps(request.context, sort_keys=True) if request.context else ""
        ]
        
     # ä½¿ç”¨å“ˆå¸Œæ‘˜è¦é¿å…è¿‡é•¿Key
        import hashlib
       key_data = "|".join(key_parts)
        return f"ai_unified_response:{hashlib.sha256(key_data.encode()).hexdigest()[:24]}"
    
    async def get_cached_response(self, request: UnifiedQueryRequest) -> Optional[UnifiedQueryResponse]:
  """ä»ç¼“å­˜è·å–å“åº”"""
        
        cache_key = self._generate_cache_key(request)
  
        try:
         cached_value = await self.redis_client.get(cache_key)
       if cached_value:
    cached_data = json.loads(cached_value)
             
      # æ ¡éªŒç¼“å­˜å®Œæ•´æ€§
                required_fields = ["query", "answer", "platform_used", "processing_time_ms"]
   if all(field in cached_data for field in required_fields):
          return UnifiedQueryResponse(**cached_data)
     
    
        except Exception as e:
        logger.warning(f"Redisç¼“å­˜è·å–å¤±è´¥: {e}")
    
        return None
    
    async def cache_response(self, request: UnifiedQueryRequest, response: UnifiedQueryResponse, ttl_seconds: int = 3600) -> None:
        """ç¼“å­˜å“åº”åˆ°Redis"""
     
        cache_key = self._generate_cache_key(request)
        
        try:
        # æ ‡å‡†åŒ–æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
     cacheable_data = {
       "query": response.query,
             "answer": response.answer,
   "platform_used": response.platform_used,
          "confidence_score": response.confidence_score,
             "sources": response.sources,
                "processing_time_ms": response.processing_time_ms,
  "metadata": response.metadata,
                "next_actions": response.next_actions,
   "user_feedback_invited": response.user_feedback_invited,
       "timestamp": response.timestamp.isoformat()
            }
      
     await self.redis_client.setex(
cache_key, ttl_seconds, json.dumps(cacheable_data)
      )
       
            logger.debug(f"å·²ç¼“å­˜å“åº” - Key: {cache_key}, TTL: {ttl_seconds}s")
            
        except Exception as e:
   logger.error(f"Redisç¼“å­˜å¤±è´¥: {e}")

class CacheToolsManager(BaseCacheManager):
 """CacheToolså†…å­˜ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        # ç®€å•çš„TTLç¼“å­˜
   self.cache = TTLCache(maxsize=5000, ttl=3600)
   logger.info("ğŸ  CacheToolså†…å­˜ç¼“å­˜åˆå§‹åŒ–")
    
    def _generate_cache_key(self, request: UnifiedQueryRequest) -> str:
        """ç”Ÿæˆç¼“å­˜Key"""
 return hash(str((request.query, request.language, request.response_format)))
    
    async def get_cached_response(self, request: UnifiedQueryRequest) -> Optional[UnifiedQueryResponse]:
 """ä»å†…å­˜ç¼“å­˜è·å–å“åº”"""
        
    try:
    cache_key = self._generate_cache_key(request)
         cached_response = self.cache.get(cache_key)
       
            if cached_response and isinstance(cached_response, UnifiedQueryResponse):
    return cached_response
    
        except Exception as e:
     logger.warning(f"å†…å­˜ç¼“å­˜è·å–å¤±è´¥: {e}")
        
        return None
    
    async def cache_response(self, request: UnifiedQueryRequest, response: UnifiedQueryResponse, ttl_seconds: int = 3600) -> None:
    """ç¼“å­˜å“åº”åˆ°å†…å­˜"""
    
        try:
            cache_key = self._generate_cache_key(request)
   self.cache[cache_key] = response
        
            logger.debug(f"å·²ç¼“å­˜å“åº”åˆ°å†…å­˜ - Key: {cache_key}")
            
        except Exception as e:
      logger.error(f"å†…å­˜ç¼“å­˜å¤±è´¥: {e}")

class SimpleCacheManager(BaseCacheManager):
    """ç®€å•å†…å­˜ç¼“å­˜ç®¡ç†å™¨ï¼ˆæœ€åå›é€€æ–¹æ¡ˆï¼‰"""
 
    def __init__(self):
        self.cache = {}
        logger.info("ğŸ  ç®€å•å†…å­˜ç¼“å­˜åˆ›å»ºï¼ˆå›é€€æ–¹æ¡ˆï¼‰")
    
    def _generate_cache_key(self, request: UnifiedQueryRequest) -> str:
        """ç”Ÿæˆç¼“å­˜Key"""
     return f"simple_cached_{hash(str(request))}"
    
    async def get_cached_response(self, request: UnifiedQueryRequest) -> Optional[UnifiedQueryResponse]:
        """ä»ç®€å•å†…å­˜ç¼“å­˜è·å–å“åº”"""
    
        try:
            cache_key = self._generate_cache_key(request)
    return self.cache.get(cache_key)
  
        except Exception as e:
    logger.warning(f"ç®€å•ç¼“å­˜è·å–å¤±è´¥: {e}")
   
        return None
    
    async def cache_response(self, request: UnifiedQueryRequest, response: UnifiedQueryResponse, ttl_seconds: int = 3600) -> None:
   """ç¼“å­˜å“åº”åˆ°ç®€å•å†…å­˜"""
 
        try:
    cache_key = self._generate_cache_key(request)
            self.cache[cache_key] = response
      
   logger.debug(f"å·²ç¼“å­˜å“åº”ï¼ˆç®€å•æ¨¡å¼ï¼‰ - Key: {cache_key}")
            
        except Exception as e:
       logger.error(f"ç®€å•ç¼“å­˜å¤±è´¥: {e}")

# -----------------------------
# è¾…åŠ©ç®¡ç†å™¨
# -----------------------------

class EnterpriseRateLimiter:
    """ä¼ä¸šçº§è¯·æ±‚é™æµç®¡ç†å™¨"""
    
    def __init__(self, config: EnterpriseAPIConfig):
        self.config = config
    self.platform_limits = {}
   self.user_limits = {}
    
        # åˆå§‹åŒ–å¹³å°çº§é™æµ
        for platform in AIPlatform:
            self.platform_limits[platform.value] = {
   "current_requests": 0,
        "last_reset": datetime.now(),
        "max_limit": self.config.concurrent_request_limit,
    "window_seconds": 60
         }
        
   logger.info("ğŸš¦ ä¼ä¸šçº§é™æµå™¨åˆå§‹åŒ–")
    
    async def is_request_allowed(self, request_id: str, platform: str = "unified") -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¢«å…è®¸ï¼ˆåŸºäºå¹³å°å’Œæ•´ä½“é™åˆ¶ï¼‰"""
        
        # å¹³å°çº§é™æµæ£€æŸ¥
   platform_info = self.platform_limits.get(platform, {})
      
        # æ¯å°æ—¶é‡ç½®è®¡æ•°
        if datetime.now() >= platform_info["last_reset"] + timedelta(seconds=platform_info["window_seconds"]):
         platform_info["current_requests"] = 0
  platform_info["last_reset"] = datetime.now()
        
   # æ£€æŸ¥å¹³å°å¹¶å‘é™åˆ¶
        if platform_info["current_requests"] >= platform_info["max_limit"]:
   logger.warning(f"é™æµè§¦å‘ - å¹³å°: {platform}, å½“å‰è¯·æ±‚: {platform_info['current_requests']}")
            return False
        
     # å¢åŠ è®¡æ•°
    platform_info["current_requests"] += 1
        
      return True

class EnterpriseQoSManager:
    """ä¼ä¸šæœåŠ¡è´¨é‡ç®¡ç†å™¨"""
    
    def __init__(self):
self.priority_queues = {
     QueryPriority.CRITICAL: asyncio.Queue(maxsize=100),
         QueryPriority.HIGH: asyncio.Queue(maxsize=200),
      QueryPriority.NORMAL: asyncio.Queue(maxsize=500),
     QueryPriority.BATCH: asyncio.Queue(maxsize=1000)
    }
     
        logger.info("âš™ï¸ æœåŠ¡è´¨é‡ç®¡ç†å™¨åˆå§‹åŒ–")
    
    async def handle_priority(self, request: UnifiedQueryRequest) -> bool:
        """å¤„ç†æŸ¥è¯¢ä¼˜å…ˆçº§"""
 # æ·»åŠ è¯·æ±‚åˆ°ç›¸åº”çš„ä¼˜å…ˆçº§é˜Ÿåˆ—
        return await self.priority_queues[request.priority].put(request) >= 0

class EnterpriseSessionManager:
    """ä¼ä¸šä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(self, timeout_minutes: int = 30):
     self.timeout_minutes = timeout_minutes
        self.session_store = {}
        
  logger.info(f"ğŸ”‘ ä¼ä¸šä¼šè¯ç®¡ç†å™¨åˆå§‹åŒ– - è¶…æ—¶: {timeout_minutes}åˆ†é’Ÿ")
    
    def create_session(self, user_id: str) -> str:
   """åˆ›å»ºä¼šè¯"""
        session_id = f"entsess_{uuid.uuid4().hex[:12]}"
    
     self.session_store[session_id] = {
"user_id": user_id,
      "created_at": datetime.now(),
     "last_activity": datetime.now(),
   "is_active": True
        }
        
  return session_id
    
    def validate_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯ä¼šè¯"""
   
     session = self.session_store.get(session_id)
        
        if not session or session["is_active"] is False:
     return None
        
        # æ£€æŸ¥è¶…æ—¶
now = datetime.now()
        if now - session["last_activity"] > timedelta(minutes=self.timeout_minutes):
            session["is_active"] = False
            return None
        
        # æ›´æ–°æ´»åŠ¨æ—¶é—´
        session["last_activity"] = now
        
        return session.most_common_keys("user_id", "created_at")

def main():
    """ä¸»å‡½æ•°ï¼šæµ‹è¯•å¤šå¹³å°ç»Ÿä¸€AIå·¥ä½œæµAPI"""
    print("ğŸŒ LangChain L3 Advanced - Week 12: å¤šå¹³å°ç»Ÿä¸€AIå·¥ä½œæµAPI")
    print("=" * 70)
  
    try:
 # 1. åˆ›å»ºä¼ä¸šçº§APIç½‘å…³é…ç½®
        api_config = EnterpriseAPIConfig(
    api_name="ä¼ä¸šçº§AIç»Ÿä¸€æœåŠ¡API v1.0",
      base_url="http://localhost:8080",
            listen_port=8080,
  decision_engine_enabled=True,
         caching_strategy="memory_hybrid",  # ä½¿ç”¨æ··åˆç¼“å­˜
 auto_scaling_enabled=True,
 load_balancer_type="intelligent",
circuit_breaker_threshold=0.8
 )
        
     # 2. åˆå§‹åŒ–ç»Ÿä¸€APIç½‘å…³
        unified_api = EnterpriseUnifiedAIAPI(api_config)
        
        print("ğŸŒ ä¼ä¸šçº§ç»Ÿä¸€APIç½‘å…³æµ‹è¯•")
  print("-" * 40)
        
        # æµ‹è¯•ç”¨ä¾‹1ï¼šçŸ¥è¯†é—®ç­”å‹æŸ¥è¯¢ï¼ˆåº”è¯¥ä¼˜é€‰RAGFlowï¼‰
        knowledge_query = UnifiedQueryRequest(
       query="ä¼ä¸šçš„å®‰å…¨ç­–ç•¥å¯¹å‘˜å·¥æ•°æ®å¤„ç†æœ‰å“ªäº›åˆè§„è¦æ±‚ï¼Ÿ",
   priority=QueryPriority.NORMAL,
   response_format=ResponseFormat.JSON,
            language="zh",
     metadata={
     "user_id": "enterprise_user_001",
        "session_id": unified_api.session_manager.create_session("ent_user_001")
            }
        )
 
        print("ğŸ“ æµ‹è¯•æŸ¥è¯¢1: ä¼ä¸šå®‰å…¨åˆè§„çŸ¥è¯†é—®ç­”")
    print(f"   æŸ¥è¯¢: {knowledge_query.query}")
     
  start_time = time.time()
 knowledge_result = asyncio.run(unified_api.process_unified_query(knowledge_query))
    processing_time = (time.time() - start_time) * 1000
        
        print(f"   å“åº”å¹³å°: {knowledge_result.platform_used}")
        print(f"   ç½®ä¿¡åº¦: {knowledge_result.confidence_score:.2f}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.0f}ms")
        print(f"   ç­”æ¡ˆé¢„è§ˆ: {knowledge_result.answer[:120]}...")
     print(f"   å»ºè®®æ“ä½œ: {'; '.join(knowledge_result.next_actions or ['æ— å…·ä½“å»ºè®®'])}")
        print("-" * 40)
    
        # æµ‹è¯•ç”¨ä¾‹2ï¼šå·¥ä½œæµè‡ªåŠ¨åŒ–ï¼ˆåº”è¯¥ä¼˜é€‰n8nï¼‰ 
        workflow_query = UnifiedQueryRequest(
      query="åˆ›å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å®¢æˆ·æ•°æ®åŒæ­¥æµç¨‹ï¼Œæ¯6å°æ—¶æ£€æŸ¥æ•°æ®æºå¹¶å‘é€æˆåŠŸé€šçŸ¥åˆ°è¿è¥å›¢é˜Ÿ",
  priority=QueryPriority.HIGH, 
   response_format=ResponseFormat.JSON,
            language="zh", 
        metadata={
        "user_id": "workflow_admin_002",
         "session_id": unified_api.session_manager.create_session("workflow_user_002"),
         "max_steps": 10
   }
        )
   
        print("âš™ï¸ æµ‹è¯•æŸ¥è¯¢2: è‡ªåŠ¨åŒ–å·¥ä½œæµåˆ›å»º")
     print(f"   æŸ¥è¯¢: {workflow_query.query}")
        print(f"   ä¼˜å…ˆçº§: {workflow_query.priority}")
        
    start_time = time.time()
        workflow_result = asyncio.run(unified_api.process_unified_query(workflow_query))
        processing_time = (time.time() - start_time) * 1000
        
     print(f"   å“åº”å¹³å°: {workflow_result.platform_used}")
        print(f"   ç½®ä¿¡åº¦: {workflow_result.confidence_score:.2f}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.0f}ms")
 print(f"   å»ºè®®æ“ä½œ: {'; '.join(workflow_result.next_actions or ['æ— å…·ä½“å»ºè®®'])}")
     print("-" * 40)
      
        # æµ‹è¯•3ï¼šAPIå¥åº·çŠ¶æ€æ£€æŸ¥
        health_status = asyncio.run(unified_api.get_api_health_status())
        
        print("ğŸ“Š APIå¥åº·çŠ¶æ€æ±‡æ€»")
   print(f"   æœåŠ¡åç§°: {health_status['service_name']}")
        print(f"   APIç‰ˆæœ¬: {health_status['version']}")
  print(f"   çŠ¶æ€: {health_status['status']}") 
print(f"   æ­£å¸¸è¿è¡Œç‡: {health_status['uptime_percentage']}%")
        print("   å¹³å°çŠ¶æ€æ‘˜è¦:")
        
        for platform, status_info in health_status.get("platform_statuses", {}).items():
            status_emoji = "âœ…" if status_info.get("status") == "healthy" else "âš ï¸"
        print(f"     {status_emoji} {platform}: {status_info.get('status', 'unknown')} ({status_info.get('avg_latency_ms', 0)}ms)")
        
        print("-" * 40)
   
        print("\nâœ… å¤šå¹³å°ç»Ÿä¸€APIæµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
        print("\nğŸ“‘ ä¸»è¦ä¼ä¸šç‰¹æ€§:")
        print("   ğŸ§  æ™ºèƒ½å¹³å°å†³ç­–å¼•æ“ï¼ˆAIé©±åŠ¨ï¼‰")
    print("   ğŸ”€ å¤šå¹³å°æ™ºèƒ½è·¯ç”±ä¸æ•…éšœè½¬ç§»")
      print("   ğŸ’¡ åŸºäºæŸ¥è¯¢æ„å›¾çš„å¹³å°é€‰æ‹©")
        print("   âš¡ é«˜æ€§èƒ½ç¼“å­˜ç®¡ç†ï¼ˆLRU/TTLï¼‰")
  print("   ğŸ­ ç»Ÿä¸€ä¼ä¸šçº§é”™è¯¯å¤„ç†ä¸å®‰å…¨")
        print("   ğŸ“ˆ å®æ—¶ç›‘æ§å’ŒæœåŠ¡è¯Šæ–­")
        
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. éƒ¨ç½²PostgreSQL/Redisé›†ç¾¤")
     print("   2. é…ç½®å¹³å°APIå¯†é’¥(.env)")
        print("   3. å¯ç”¨å†³ç­–å¼•æ“åŠ¨æ€é€‰æ‹©")
   print("   4. æµ‹è¯•æ•…éšœè½¬ç§»æœºåˆ¶")
        print("   5. é…ç½®ä¼ä¸šç›‘æ§å‘Šè­¦")
        
    except Exception as e:
        print(f"\nâŒ å¤šå¹³å°ç»Ÿä¸€APIæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()